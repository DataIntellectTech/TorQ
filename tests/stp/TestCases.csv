Process,Functionality ,Sub Functionality ,ID,Description,Expected Result,Notes,Assignee,Status ,Contained in File,
RDB (or any Subscriber),Subscriptions,All Subscriptions,1.1.1,"Data is published, subscriber receives it (all data)",Consumer receives data,"Tests will require a feed, a TP and an RDB. The test script itself can be the feed (e.g. send a specific set of messages from the test script to the TP, make sure they get to the RDB)",MP,Done,subscription/subs.csv,
,,Disconnect,1.1.2,"Subscriber disconnects, is removed from the subscription queue",Consumer removed from subscription queue,,MP,Done,subscription/subs.csv,
,,Filtered Subscription on Sym,1.1.3,Subscriber subscribes with .u.sub[`table;`list`of`symbols],Consumer subscribes to specific list of sym data only,,MP,Done,subscription/subs.csv,
,,Complex filtered subscription,1.1.4,"Complex subscription e.g. ""price within 100 200, size>100""",Consumer only receives updates where filter conditions are met,,MP,Done,subscription/subs.csv,
,,Complex filtered disconnect,1.1.5,Subscriber disconnects,Subscriber Is removed from subscription queue,,MP,Done,subscription/subs.csv,
,,Update logs,1.1.6,"Data is published, it is saved in the log file",Update saved in log file,,MP,Done,subscription/subs.csv,
,,Error logs,1.1.7,"Bad update published, gets saved in error log",Bad update saved to log file,,MP,Done,subscription/subs.csv,
,,Filtered Subscription file missing ,1.1.8,Filtered sub file doesn't exist,Process fails to start,,MP,Done,subfile/subfilemissing.csv,is this one covered? 
,,Publish only by filtered subsciptions,1.1.9,Subscriber subscribes with .u.sub[`table;`list`of`symbols],Consumer subscribes to specific list of sym data with only the specific data being published,,PK,In Progress,pubsub/pubsub.csv,
RDB ,Recovery - single daily log file (tabperiod=`none),"RDB Restart, all subscriptions",2.1.1,"RDB subscribes for all data, is bounced",Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverysinglelog.csv,
,,"RDB restart, sym subscriptions (using .rdb.subscribesyms variable)",2.1.2,"RDB subscribes to a subset of syms, is bounced",Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverysinglelog.csv,
,,"RDB restart, complex filter using filtered subs functionality ",2.1.3,RDB subscribes to a subset of data using filtered functionality ,Data is the same in the RDB pre and post bounce,"These are similar to 2.1.1 but for this type of TP log file the TP may have multple log files, all of which should be replayed. So the set up should be:
1. Start TP
2. Publish some data
3. Bounce TP
4. Publish some more data
5. Start RDB, ensure all expected data is replayed",AW,Done,recovery/recoverysinglelog.csv,
,"Recovery - log file per table (in STP, tabperiod=`tabular)","RDB Restart, all subscriptions",2.2.1,"RDB subscribes for all data, is bounced",Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverytabular.csv,
,,"RDB restart, sym subscriptions (using .rdb.subscribesyms variable)",2.2.2,"RDB subscribes to a subset of syms, is bounced",Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverytabular.csv,
,,"RDB restart, complex filter using filtered subs functionality ",2.2.3,RDB subscribes to a subset of data using filtered functionality ,Data is the same in the RDB pre and post bounce,As 2.2.1,AW,Done,recovery/recoverytabular.csv,
,"Recovery - log file per table and per period (in STP, tabperiod=multilog","RDB Restart, all subscriptions",2.3.1,"RDB subscribes for all data, is bounced",Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverytabperiod.csv,
,,"RDB restart, sym subscriptions (using .rdb.subscribesyms variable)",2.3.2,"RDB subscribes to a subset of syms, is bounced",Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverytabperiod.csv,
,,"RDB restart, complex filter using filtered subs functionality ",2.3.3,RDB subscribes to a subset of data using filtered functionality ,Data is the same in the RDB pre and post bounce,,AW,Done,recovery/recoverytabperiod.csv,
,,"RDB Restart, all subscriptions but for the last period",2.3.4,"As 2.3.1, except replayperiod=`period rather than `day","In this case, the RDB will only recover the data since the start of the last period (e.g. for the last hour)","As 2.2.1, but we need to make sure the old TP runs as well (i.e. we haven't broken the old code)",MP,Done,recovery/recoveryreplayperiod.csv,
,Recovery - Old TP (not segmented),"RDB Restart, all subscriptions",2.4.1,"RDB subscribes for all data, is bounced",Data is the same in the RDB pre and post bounce,,MP,Done,tpvalidation/oldtp.csv,
,,"RDB restart, sym subscriptions (using .rdb.subscribesyms variable)",2.4.2,"RDB subscribes to a subset of syms, is bounced",Data is the same in the RDB pre and post bounce,This will require TP log files to be built up. Can probably just use those generatedd as output from the 2.* tests. It might be that these tests are run subsequently to the 2.* tests as then the log files will all have been generated. Need to ensure that row counts in HDB post replay match those generated and sent to log file ,MP,Done,tpvalidation/oldtp.csv,
TickerLog Replay,Replay of single daily log file,"Log replay, all tables ",3.1.1,TP replay replays all tables to correct HDB partition,,,Jamie Carton,Done,tickerlog/stpnone,
,,"Log replay, subset of tables",3.1.2,TP replay replays correct subst of tables to HDB partition,,,Jamie Carton,Done,tickerlog/singnone,
,"Recovery, log file per table ","Log replay, all tables ",3.2.1,TP replay replays all tables to correct HDB partition,,,Jamie Carton,Done,tickerlog/stptabular,
,,"Log replay, subset of tables",3.2.2,TP replay replays correct subst of tables to HDB partition,Must ensure that log files for the tables that aren't to be replayed are not replayed through - should be ignored,,Jamie Carton,Done,tickerlog/singtabular,
,"Recovery, log file per table per time period","Log replay, all tables ",3.3.1,TP replay replays all tables to correct HDB partition,,,Jamie Carton,Done,tickerlog/stptabperiod,
,,"Log replay, subset of tables",3.3.2,TP replay replays correct subst of tables to HDB partition,Must ensure that log files for the tables that aren't to be replayed are not replayed through - should be ignored,,,Done,tickerlog/singtabperiod,
,"Recovery, old TP log format","Log replay, all tables ",3.4.1,TP replay replays all tables to correct HDB partition,,,Jamie Carton,Done,tickerlog/oldlog,
,,"Log replay, subset of tables",3.4.2,TP replay replays correct subst of tables to HDB partition,Must ensure that log files for the tables that aren't to be replayed are not replayed through - should be ignored,"The purpose of these tests is to make sure there isn't any data loss or replication if a subscriber subscribes to data mid flow. The TP should report the correct log counts to replay, and the subsequent data should be waiting on the handle to be delivered to the consumer. ",Jamie Carton,Done,tickerlog/singoldlog,
Segmented Tickerplant,Batch subscription behaviour,Mid-batch subscription,4.1.1,"Publish batch of data to TP, stop publishing, publish second batch, RDB subscribes","The first batch should be replayed from the log file straight into the RDB, while the second batch should remain on the handle until released",,MP,Done,batching/defbatch.csv,
,,Mid-memorybatch subscription,4.1.2,"Publish batch of data to TP, stop publishing, publish second batch, RDB subscribes","The first batch should be replayed from the log file straight into the RDB, while the second batch should remain on the handle until released",,MP,Done,batching/membatch.csv,
,,Immediate mode publishing,4.1.3,"Turn off timer, publish data batch",It should publish to the subscriber and write to logs immediately even without a timer,"Assume two tables, trade and quote. Trade uses the standard (non customised) upd function and will timestamp on the time. Quote gets it's own upd function which stamps on both the time and the global message sequence number (as defined by the TP). Ensure the table updates are interleaved",MP,Done,batching/immbatch.csv,
,Upd functions,"Custom UPD, Immediate mode",5.1.1,Tables have different upd functions,The STP stamps the tables appropriately,,Elliot,Done,upds/upds.csv,
,,"Custom UPD, batch mode",5.1.2,Tables have different upd functions,The STP stamps the tables appropriately,,Elliot,Done,upds/upds.csv,
,,"Custom UPD, autobatch mode",5.1.3,Tables have different upd functions,The STP stamps the tables appropriately,"Interleave good messages with bad messages (e.g. wrong type, wrong lenghth. Do this as an extension of the 5.1 tests, so we are testing error mode in conjunction with custom upd functions",Elliot,Done,upds/upds.csv,
,,"Error trapping, immediate mode",5.2.1,Errors are written to error log file when in error mode. ,Errors go to error log file,,Elliot,Done,upds/errorupds.csv,
,,"Error trapping, batch mode",5.2.2,Errors are written to error log file when in error mode. ,Errors go to error log file,,Elliot,Done,upds/errorupds.csv,
,,"Error trapping, autobatch mode",5.2.3,Errors are written to error log file when in error mode. ,Errors go to error log file,,Elliot,Done,upds/errorupds.csv,
,Period End,Endp message - all,6.1.1,"Period ends, sub receives endp message from STP",Sub receives endp message,,MP,Done,periodend/endp.csv,
,,Endp message - table,6.1.2,"Period ends, sub receives endp message from STP",Sub receives endp message,,MP,Done,periodend/endp.csv,
,,Endp message - filtered,6.1.3,"Period ends, sub receives endp message from STP",Sub receives endp message,,MP,Done,periodend/endp.csv,
,,Meta table,6.1.4,"Period ends, STP meta table is correctly updated",STP meta table is correctly updated,,MP,Done,periodend/endp.csv,
,,Log names,6.1.5,"Period ends, STP currlog table is correctly updated",STP currlog table is correctly updated,"We will need a reliable, non-disruptive way to spoof EoD to be able to test this properly",MP,Done,periodend/endp.csv,
,EoD,EoD message,6.2.1,Trigger EoD,Subs should receive .u.end message,,MP,Done,eod/eodroutine.csv,
,,EoD roll log,6.2.2,Trigger EoD,The STP should roll the previous day's log,,MP,Done,eod/eodroutine.csv,
,,New logs,6.2.3,Trigger EoD,STP should create a new set of logs for 'today',,MP,Done,eod/eodroutine.csv,
,Custom logging modes,Custom mode logs,6.3.1,Start up STP with custom logging,Tables should direct to log files as specified in the custom CSV,,MP,Done,custommode/logs.csv,
,,Custom mode meta,6.3.2,Start up STP with custom logging,Meta table should reflect the custom logging scheme,,MP,Done,custommode/metaroll.csv,
,,Custom mode roll,6.3.3,"Start up STP with custom logging, roll period",Some log files should roll and others shouldn't,,MP,Done,custommode/metaroll.csv,
,STP tables,Log table,6.4.1,Check log table is formed properly,Log table has the correct info,,MP,Done,subscription/subs.csv,
,,Meta table,6.4.2,Check meta table is formed properly,Meta table has the correct info,,MP,Done,subscription/subs.csv,
,Timezone adjustments,datatimezone,6.5.1,Change the datatimezone and ensure the data is stamped in the correct timezone. ,"Change it to a TZ with a fixed offset from GMT so we don't have to worry about daylight savings e.g. HK time, Singapore time, EST",datatimezone changes the timezone that the data is stamped in. rolltimezone changes the timezone that the TP rolls down in. rolltimeoffset changes the offset from midnight to save down at. ,MP,Done,tz/tz.csv,
,,rolltimezone,6.5.2,Change the rolltimezone and ensure the TP rolls over at midnight in the correct timezone. Make sure tha the date changes to the correct date new date in the given timezone,"As 6.5.1, use a TZ with a fixed offset. So for example if the TP is set to roll in Singapore time (GMT+8) then at 2020.01.01D16:00 GMT the TP should roll and the new date in the TP should be set as 2020.01.02 ",,MP,Done,tz/tzcustom.csv,
,,rolltimeoffset,6.5.3,"Change the offset from midnight. Do this in conjunction with the rolltimezone change. Need to make sure this works forward and backward, and that the ""date"" rolls correctly","This gets a bit tricky now. If we extend the above example that the rolltimeoffset is set to -0D02 then the TP should roll at 10pm Singapore time, which is 2020.01.01D14:00 GMT and at that time the date should move to 2020.01.02. If the rolltimeoffset is +0D02 then it should roll at 2020.01.01D18 GMT. ",,MP,Done,tz/tzcustom.csv,
WDB,Recovery - single daily log file (tabperiod=`none),"WDB Restart, all subscriptions",7.1.1,"WDB subscribes for all data, is bounced",Data on disk is the same pre and post bounce,"For each of these tests you need to play in a set amount of data to the TP and then bounce the WDB. The data before bounce should be exactly the same as the data after the bounce. It might be that it is better to run the tests with multiple WDB instances, to cover the filtered and non-filtered cases at the same time. To force the data to disk, I think you should set .wdb.numrows to 0, and probably manually invoke .wdb.savetodisk to force it to save down

* Note this is very similar to test set 2.1 *",MP,Done,wdb/singlelog.csv,
,,"WDB restart, sym subscriptions (using .wdb.subsyms variable)",7.1.2,"WDB subscribes to a subset of syms, is bounced",Data is the same in the WDB pre and post bounce,,MP,Done,wdb/singlelog.csv,
,,"WDB restart, only subscribing to a subset of syms using .wdb.subtabs",7.1.3,WDB subscribes to a subset of tables ,Data is the same in the WDB pre and post bounce,"These are similar to 7.1.1 but for this type of TP log file the TP may have multple log files, all of which should be replayed. So the set up should be:
1. Start TP
2. Publish some data
3. Bounce TP
4. Publish some more data
5. Start WDB, ensure all expected data is replayed",MP,Done,wdb/singlelog.csv,
,"Recovery - log file per table (in STP, tabperiod=`tabular)","WDB Restart, all subscriptions",7.2.1,"WDB subscribes for all data, is bounced",Data on disk is the same pre and post bounce,,MP,Done,,
,,"WDB restart, sym subscriptions (using .wdb.subsyms variable)",7.2.2,"WDB subscribes to a subset of syms, is bounced",Data is the same in the WDB pre and post bounce,,MP,Done,wdb/tabular.csv,
,,"WDB restart, only subscribing to a subset of syms using .wdb.subtabs",7.2.3,WDB subscribes to a subset of tables ,Data is the same in the WDB pre and post bounce,As 7.2.1,MP,Done,wdb/tabular.csv,
,"Recovery - log file per table and per period (in STP, tabperiod=multilog","WDB Restart, all subscriptions",7.3.1,"WDB subscribes for all data, is bounced",Data on disk is the same pre and post bounce,,MP,Done,wdb/tabperiod.csv,
,,"WDB restart, sym subscriptions (using .wdb.subsyms variable)",7.3.2,"WDB subscribes to a subset of syms, is bounced",Data is the same in the WDB pre and post bounce,,MP,Done,wdb/tabperiod.csv,
,,"WDB restart, only subscribing to a subset of syms using .wdb.subtabs",7.3.3,WDB subscribes to a subset of tables ,Data is the same in the WDB pre and post bounce,"As 7.2.1, but we need to make sure the old TP runs as well (i.e. we haven't broken the old code)",MP,Done,wdb/tabperiod.csv,
,Recovery - Old TP (not segmented),"WDB Restart, all subscriptions",7.4.1,"WDB subscribes for all data, is bounced",Data on disk is the same pre and post bounce,,MP,Done,tpvalidation/oldtp.csv,
,,"WDB restart, sym subscriptions and table subscriptions using .wdb.subsyms and .wdb.subtabs variables ",7.4.2,"WDB subscribes to a subset of tables and syms, is bounced",Data is the same in the WDB pre and post bounce,"These tests are going to need feed, segemented TP, CTP, and subscriber. We want to make sure that the data gets from the feed to the subscriber (which is connected to the CTP). The test process probably needs to run externally. The subscriber can be simple, should just insert into memory

The first of these tests are very similar to 1.1.*",MP,Done,tpvalidation/oldtp.csv,
Chained Tickerplant,Subscriptions from client ,All Subscriptions,8.1.1,"Data is published, subscriber receives it (all data)",Consumer receives data,,Elliot,Done,chainedstp/sub.csv,
,,Disconnect,8.1.2,"Subscriber disconnects, is removed from the subscription queue",Consumer removed from subscription queue,,Elliot,Done,chainedstp/sub.csv,
,,Filtered Subscription on Sym,8.1.3,Subscriber subscribes with .u.sub[`table;`list`of`symbols],Consumer subscribes to specific list of sym data only,,Elliot,Done,chainedstp/sub.csv,
,,Complex filtered subscription,8.1.4,"Complex subscription e.g. ""price within 100 200, size>100""",Consumer only receives updates where filter conditions are met,,Elliot,Done,chainedstp/sub.csv,
,,Complex filtered disconnect,8.1.5,Subscriber disconnects,Subscriber Is removed from subscription queue,,Elliot,Done,chainedstp/sub.csv,
,,Filtered Subscription file missing ,8.1.6,Filtered sub file doesn't exist,Process fails to start,See 1.1.8,Elliot,Done,,
,,Segmented TP dies,8.1.7,"Segmented TP should die, should cause the SCTP to also die","CTP should also die, which should trigger a disconnect in the subscriber process",,Elliot,Done,chainedeod/endroutine.csv,
,,End-of-period,8.1.8,End-of-period-message (sent from STP) is propagated to client,Subscriber receives end of period,,Elliot,Done,chainedperiodend/endp.csv,
,,End-of-day,8.1.9,End-of-day message (sent from STP) is propagated to client,Subscriber recives end-of-day,"For these tests, given that we are using the same logging and pub/sub logic/code as in main segented TP mode, we shouldn’t have to re-test all the logic again",Elliot,Done,chainedeod/endroutine.csv,
,Logging,CTP creates log file - tabperiod=`multilog,8.2.1,"CTP should create a log file directory and a meta file, same as standard TP. Need to bounce the CTP to ensure new log files are created. Start TP+CTP+Feed. CTP should create log file. Bounce the CTP, should create new log file",Log files are created as expected,,Elliot,Done,chainedstp/sub.csv,
,,Subscriber subscriptions - tabperiod,8.2.2,CTP creates tabperiod log files. Subscriber to CTP (e.g. connect RDB to CTP) should be able to recover from them. Use equivalent of 2.3.1 for this ,Subscriber recovers,,Elliot,Done,chainedrecovery/tabperiod.csv,
,,STP still functions when logging turned off,8.2.3,"When createlogs:0b, the STP process should function as normal",STP functions normally,,Elliot,Done,chainedstp/sub.csv,
,,Data in CTP flushed,8.2.4,,Data in CTP flushed to subscribers at end of period/day,,Elliot,Done,chainedeod/endroutine.csv & chainedperiodend/endp.csv,
,Pass Through Logging,Recovery,8.3.1,,,"I don't think we have written this functionality yet. The crux of it is that if the CTP can access the TP's log file then it should be able to pass through the subscription request and it's client should be able to recover from the main TP log file, without dropping messages or having duplicates",Elliot,Done,chainedrecovery/tabperiod.csv,
,,,,,,,,,,
,EoD / End of Period,EoD Rolloing,8.4.1,"At EoD, log files and meta table get rolled correctly (assuming log files are turned on)","New log file directory is created, new meta file is created",,Elliot,Done,chainedeod/endroutine.csv,
,,EoP Rolling,8.4.2,"At end of period, new log file is created, meta is updated","New log file is created, meta updated",,Elliot,Done,chainedperiodend/endp.csv,
,,EoD Date,8.4.3,"At EoD, new date is incremented (should be the same as the TP after EoD)",,,Elliot,Done,chainedeod/endroutine.csv,
,,,,,,,,,,
Housekeeping,Zipping,Zipping,9.1.1,Housekeeping process has a new zip function as part of this development. This should zip the whole directory and create a single zip file containing all the log files for the date,Single zipped file with all the logs in it,,Jamie Carton,,housekeeping/zippingtest.csv,
Performance,Individual message rates,immediate mode,10.1.1,measure the performance (messages per second) that the TP can handle when being sent small updates for a single table,"We will need to have a baseline value for messages per second, but it will be dependent on hardware. Relative figures are probably more interesting for an specific hardware spec","For each of these tests we need to use what Rob Sketch put together. We need to have a work out how many messages we can push through a TP per second (one feedhandler sending async messages, one TP, one subscriber receiving the data. ",,,,
,,memory batch,10.1.2,"As 10.1.1, but TP in memory batch mode",,,,,,
,,default batch,10.1.3,"As 10.1.2, but TP in default batch mode",,,,,,
,bulk message rates,immediate mode,10.2.1,"As 10.1.1, but with small batch messages (e.g. 100 rows at a time)",,,,,,
,,memory batch,10.2.2,"As 10.2.1, but TP in memory batch mode",,,,,,
,,default batch,10.2.3,"As 10.2.1, but TP in default batch mode",,,,,,
