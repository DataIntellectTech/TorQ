{
    "docs": [
        {
            "location": "/", 
            "text": "The TorQ framework created by \nAquaQ Analytics\n forms the basis of a production kdb+ system by implementing some core functionality and utilities on top of kdb+, allowing developers to concentrate on the application business logic. It incorporates as many best practices as possible, with particular focus on performance, process management, diagnostic information, maintainability and extensibility. Wherever possible, we have tried to avoid re-inventing the wheel and instead have used contributed code from \ncode.kx.com\n (either directly or modified). This framework will be suitable for those looking to create a new kdb+ system from scratch or those looking to add additional functionality to their existing kdb+ systems.\n\n\nThe easiest way to get a production capture started is to download and install one of the \nStarter Packs\n (here's a \nshort video\n to get going), or read the manual. We also have a \nGoogle Group for questions/discussions\n.\n\n\nFor recent updates to TorQ please check out our \nblog\n.\n\n\nFor email support contact", 
            "title": "Home"
        }, 
        {
            "location": "/Overview/", 
            "text": "Overview\n\n\n\n\nWhat is kdb+?\n\n\nkdb+ is the market leading timeseries database from Kx Systems. kdb+\nis used predominently in the Financial Services sector to capture,\nprocess and analyse billions of records on a daily basis, with Kx\ncounting almost all of the top tier investment banks as customers. kdb+\nincorporates a programming language, q, which is known for its\nperformance and expressive power. Given the unsurpassed data management\nand analytical capabilities of kdb+, the applicability of kdb+\ntechnology extends beyond the financial domain into any sector where\nrapid pre-built or adhoc analysis of large datasets is required. Other\nsectors which have good use cases for kdb+ include utilities,\npharmaceuticals, telecoms, manufacturing, retail and any sector\nutilising telemetry or sensor data.\n\n\n\n\nWhat is AquaQ TorQ?\n\n\nAquaQ TorQ is a framework which forms the basis of a production kdb+\nsystem by implementing some core functionality and utilities on top of\nkdb+, allowing developers to concentrate on the application business\nlogic. We have incorporated as many best practices as possible, with\nparticular focus on performance, process management, diagnostic\ninformation, maintainability and extensibility. We have kept the code as\nreadable as possible using descriptive comments, error messages,\nfunction names and variable names. Wherever possible, we have tried to\navoid re-inventing the wheel and instead have used contributed code from\ncode.kx.com (either directly or modified). All code sections taken from\ncode.kx.com are referenced in this document.\n\n\nAquaQ TorQ can be extended or modified as required. We have chosen some\ndefault behaviour, but it can all be overridden. The features of AquaQ\nTorQ are:\n\n\n\n\n\n\nProcess Management: Each process is given a type and name. By\n    default these are used to determine the code base it loads, the\n    configuration loaded, log file naming and how it reports itself to\n    discovery services. Whenever possible we have tried to ensure that\n    all default behaviour can be overridden at the process type level,\n    and further at the process name level.\n\n\n\n\n\n\nCode Management: Processes can optionally load common or process\n    type/name specific code bases. All code loading is error trapped.\n\n\n\n\n\n\nConfiguration Management: Configuration scripts can be loaded as\n    standard and with specific process type/name configuration\n    overriding default values. Configuration scripts are loaded in a\n    specific order; default, then process type specific, then process\n    name specific. Values loaded last will override values loaded\n    previously.\n\n\n\n\n\n\nUsage Logging: All process usage is logged to a single text log file\n    and periodically rolled. Logging includes opening/closing of\n    connections, synchronous and asynchronous queries and functions\n    executed on the timer. Logged values include the request, the\n    details of where it came from, the time it was received, the time it\n    took, memory usage before and after, the size of the result set, the\n    status and any error message.\n\n\n\n\n\n\nIncoming and Outgoing Connection Management: Incoming (client) and\n    outgoing (server) connections are stored and their usage monitored\n    through query counts and total data size counts. Connections are\n    stored and retrieved and can be set to automatically be re-opened as\n    required. The password used for outgoing connections can be\n    overridden at default, process type and process name level.\n\n\n\n\n\n\nAccess Controls: Basic access controls are provided, and could be\n    extended. These apply restrictions on the IP addresses of remote\n    connections, the users who can access the process, and the functions\n    that each user can execute. A similar hierarchical approach is used\n    for access control management as for configuration management.\n\n\n\n\n\n\nTimer Extensions: Mechanism to allow multiple functions to be added\n    to the timer either on a repeating or one-off basis. Multiple\n    re-scheduling algorithms supplied for repeating timers.\n\n\n\n\n\n\nStandard Out/Error Logging: Functions to print formatted messages to\n    standard out and error. Hooks are provided to extend these as\n    required, e.g. publication to centralised logging database. Standard\n    out and error are redirected to appropriately named, timestamped and\n    aliased log files, which are periodically rolled.\n\n\n\n\n\n\nError Handling: Different failure options are supplied in case code\n    fails to load; either exit upon failure, stop at the point of\n    failure or trap and continue.\n\n\n\n\n\n\nVisualisation: Utilities to ease GUI development using websockets\n    and HTML5.\n\n\n\n\n\n\nDocumentation and Development Tools: Functionality to document the\n    system is built into AquaQ TorQ, and can be accessed directly from\n    every q session. Developers can extend the documentation as they add\n    new functions. Functionality for searching for functions and\n    variables by name and definition is provided, and for ordering\n    variables by memory usage. The standard help.q from code.kx is also\n    included.\n\n\n\n\n\n\nUtilities: We intend to build out and add utilities as we find them\n    to be suitably useful and generic. So far we have:\n\n\n\n\n\n\nCaching: allows a result set cache to be declared and result\n    sets to be stored and retrieved from the cache. Suitable for\n    functions which may be run multiple times with the same\n    parameters, with the underlying data not changing in a short\n    time frame;\n\n\n\n\n\n\nTimezone Handling: derived from code.kx, allows conversion\n    between timestamps in different timezones;\n\n\n\n\n\n\nEmail: an library to send emails;\n\n\n\n\n\n\nAsync Messaging: allows easy use of advanced async messaging\n    methods such as deferred synchronous communication and async\n    communication using postback functions;\n\n\n\n\n\n\nHeartbeating: each process can be set to publish heartbeats, and\n    subscribe to and manage heartbeats from other processes in the\n    environment;\n\n\n\n\n\n\nData Loading: utility wrapper around .Q.fsn to read a data file\n    from disk, manipulate it and write it out in chunks;\n\n\n\n\n\n\nSubscriptions: allow processes to dynamically detect and\n    subscribe to datasources;\n\n\n\n\n\n\nTickerplant Log File Recovery: recover as many messages as\n    possible from corrupt log files;\n\n\n\n\n\n\nDatabase Writing: utility functions for writing to, sorting and\n    parting on disk databases;\n\n\n\n\n\n\nCompression: allows compression of a database. This can be\n    performed using a set of parameters for the entire database, but\n    also gives the flexibilty of compressing user-specified tables\n    and/or columns of those tables with different parameters if\n    required, and also offers decompression.\n\n\n\n\n\n\n\n\n\n\nAquaQ TorQ will wrap easily around kdb+tick and therefore around any\ntickerplant, RDB, HDB or real time processing application. We currently\nhave several customised processes of our own:\n\n\n\n\n\n\nDiscovery Service: Every process has a type, name and set of\n    available attributes, which are used by other processes to connect\n    to it. The Discovery Service is a central point that can be used to\n    find other available processes. Client processes can subscribe to\n    updates from the discovery service as new processes become\n    available- the discovery service will notify its subscribers, which\n    can then use the supplied hook to implement required behavior e.g.\n    connect to the newly available process;\n\n\n\n\n\n\nGateway: A fully synchronous and asynchronous gateway is provided.\n\n\nThe gateway will connect to a defined list of process types (can be\nhomogenous or heterogeneous processes) and will route queries across\nthem according to the priority of received requests. The routing\nalgorithms can be easily modified e.g. give priority to user X, or\nonly route queries to processes which exist in the same data centre\nor geographical region to avoid the WAN (this would entail using the\nprocess attributes). The gateway can either return the result to the\nclient back down the same handle, or it can wrap it in a callback\nfunction to be invoked on the client;\n\n\n\n\n\n\nReal Time Database (RDB): A customized version of the kdb+tick RDB,\n    to allow dynamic tickerplant subscriptions, reloading of multiple\n    HDBs using authenticated connections, and customized end-of-day save\n    downs. The RDB, WDB and tickerplant log replay share a common code\n    base to ensure that a save-down modification to a table is applied\n    across each of these processes.\n\n\n\n\n\n\nWrite Database (WDB): The job of a WDB is to write data to disk\n    rather than to serve client queries. WDBs usually write data out\n    periodically throughout the day, and are useful when there is too\n    much data to fit into memory and/or the end of day save operation\n    needs to be speeded up. The concept is based on\n    \nw.q\n\n\n\n\n\n\nTickerplant Log Replay: A process for replaying tickerplant log\n    files to create on-disk data sets. Extended features are provided\n    for only replaying subsets of log files (by message number and/or\n    table name), replaying in chunks, invoking bespoke final behaviour\n    etc.;\n\n\n\n\n\n\nReporter: The Reporter Process runs defined reports (q queries or\n    parameterized functions) against specific database or gateways on a\n    schedule. The results are retrieved and processed. Processing can be\n    user defined, or can be a standard operation such as writing the\n    data to disk, or emailing the results to a list of recipients. This\n    can be useful for running system checks or generating management\n    reports.\n\n\n\n\n\n\nHousekeeping: A process to undertake housekeeping tasks\n    periodically, such as compressing and removing files that are no\n    longer used. Housekeeping looks up a file of instructions and\n    performs maintenance tasks on directories accordingly. Features\n    allow selective file deletion and zipping according to file age,\n    including a search string parameter and the ability to exclude items\n    from the search. The process can be scheduled, or run immediately\n    from the command line and can be extended as required to incorporate\n    more tasks.\n\n\n\n\n\n\nFile Alerter: A process to periodically scan a set of directories\n    and execute a function based on the availability of a file. This is\n    useful where files may arrive to the system during the day and must\n    be acted upon (e.g. files are uploaded to a shared directory by\n    users/clients). The functions to execute are defined by the user and\n    the whole process is driven by a csv file detailing the file to\n    search for, the function to execute and, optionally, a directory to\n    move the file to after it has been processed.\n\n\n\n\n\n\nMonitor: A basic monitoring process which uses the Discovery Service\n    to locate the other processes within the system, listens for\n    heartbeats, and subscribes for log messages. This should be extended\n    as required but provides a basic central point for system health\n    checks;\n\n\n\n\n\n\nKill: A process used to kill other processes, optionally using the\n    Discovery Service to locate them.\n\n\n\n\n\n\n\n\nA Large Scale Data Processing Platform\n\n\nOne of the key drivers behind TorQ development has been to ensure all\nthe tools necessary to build a large scale data processing platform are\navailable. \nkdb+tick\n\nprovides the basic building blocks, and a standard set-up usually looks\nsomething like this:\n\n\n\n\nHowever, in reality it is usually more complicated. A larger scale\narchitecture serving large numbers of client queries and receiving data\nfrom multiple sources may look like this:\n\n\n\n\nA common practice is to use a gateway (section gateway) to\nmanage client queries across back-end processes. The gateway can load\nbalance across processes and make failures transparent to the client. If\nthe clients access the gateway with asynchronous calls, then the gateway\ncan serve many requests at once and additionally implement client\nqueuing algorithms.\n\n\nOther common production features include:\n\n\n\n\n\n\nA modified version of the RDB (section sec:rdb) which does\n    different operations at end-of-day, reloads multiple HDB processes\n    etc.\n\n\n\n\n\n\nA Write Database (section [sec:wdb]) which receives data from the\n    tickerplant and periodically writes it to disk. WDBs are used when\n    there is too much data in a day to fit into memory and/or to speed\n    up the end-of-day rollover job\n\n\n\n\n\n\nProcesses that load data from other sources either into the HDB\n    directly or to the RDB potentially via the tickerplant (section\n    [sec:dataloader]). The data may be dropped in specific locations\n    which have to be monitored (section [sec:filealerter])\n\n\n\n\n\n\nA Reporting Engine (section [sec:reporter]) to run periodic\n    reports and do something with the result (e.g. generate an xls file\n    from the database and email it to senior management). Reporting\n    engines can also be used to run periodic checks of the system\n\n\n\n\n\n\nA Discovery Service (section [sec:discovery]) to allow processes\n    to locate each other, and to allow processes to dynamically register\n    availability and push notifications around the system.\n\n\n\n\n\n\nBasic Monitoring (section [sec:monitor]) of process availability\n\n\n\n\n\n\nHousekeeping (section [sec:housekeeping]) to ensure log files are\n    tidied up, tickerplant log files are compressed/moved in a timely\n    fashion etc.\n\n\n\n\n\n\n\n\nDo I Really Have to Read This Whole Document?\n\n\nHopefully not. The core of AquaQ TorQ is a script called torq.q and we\nhave tried to make it as descriptive as possible, so perhaps that will\nsuffice. The first place to look will be in the config files, the main\none being \\$KDBCONFIG/settings/default.q. This should contain a lot of\ninformation on what can be modified. In addition:\n\n\n\n\n\n\nWe have added a load of usage information:\n\n\naquaq$ q torq.q -usage\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\nGeneral:\n This script should form the basis of a production kdb+ environment.\n It can be sourced from other files if required, or used as a launch script before loading other files/directories using either -load or -loaddir flags \n... etc ...\n\n\n\nIf sourcing from another script there are hooks to modify and extend\nthe usage information as required.\n\n\n\n\n\n\nWe have some pretty extensive logging:\n\n\naquaq$ q torq.q -p 9999 -debug\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\n2013.11.05D12:22:42.597500000|aquaq|torq.q_3139_9999|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0\n2013.11.05D12:22:42.597545000|aquaq|torq.q_3139_9999|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0\n2013.11.05D12:22:42.597810000|aquaq|torq.q_3139_9999|INF|init|attempting to read required process parameters proctype,procname from file /torqhome/config/process.csv\n2013.11.05D12:22:42.598081000|aquaq|torq.q_3139_9999|INF|init|read in process parameters of proctype=hdb; procname=hdb1\n2013.11.05D12:22:42.598950000|aquaq|hdb1|INF|fileload|config file /torqhome/config/default.q found\n... etc ...\n\n\n\n\n\n\n\nWe have added functionality to find functions or variables defined\n    in the session, and also to search function definitions.\n\n\nq).api.f`max                                                                                                                                                                                   \nname                | vartype   namespace public descrip              ..\n--------------------| ------------------------------------------------..\nmaxs                | function  .q        1      \"\"                   ..\nmmax                | function  .q        1      \"\"                   ..\n.clients.MAXIDLE    | variable  .clients  0      \"\"                   ..\n.access.MAXSIZE     | variable  .access   0      \"\"                   ..\n.cache.maxsize      | variable  .cache    1      \"The maximum size in ..\n.cache.maxindividual| variable  .cache    1      \"The maximum size in ..\nmax                 | primitive           1      \"\"                   ..\n\nq)first 0!.api.p`.api                                                                                                                                                                          \nname     | `.api.f\nvartype  | `function\nnamespace| `.api\npublic   | 1b\ndescrip  | \"Find a function/variable/table/view in the current process\"\nparams   | \"[string:search string]\"\nreturn   | \"table of matching elements\"\n\nq).api.p`.api                                                                                                                                                                                  \nname        | vartype  namespace public descrip                       ..\n------------| --------------------------------------------------------..\n.api.f      | function .api      1      \"Find a function/variable/tabl..\n.api.p      | function .api      1      \"Find a public function/variab..\n.api.u      | function .api      1      \"Find a non-standard q public ..\n.api.s      | function .api      1      \"Search all function definitio..\n.api.find   | function .api      1      \"Generic method for finding fu..\n.api.search | function .api      1      \"Generic method for searching ..\n.api.add    | function .api      1      \"Add a function to the api des..\n.api.fullapi| function .api      1      \"Return the full function api ..\n\n\n\n\n\n\n\nWe have incorporated help.q.\n\n\nq)help`                                                                                                                                                                                        \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date \n time casts\nverbs     | verbs/functions\n\n\n\n\n\n\n\nWe have separated and commented all of our config:\n\n\naquaq$ head config/default.q \n/- Default configuration - loaded by all processes\n\n/- Process initialisation\n\\d .proc\nloadcommoncode:1b   /- whether to load the common code defined at\n                        /- ${KDBCODE}/common\nloadprocesscode:0b  /- whether to load the process specific code defined at \n                        /- ${KDBCODE}/{process type} \nloadnamecode:0b     /- whether to load the name specific code defined at \n                    /- ${KDBCODE}/{name of process}\nloadhandlers:1b     /- whether to load the message handler code defined at \n                        /- ${KDBCODE}/handlers\nlogroll:1b      /- whether to roll the std out/err logs daily\n... etc ...\n\n\n\n\n\n\n\n\n\nOperating System and kdb+ Version\n\n\nAquaQ TorQ has been built and tested on the linux and OSX operating\nsystems though as far as we are aware there is nothing that would make\nthis incompatible with Solaris or Windows. It has also been tested with\nkdb+ 3.1 and 2.8. Please report any incompatibilities with other kdb+\nversions or operating systems.\n\n\n\n\nLicense\n\n\nThis code is released under the MIT license.", 
            "title": "About"
        }, 
        {
            "location": "/Overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/Overview/#what-is-kdb", 
            "text": "kdb+ is the market leading timeseries database from Kx Systems. kdb+\nis used predominently in the Financial Services sector to capture,\nprocess and analyse billions of records on a daily basis, with Kx\ncounting almost all of the top tier investment banks as customers. kdb+\nincorporates a programming language, q, which is known for its\nperformance and expressive power. Given the unsurpassed data management\nand analytical capabilities of kdb+, the applicability of kdb+\ntechnology extends beyond the financial domain into any sector where\nrapid pre-built or adhoc analysis of large datasets is required. Other\nsectors which have good use cases for kdb+ include utilities,\npharmaceuticals, telecoms, manufacturing, retail and any sector\nutilising telemetry or sensor data.", 
            "title": "What is kdb+?"
        }, 
        {
            "location": "/Overview/#what-is-aquaq-torq", 
            "text": "AquaQ TorQ is a framework which forms the basis of a production kdb+\nsystem by implementing some core functionality and utilities on top of\nkdb+, allowing developers to concentrate on the application business\nlogic. We have incorporated as many best practices as possible, with\nparticular focus on performance, process management, diagnostic\ninformation, maintainability and extensibility. We have kept the code as\nreadable as possible using descriptive comments, error messages,\nfunction names and variable names. Wherever possible, we have tried to\navoid re-inventing the wheel and instead have used contributed code from\ncode.kx.com (either directly or modified). All code sections taken from\ncode.kx.com are referenced in this document.  AquaQ TorQ can be extended or modified as required. We have chosen some\ndefault behaviour, but it can all be overridden. The features of AquaQ\nTorQ are:    Process Management: Each process is given a type and name. By\n    default these are used to determine the code base it loads, the\n    configuration loaded, log file naming and how it reports itself to\n    discovery services. Whenever possible we have tried to ensure that\n    all default behaviour can be overridden at the process type level,\n    and further at the process name level.    Code Management: Processes can optionally load common or process\n    type/name specific code bases. All code loading is error trapped.    Configuration Management: Configuration scripts can be loaded as\n    standard and with specific process type/name configuration\n    overriding default values. Configuration scripts are loaded in a\n    specific order; default, then process type specific, then process\n    name specific. Values loaded last will override values loaded\n    previously.    Usage Logging: All process usage is logged to a single text log file\n    and periodically rolled. Logging includes opening/closing of\n    connections, synchronous and asynchronous queries and functions\n    executed on the timer. Logged values include the request, the\n    details of where it came from, the time it was received, the time it\n    took, memory usage before and after, the size of the result set, the\n    status and any error message.    Incoming and Outgoing Connection Management: Incoming (client) and\n    outgoing (server) connections are stored and their usage monitored\n    through query counts and total data size counts. Connections are\n    stored and retrieved and can be set to automatically be re-opened as\n    required. The password used for outgoing connections can be\n    overridden at default, process type and process name level.    Access Controls: Basic access controls are provided, and could be\n    extended. These apply restrictions on the IP addresses of remote\n    connections, the users who can access the process, and the functions\n    that each user can execute. A similar hierarchical approach is used\n    for access control management as for configuration management.    Timer Extensions: Mechanism to allow multiple functions to be added\n    to the timer either on a repeating or one-off basis. Multiple\n    re-scheduling algorithms supplied for repeating timers.    Standard Out/Error Logging: Functions to print formatted messages to\n    standard out and error. Hooks are provided to extend these as\n    required, e.g. publication to centralised logging database. Standard\n    out and error are redirected to appropriately named, timestamped and\n    aliased log files, which are periodically rolled.    Error Handling: Different failure options are supplied in case code\n    fails to load; either exit upon failure, stop at the point of\n    failure or trap and continue.    Visualisation: Utilities to ease GUI development using websockets\n    and HTML5.    Documentation and Development Tools: Functionality to document the\n    system is built into AquaQ TorQ, and can be accessed directly from\n    every q session. Developers can extend the documentation as they add\n    new functions. Functionality for searching for functions and\n    variables by name and definition is provided, and for ordering\n    variables by memory usage. The standard help.q from code.kx is also\n    included.    Utilities: We intend to build out and add utilities as we find them\n    to be suitably useful and generic. So far we have:    Caching: allows a result set cache to be declared and result\n    sets to be stored and retrieved from the cache. Suitable for\n    functions which may be run multiple times with the same\n    parameters, with the underlying data not changing in a short\n    time frame;    Timezone Handling: derived from code.kx, allows conversion\n    between timestamps in different timezones;    Email: an library to send emails;    Async Messaging: allows easy use of advanced async messaging\n    methods such as deferred synchronous communication and async\n    communication using postback functions;    Heartbeating: each process can be set to publish heartbeats, and\n    subscribe to and manage heartbeats from other processes in the\n    environment;    Data Loading: utility wrapper around .Q.fsn to read a data file\n    from disk, manipulate it and write it out in chunks;    Subscriptions: allow processes to dynamically detect and\n    subscribe to datasources;    Tickerplant Log File Recovery: recover as many messages as\n    possible from corrupt log files;    Database Writing: utility functions for writing to, sorting and\n    parting on disk databases;    Compression: allows compression of a database. This can be\n    performed using a set of parameters for the entire database, but\n    also gives the flexibilty of compressing user-specified tables\n    and/or columns of those tables with different parameters if\n    required, and also offers decompression.      AquaQ TorQ will wrap easily around kdb+tick and therefore around any\ntickerplant, RDB, HDB or real time processing application. We currently\nhave several customised processes of our own:    Discovery Service: Every process has a type, name and set of\n    available attributes, which are used by other processes to connect\n    to it. The Discovery Service is a central point that can be used to\n    find other available processes. Client processes can subscribe to\n    updates from the discovery service as new processes become\n    available- the discovery service will notify its subscribers, which\n    can then use the supplied hook to implement required behavior e.g.\n    connect to the newly available process;    Gateway: A fully synchronous and asynchronous gateway is provided.  The gateway will connect to a defined list of process types (can be\nhomogenous or heterogeneous processes) and will route queries across\nthem according to the priority of received requests. The routing\nalgorithms can be easily modified e.g. give priority to user X, or\nonly route queries to processes which exist in the same data centre\nor geographical region to avoid the WAN (this would entail using the\nprocess attributes). The gateway can either return the result to the\nclient back down the same handle, or it can wrap it in a callback\nfunction to be invoked on the client;    Real Time Database (RDB): A customized version of the kdb+tick RDB,\n    to allow dynamic tickerplant subscriptions, reloading of multiple\n    HDBs using authenticated connections, and customized end-of-day save\n    downs. The RDB, WDB and tickerplant log replay share a common code\n    base to ensure that a save-down modification to a table is applied\n    across each of these processes.    Write Database (WDB): The job of a WDB is to write data to disk\n    rather than to serve client queries. WDBs usually write data out\n    periodically throughout the day, and are useful when there is too\n    much data to fit into memory and/or the end of day save operation\n    needs to be speeded up. The concept is based on\n     w.q    Tickerplant Log Replay: A process for replaying tickerplant log\n    files to create on-disk data sets. Extended features are provided\n    for only replaying subsets of log files (by message number and/or\n    table name), replaying in chunks, invoking bespoke final behaviour\n    etc.;    Reporter: The Reporter Process runs defined reports (q queries or\n    parameterized functions) against specific database or gateways on a\n    schedule. The results are retrieved and processed. Processing can be\n    user defined, or can be a standard operation such as writing the\n    data to disk, or emailing the results to a list of recipients. This\n    can be useful for running system checks or generating management\n    reports.    Housekeeping: A process to undertake housekeeping tasks\n    periodically, such as compressing and removing files that are no\n    longer used. Housekeeping looks up a file of instructions and\n    performs maintenance tasks on directories accordingly. Features\n    allow selective file deletion and zipping according to file age,\n    including a search string parameter and the ability to exclude items\n    from the search. The process can be scheduled, or run immediately\n    from the command line and can be extended as required to incorporate\n    more tasks.    File Alerter: A process to periodically scan a set of directories\n    and execute a function based on the availability of a file. This is\n    useful where files may arrive to the system during the day and must\n    be acted upon (e.g. files are uploaded to a shared directory by\n    users/clients). The functions to execute are defined by the user and\n    the whole process is driven by a csv file detailing the file to\n    search for, the function to execute and, optionally, a directory to\n    move the file to after it has been processed.    Monitor: A basic monitoring process which uses the Discovery Service\n    to locate the other processes within the system, listens for\n    heartbeats, and subscribes for log messages. This should be extended\n    as required but provides a basic central point for system health\n    checks;    Kill: A process used to kill other processes, optionally using the\n    Discovery Service to locate them.", 
            "title": "What is AquaQ TorQ?"
        }, 
        {
            "location": "/Overview/#a-large-scale-data-processing-platform", 
            "text": "One of the key drivers behind TorQ development has been to ensure all\nthe tools necessary to build a large scale data processing platform are\navailable.  kdb+tick \nprovides the basic building blocks, and a standard set-up usually looks\nsomething like this:   However, in reality it is usually more complicated. A larger scale\narchitecture serving large numbers of client queries and receiving data\nfrom multiple sources may look like this:   A common practice is to use a gateway (section gateway) to\nmanage client queries across back-end processes. The gateway can load\nbalance across processes and make failures transparent to the client. If\nthe clients access the gateway with asynchronous calls, then the gateway\ncan serve many requests at once and additionally implement client\nqueuing algorithms.  Other common production features include:    A modified version of the RDB (section sec:rdb) which does\n    different operations at end-of-day, reloads multiple HDB processes\n    etc.    A Write Database (section [sec:wdb]) which receives data from the\n    tickerplant and periodically writes it to disk. WDBs are used when\n    there is too much data in a day to fit into memory and/or to speed\n    up the end-of-day rollover job    Processes that load data from other sources either into the HDB\n    directly or to the RDB potentially via the tickerplant (section\n    [sec:dataloader]). The data may be dropped in specific locations\n    which have to be monitored (section [sec:filealerter])    A Reporting Engine (section [sec:reporter]) to run periodic\n    reports and do something with the result (e.g. generate an xls file\n    from the database and email it to senior management). Reporting\n    engines can also be used to run periodic checks of the system    A Discovery Service (section [sec:discovery]) to allow processes\n    to locate each other, and to allow processes to dynamically register\n    availability and push notifications around the system.    Basic Monitoring (section [sec:monitor]) of process availability    Housekeeping (section [sec:housekeeping]) to ensure log files are\n    tidied up, tickerplant log files are compressed/moved in a timely\n    fashion etc.", 
            "title": "A Large Scale Data Processing Platform"
        }, 
        {
            "location": "/Overview/#do-i-really-have-to-read-this-whole-document", 
            "text": "Hopefully not. The core of AquaQ TorQ is a script called torq.q and we\nhave tried to make it as descriptive as possible, so perhaps that will\nsuffice. The first place to look will be in the config files, the main\none being \\$KDBCONFIG/settings/default.q. This should contain a lot of\ninformation on what can be modified. In addition:    We have added a load of usage information:  aquaq$ q torq.q -usage\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\nGeneral:\n This script should form the basis of a production kdb+ environment.\n It can be sourced from other files if required, or used as a launch script before loading other files/directories using either -load or -loaddir flags \n... etc ...  If sourcing from another script there are hooks to modify and extend\nthe usage information as required.    We have some pretty extensive logging:  aquaq$ q torq.q -p 9999 -debug\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\n2013.11.05D12:22:42.597500000|aquaq|torq.q_3139_9999|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0\n2013.11.05D12:22:42.597545000|aquaq|torq.q_3139_9999|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0\n2013.11.05D12:22:42.597810000|aquaq|torq.q_3139_9999|INF|init|attempting to read required process parameters proctype,procname from file /torqhome/config/process.csv\n2013.11.05D12:22:42.598081000|aquaq|torq.q_3139_9999|INF|init|read in process parameters of proctype=hdb; procname=hdb1\n2013.11.05D12:22:42.598950000|aquaq|hdb1|INF|fileload|config file /torqhome/config/default.q found\n... etc ...    We have added functionality to find functions or variables defined\n    in the session, and also to search function definitions.  q).api.f`max                                                                                                                                                                                   \nname                | vartype   namespace public descrip              ..\n--------------------| ------------------------------------------------..\nmaxs                | function  .q        1      \"\"                   ..\nmmax                | function  .q        1      \"\"                   ..\n.clients.MAXIDLE    | variable  .clients  0      \"\"                   ..\n.access.MAXSIZE     | variable  .access   0      \"\"                   ..\n.cache.maxsize      | variable  .cache    1      \"The maximum size in ..\n.cache.maxindividual| variable  .cache    1      \"The maximum size in ..\nmax                 | primitive           1      \"\"                   ..\n\nq)first 0!.api.p`.api                                                                                                                                                                          \nname     | `.api.f\nvartype  | `function\nnamespace| `.api\npublic   | 1b\ndescrip  | \"Find a function/variable/table/view in the current process\"\nparams   | \"[string:search string]\"\nreturn   | \"table of matching elements\"\n\nq).api.p`.api                                                                                                                                                                                  \nname        | vartype  namespace public descrip                       ..\n------------| --------------------------------------------------------..\n.api.f      | function .api      1      \"Find a function/variable/tabl..\n.api.p      | function .api      1      \"Find a public function/variab..\n.api.u      | function .api      1      \"Find a non-standard q public ..\n.api.s      | function .api      1      \"Search all function definitio..\n.api.find   | function .api      1      \"Generic method for finding fu..\n.api.search | function .api      1      \"Generic method for searching ..\n.api.add    | function .api      1      \"Add a function to the api des..\n.api.fullapi| function .api      1      \"Return the full function api ..    We have incorporated help.q.  q)help`                                                                                                                                                                                        \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date   time casts\nverbs     | verbs/functions    We have separated and commented all of our config:  aquaq$ head config/default.q \n/- Default configuration - loaded by all processes\n\n/- Process initialisation\n\\d .proc\nloadcommoncode:1b   /- whether to load the common code defined at\n                        /- ${KDBCODE}/common\nloadprocesscode:0b  /- whether to load the process specific code defined at \n                        /- ${KDBCODE}/{process type} \nloadnamecode:0b     /- whether to load the name specific code defined at \n                    /- ${KDBCODE}/{name of process}\nloadhandlers:1b     /- whether to load the message handler code defined at \n                        /- ${KDBCODE}/handlers\nlogroll:1b      /- whether to roll the std out/err logs daily\n... etc ...", 
            "title": "Do I Really Have to Read This Whole Document?"
        }, 
        {
            "location": "/Overview/#operating-system-and-kdb-version", 
            "text": "AquaQ TorQ has been built and tested on the linux and OSX operating\nsystems though as far as we are aware there is nothing that would make\nthis incompatible with Solaris or Windows. It has also been tested with\nkdb+ 3.1 and 2.8. Please report any incompatibilities with other kdb+\nversions or operating systems.", 
            "title": "Operating System and kdb+ Version"
        }, 
        {
            "location": "/Overview/#license", 
            "text": "This code is released under the MIT license.", 
            "title": "License"
        }, 
        {
            "location": "/gettingstarted/", 
            "text": "Getting Started\n\n\nkdb+ is very customisable. Customisations are contained in q scripts (.q\nfiles), which define functions and variables which modify the behaviour\nof a process. Every q process can load a single q script, or a directory\ncontaining q scripts and/or q data files. Hooks are provided to enable\nthe programmer to apply a custom function to each entry point of the\nprocess (.z.p*), to be invoked on the timer (.z.ts) or when a variable\nin the top level namespace is amended (.z.vs). By default none of these\nhooks are implemented.\n\n\nWe provide a codebase and a single main script, torq.q. torq.q is\nessentially a wrapper for bespoke functionality which can load other\nscripts/directories, or can be sourced from other scripts. Whenever\npossible, torq.q should be invoked directly and used to load other\nscripts as required. torq.q will:\n\n\n\n\n\n\nensure the environment is set up correctly;\n\n\n\n\n\n\ndefine some common utility functions (such as logging);\n\n\n\n\n\n\nexecute process management tasks, such as discovering the name and\n    type of the process, and re-directing output to log files;\n\n\n\n\n\n\nload configuration;\n\n\n\n\n\n\nload the shared code based;\n\n\n\n\n\n\nset up the message handlers;\n\n\n\n\n\n\nload any required bespoke scripts.\n\n\n\n\n\n\nThe behavior of torq.q is modified by both command line parameters and\nconfiguration. We have tried to keep as much as possible in\nconfiguration files, but if the parameter either has a global effect on\nthe process or if it is required to be known before the configuration is\nread, then it is a command line parameter.\n\n\n\n\nUsing torq.q\n\n\ntorq.q can be invoked directly from the command line and be set to\nsource a specified file or directory. torq.q requires the 5 environment\nvariables to be set (see section\u00a0envvar). If using a unix\nenvironment, this can be done with the setenv.sh script. To start a\nprocess in the foreground without having to modify any other files (e.g.\nprocess.csv) you need to specify the type and name of the process as\nparameters. An example is below.\n\n\n$ . setenv.sh\n$ q torq.q -debug -proctype testproc -procname test1\n\n\n\nTo load a file, do:\n\n\n$ q torq.q -load myfile.q -debug -proctype testproc -procname test1\n\n\n\nIt can also be sourced from another script. If this is the case, some of\nthe variables can be overridden, and the usage information can be\nmodified or extended. Any variable that has a definition like below can\nbe overridden from the loading script.\n\n\nmyvar:@[value;`myvar;1 2 3]\n\n\n\nThe available command line parameters are:\n\n\n\n\n\n\n\n\nCmd Line Param\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-procname x -proctype y\n\n\nThe process name and process type\n\n\n\n\n\n\n-procfile x\n\n\nThe name of the file to get the process information from\n\n\n\n\n\n\n-load x [y..z]\n\n\nThe files or database directory to load\n\n\n\n\n\n\n-loaddir x [y..z]\n\n\nLoad all .q, .k files in specified directories\n\n\n\n\n\n\n-localtime\n\n\nSets processes running in local time rather than GMT for log messages, timer calls etc. The change is backwards compatible; without -localtime flag the process will print logs etc. in GMT but can also have a different .z.P\n\n\n\n\n\n\n-trap\n\n\nAny errors encountered during initialization when loading external files will be caught and logged, processing will continue\n\n\n\n\n\n\n-stop\n\n\nStop loading the file if an error is encountered but do not exit\n\n\n\n\n\n\n-noredirect\n\n\nDo not redirect std out/std err to a file (useful for debugging)\n\n\n\n\n\n\n-noredirectalias\n\n\nDo not create an alias for the log files (aliases drop any suffix e.g. timestamp suffix)\n\n\n\n\n\n\n-noconfig\n\n\nDo not load configuration\n\n\n\n\n\n\n-nopi\n\n\nReset the definition of .z.pi to the initial value (useful for debugging)\n\n\n\n\n\n\n-debug\n\n\nEquivalent to [-nopi -noredirect]\n\n\n\n\n\n\n-usage\n\n\nPrint usage info and exit\n\n\n\n\n\n\n\n\nIn addition any process variable in a namespace (.*.*) can be\noverridden from the command line. Any value supplied on the command line\nwill take priority over any other predefined value (.e.g. in a\nconfiguration or wrapper). Variable names should be supplied with full\nqualification e.g. -.servers.HOPENTIMEOUT 5000.\n\n\n\n\nEnvironment Variables\n\n\nFive environment variables are required:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKDBCONFIG\n\n\nThe base configuration directory\n\n\n\n\n\n\nKDBCODE\n\n\nThe base code directory\n\n\n\n\n\n\nKDBLOGS\n\n\nWhere standard out/error and usage logs are written\n\n\n\n\n\n\nKDBHTML\n\n\nContains HTML files\n\n\n\n\n\n\nKDBLIB\n\n\nContains supporting library files\n\n\n\n\n\n\n\n\ntorq.q will check for these and exit if they are not set. If torq.q is\nbeing sourced from another script, the required environment variables\ncan be extended by setting .proc.envvars before loading torq.q.\n\n\n\n\nProcess Identification\n\n\nAt the crux of AquaQ TorQ is how processes identify themselves. This is\ndefined by two variables - .proc.proctype and .proc.procname which are\nthe type and name of the process respectively. These two values\ndetermine the code base and configuration loaded, and how they are\nconnected to by other processes. If both of these are not defined, the\nTorQ will attempt to use the port number a process was started on to\ndetermine the code base and configuration loaded.\n\n\nThe most important of these is the proctype. It is up to the user to\ndefine at what level to specify a process type. For example, in a\nproduction environment it would be valid to specify processes of type\n\u201chdb\u201d (historic database) and \u201crdb\u201d (real time database). It would also\nbe valid to segregate a little more granularly based on approximate\nfunctionality, for example \u201chdbEMEA\u201d and \u201chdbAmericas\u201d. The actual\nfunctionality of a process can be defined more specifically, but this\nwill be discussed later. The procname value is used solely for\nidentification purposes. A process can determine its type and name in a\nnumber of ways:\n\n\n\n\n\n\nFrom the process file in the default location of\n    $KDBCONFIG/process.csv;\n\n\n\n\n\n\nFrom the process file defined using the command line parameter\n    -procfile;\n\n\n\n\n\n\nFrom the port number it is started on, by referring to the process\n    file for further process details;\n\n\n\n\n\n\nUsing the command line parameters -proctype and -procname;\n\n\n\n\n\n\nBy defining .proc.proctype and .proc.procname in a script which\n    loads torq.q.\n\n\n\n\n\n\nFor options 4 and 5, both parameters must be defined using that method\nor neither will be used (the values will be read from the process file).\n\n\nFor option 3, TorQ will check the process file for any entries where the\nport matches the port number it has been started on, and deduce it\u2019s\nproctype and procname based on this port number and the corresponding\nhostname entry.\n\n\nThe process file has format as below.\n\n\naquaq$ cat config/process.csv \nhost,port,proctype,procname\naquaq,9997,rdb,rdb_europe_1\naquaq,9998,hdb,hdb_europe_1\naquaq,9999,hdb,hdb_europa_2\n\n\n\nThe process will read the file and try to identify itself based on the\nhost and port it is started on. The host can either be the value\nreturned by .z.h, or the ip address of the server. If the process can\nnot automatically identify itself it will exit, unless proctype and\nprocname were both passed in as command line parameters. If both of\nthese parameters are passed in then default configuration settings will\nbe used.\n\n\n\n\nLogging\n\n\nBy default, each process will redirect output to a standard out log and\na standard error log, and create aliases for them. These will be rolled\nat midnight on a daily basis. They are all written to the $KDBLOGS\ndirectory. The log files created are:\n\n\n\n\n\n\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nout_[procname]_[date].log\n\n\nTimestamped out log\n\n\n\n\n\n\nerr_[procname]_[date].log\n\n\nTimestamped error log\n\n\n\n\n\n\nout_[procname].log\n\n\nAlias to current log log\n\n\n\n\n\n\nerr_[procname].log\n\n\nAlias to current error log\n\n\n\n\n\n\n\n\nThe date suffix can be overridden by modifying the .proc.logtimestamp\nfunction and sourcing torq.q from another script. This could, for\nexample, change the suffixing to a full timestamp.\n\n\n\n\nConfiguration Loading\n\n\nDefault Configuration Loading\n\n\nDefault process configuration is contained in q scripts, and stored in\nthe $KDBCONFIG /settings directory. Each process tries to load all the\nconfiguration it can find and will attempt to load three configuration\nfiles in the below order:-\n\n\n\n\n\n\ndefault.q: default configuration loaded by all processes. In a\n    standard installation this should contain the superset of\n    customisable configuration, including comments;\n\n\n\n\n\n\n[[proctype]]{}.q: configuration for a specific process type;\n\n\n\n\n\n\n[[procname]]{}.q: configuration for a specific named process.\n\n\n\n\n\n\nThe only one which should always be present is default.q. Each of the\nother scripts can contain a subset of the configuration variables, which\nwill override anything loaded previously.\n\n\nApplication Configuration Loading\n\n\nApplication specific configuration can be stored in a user defined\ndirectory and made visible to TorQ by setting the $KDBAPPCONFIG\nenvironment variable. If $KDBAPPCONFIG is set, then TorQ will search\nthe $KDBAPPCONFIG/settings directory and load all configuration it can\nfind. Application configuration will be loaded after all default\nconfiguration in the following order:-\n\n\n\n\n\n\ndefault.q: Application default configuration loaded by all\n    processes.\n\n\n\n\n\n\n[[proctype]]{}.q: Application specific configuration for a\n    specific process type.\n\n\n\n\n\n\n[[procname]]{}.q: Appliction specific configuration for a specific\n    named process.\n\n\n\n\n\n\nAll loaded configuration will override anything loaded previously. None\nof the above scripts are required to be present and can contain a subset\nof the default configuration variables from the default configuration\ndirectory.\n\n\nAll configuration is loaded before code.\n\n\n\n\nCode Loading\n\n\nCode is loaded from the $KDBCODE directory. There is also a common\ncodebase, a codebase for each process type, and a code base for each\nprocess name, contained in the following directories and loaded in this\norder:\n\n\n\n\n\n\n$KDBCODE/common: shared codebase loaded by all processes;\n\n\n\n\n\n\n$KDBCODE/[proctype]: code for a specific process type;\n\n\n\n\n\n\n$KDBCODE/[procname]: code for a specific process name;\n\n\n\n\n\n\nFor any directory loaded, the load order can be specified by adding\norder.txt to the directory. order.txt dictates the order that files in\nthe directory are loaded. If a file is not in order.txt, it will still\nbe loaded but after all the files listed in order.txt have been loaded.\n\n\nAdditional directories can be loaded using the -loaddir command line\nparameter.\n\n\n\n\nInitialization Errors\n\n\nInitialization errors can be handled in different ways. The default\naction is any initialization error causes the process to exit. This is\nto enable fail-fast type conditions, where it is better for a process to\nfail entirely and immediately than to start up in an indeterminate\nstate. This can be overridden with the -trap or -stop command line\nparameters. With -trap, the process will catch the error, log it, and\ncontinue. This is useful if, for example, the error is encountered\nloading a file of stored procedures which may not be invoked and can be\nreloaded later. With -stop the process will halt at the point of the\nerror but will not exit. Both -stop and -trap are useful for debugging.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/gettingstarted/#getting-started", 
            "text": "kdb+ is very customisable. Customisations are contained in q scripts (.q\nfiles), which define functions and variables which modify the behaviour\nof a process. Every q process can load a single q script, or a directory\ncontaining q scripts and/or q data files. Hooks are provided to enable\nthe programmer to apply a custom function to each entry point of the\nprocess (.z.p*), to be invoked on the timer (.z.ts) or when a variable\nin the top level namespace is amended (.z.vs). By default none of these\nhooks are implemented.  We provide a codebase and a single main script, torq.q. torq.q is\nessentially a wrapper for bespoke functionality which can load other\nscripts/directories, or can be sourced from other scripts. Whenever\npossible, torq.q should be invoked directly and used to load other\nscripts as required. torq.q will:    ensure the environment is set up correctly;    define some common utility functions (such as logging);    execute process management tasks, such as discovering the name and\n    type of the process, and re-directing output to log files;    load configuration;    load the shared code based;    set up the message handlers;    load any required bespoke scripts.    The behavior of torq.q is modified by both command line parameters and\nconfiguration. We have tried to keep as much as possible in\nconfiguration files, but if the parameter either has a global effect on\nthe process or if it is required to be known before the configuration is\nread, then it is a command line parameter.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/gettingstarted/#using-torqq", 
            "text": "torq.q can be invoked directly from the command line and be set to\nsource a specified file or directory. torq.q requires the 5 environment\nvariables to be set (see section\u00a0envvar). If using a unix\nenvironment, this can be done with the setenv.sh script. To start a\nprocess in the foreground without having to modify any other files (e.g.\nprocess.csv) you need to specify the type and name of the process as\nparameters. An example is below.  $ . setenv.sh\n$ q torq.q -debug -proctype testproc -procname test1  To load a file, do:  $ q torq.q -load myfile.q -debug -proctype testproc -procname test1  It can also be sourced from another script. If this is the case, some of\nthe variables can be overridden, and the usage information can be\nmodified or extended. Any variable that has a definition like below can\nbe overridden from the loading script.  myvar:@[value;`myvar;1 2 3]  The available command line parameters are:     Cmd Line Param  Description      -procname x -proctype y  The process name and process type    -procfile x  The name of the file to get the process information from    -load x [y..z]  The files or database directory to load    -loaddir x [y..z]  Load all .q, .k files in specified directories    -localtime  Sets processes running in local time rather than GMT for log messages, timer calls etc. The change is backwards compatible; without -localtime flag the process will print logs etc. in GMT but can also have a different .z.P    -trap  Any errors encountered during initialization when loading external files will be caught and logged, processing will continue    -stop  Stop loading the file if an error is encountered but do not exit    -noredirect  Do not redirect std out/std err to a file (useful for debugging)    -noredirectalias  Do not create an alias for the log files (aliases drop any suffix e.g. timestamp suffix)    -noconfig  Do not load configuration    -nopi  Reset the definition of .z.pi to the initial value (useful for debugging)    -debug  Equivalent to [-nopi -noredirect]    -usage  Print usage info and exit     In addition any process variable in a namespace (.*.*) can be\noverridden from the command line. Any value supplied on the command line\nwill take priority over any other predefined value (.e.g. in a\nconfiguration or wrapper). Variable names should be supplied with full\nqualification e.g. -.servers.HOPENTIMEOUT 5000.", 
            "title": "Using torq.q"
        }, 
        {
            "location": "/gettingstarted/#environment-variables", 
            "text": "Five environment variables are required:     Environment Variable  Description      KDBCONFIG  The base configuration directory    KDBCODE  The base code directory    KDBLOGS  Where standard out/error and usage logs are written    KDBHTML  Contains HTML files    KDBLIB  Contains supporting library files     torq.q will check for these and exit if they are not set. If torq.q is\nbeing sourced from another script, the required environment variables\ncan be extended by setting .proc.envvars before loading torq.q.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/gettingstarted/#process-identification", 
            "text": "At the crux of AquaQ TorQ is how processes identify themselves. This is\ndefined by two variables - .proc.proctype and .proc.procname which are\nthe type and name of the process respectively. These two values\ndetermine the code base and configuration loaded, and how they are\nconnected to by other processes. If both of these are not defined, the\nTorQ will attempt to use the port number a process was started on to\ndetermine the code base and configuration loaded.  The most important of these is the proctype. It is up to the user to\ndefine at what level to specify a process type. For example, in a\nproduction environment it would be valid to specify processes of type\n\u201chdb\u201d (historic database) and \u201crdb\u201d (real time database). It would also\nbe valid to segregate a little more granularly based on approximate\nfunctionality, for example \u201chdbEMEA\u201d and \u201chdbAmericas\u201d. The actual\nfunctionality of a process can be defined more specifically, but this\nwill be discussed later. The procname value is used solely for\nidentification purposes. A process can determine its type and name in a\nnumber of ways:    From the process file in the default location of\n    $KDBCONFIG/process.csv;    From the process file defined using the command line parameter\n    -procfile;    From the port number it is started on, by referring to the process\n    file for further process details;    Using the command line parameters -proctype and -procname;    By defining .proc.proctype and .proc.procname in a script which\n    loads torq.q.    For options 4 and 5, both parameters must be defined using that method\nor neither will be used (the values will be read from the process file).  For option 3, TorQ will check the process file for any entries where the\nport matches the port number it has been started on, and deduce it\u2019s\nproctype and procname based on this port number and the corresponding\nhostname entry.  The process file has format as below.  aquaq$ cat config/process.csv \nhost,port,proctype,procname\naquaq,9997,rdb,rdb_europe_1\naquaq,9998,hdb,hdb_europe_1\naquaq,9999,hdb,hdb_europa_2  The process will read the file and try to identify itself based on the\nhost and port it is started on. The host can either be the value\nreturned by .z.h, or the ip address of the server. If the process can\nnot automatically identify itself it will exit, unless proctype and\nprocname were both passed in as command line parameters. If both of\nthese parameters are passed in then default configuration settings will\nbe used.", 
            "title": "Process Identification"
        }, 
        {
            "location": "/gettingstarted/#logging", 
            "text": "By default, each process will redirect output to a standard out log and\na standard error log, and create aliases for them. These will be rolled\nat midnight on a daily basis. They are all written to the $KDBLOGS\ndirectory. The log files created are:     Log File  Description      out_[procname]_[date].log  Timestamped out log    err_[procname]_[date].log  Timestamped error log    out_[procname].log  Alias to current log log    err_[procname].log  Alias to current error log     The date suffix can be overridden by modifying the .proc.logtimestamp\nfunction and sourcing torq.q from another script. This could, for\nexample, change the suffixing to a full timestamp.", 
            "title": "Logging"
        }, 
        {
            "location": "/gettingstarted/#configuration-loading", 
            "text": "", 
            "title": "Configuration Loading"
        }, 
        {
            "location": "/gettingstarted/#default-configuration-loading", 
            "text": "Default process configuration is contained in q scripts, and stored in\nthe $KDBCONFIG /settings directory. Each process tries to load all the\nconfiguration it can find and will attempt to load three configuration\nfiles in the below order:-    default.q: default configuration loaded by all processes. In a\n    standard installation this should contain the superset of\n    customisable configuration, including comments;    [[proctype]]{}.q: configuration for a specific process type;    [[procname]]{}.q: configuration for a specific named process.    The only one which should always be present is default.q. Each of the\nother scripts can contain a subset of the configuration variables, which\nwill override anything loaded previously.", 
            "title": "Default Configuration Loading"
        }, 
        {
            "location": "/gettingstarted/#application-configuration-loading", 
            "text": "Application specific configuration can be stored in a user defined\ndirectory and made visible to TorQ by setting the $KDBAPPCONFIG\nenvironment variable. If $KDBAPPCONFIG is set, then TorQ will search\nthe $KDBAPPCONFIG/settings directory and load all configuration it can\nfind. Application configuration will be loaded after all default\nconfiguration in the following order:-    default.q: Application default configuration loaded by all\n    processes.    [[proctype]]{}.q: Application specific configuration for a\n    specific process type.    [[procname]]{}.q: Appliction specific configuration for a specific\n    named process.    All loaded configuration will override anything loaded previously. None\nof the above scripts are required to be present and can contain a subset\nof the default configuration variables from the default configuration\ndirectory.  All configuration is loaded before code.", 
            "title": "Application Configuration Loading"
        }, 
        {
            "location": "/gettingstarted/#code-loading", 
            "text": "Code is loaded from the $KDBCODE directory. There is also a common\ncodebase, a codebase for each process type, and a code base for each\nprocess name, contained in the following directories and loaded in this\norder:    $KDBCODE/common: shared codebase loaded by all processes;    $KDBCODE/[proctype]: code for a specific process type;    $KDBCODE/[procname]: code for a specific process name;    For any directory loaded, the load order can be specified by adding\norder.txt to the directory. order.txt dictates the order that files in\nthe directory are loaded. If a file is not in order.txt, it will still\nbe loaded but after all the files listed in order.txt have been loaded.  Additional directories can be loaded using the -loaddir command line\nparameter.", 
            "title": "Code Loading"
        }, 
        {
            "location": "/gettingstarted/#initialization-errors", 
            "text": "Initialization errors can be handled in different ways. The default\naction is any initialization error causes the process to exit. This is\nto enable fail-fast type conditions, where it is better for a process to\nfail entirely and immediately than to start up in an indeterminate\nstate. This can be overridden with the -trap or -stop command line\nparameters. With -trap, the process will catch the error, log it, and\ncontinue. This is useful if, for example, the error is encountered\nloading a file of stored procedures which may not be invoked and can be\nreloaded later. With -stop the process will halt at the point of the\nerror but will not exit. Both -stop and -trap are useful for debugging.", 
            "title": "Initialization Errors"
        }, 
        {
            "location": "/utilities/", 
            "text": "Utilities\n\n\nWe have provided several utility scripts, which either implement\ndeveloper aids or standard operations which are useful across processes.\n\n\n\n\napi.q\n\n\nThis provides a mechanism for documenting and publishing\nfunction/variable/table or view definitions within the kdb+ process. It\nprovides a search facility both by name and definition (in the case of\nfunctions). There is also a function for returning the approximate\nmemory usage of each variable in the process in descending order.\n\n\nDefinitions are added using the .api.add function. A variable can be\nmarked as public or private, and given a description, parameter list and\nreturn type. The search functions will return all the values found which\nmatch the pattern irrespective of them having a pre-defined definition.\n\n\nWhether a value is public or private is defined in the definitions\ntable. If not found then by default all values are private, except those\nwhich live in the .q or top level namespace.\n\n\n.api.f is used to find a function, variable, table or view based on a\ncase-insensitive pattern search. If a symbol parameter is supplied, a\nwildcard search of *[suppliedvalue]* is done. If a string is\nsupplied, the value is used as is, meaning other non-wildcard regex\npattern matching can be done.\n\n\n\n    q).api.f`max                                                                                                                                                                                                                    \n    name                | vartype   namespace public descrip             ..\n    --------------------| -----------------------------------------------..\n    maxs                | function  .q        1      \n                  ..\n    mmax                | function  .q        1      \n                  ..\n    .clients.MAXIDLE    | variable  .clients  0      \n                  ..\n    .access.MAXSIZE     | variable  .access   0      \n                  ..\n    .cache.maxsize      | variable  .cache    1      \nThe maximum size in..\n    .cache.maxindividual| variable  .cache    1      \nThe maximum size in..\n    max                 | primitive           1      \n                  ..\n    q).api.f\nmax*\n                                                                                                                                                                                                                  \n    name| vartype   namespace public descrip params return\n    ----| ------------------------------------------------\n    maxs| function  .q        1      \n      \n     \n    \n    max | primitive           1      \n      \n     \n    \n\n\n\n\n\n.api.p is the same as .api.f, but only returns public functions. .api.u\nis as .api.p, but only includes user defined values i.e. it excludes q\nprimitives and values found in the .q, .Q, .h and .o namespaces.\n.api.find is a more general version of .api.f which can be used to do\ncase sensitive searches.\n\n\n.api.s is used to search function definitions for specific values.\n\n\nq).api.s\"*max*\"                                                                                                                                                                                                                 \nfunction            definition                                       ..\n---------------------------------------------------------------------..\n.Q.w                \"k){`used`heap`peak`wmax`mmap`mphy`syms`symw!(.\\\"..\n.clients.cleanup    \"{if[count w0:exec w from`.clients.clients where ..\n.access.validsize   \"{[x;y;z] $[superuser .z.u;x;MAXSIZE\ns:-22!x;x;'\\..\n.servers.getservers \"{[nameortype;lookups;req;autoopen;onlyone]\\n r:$..\n.cache.add          \"{[function;id;status]\\n \\n res:value function;\\n..\n\n\n\n.api.m is used to return the approximate memory usage of variables and\nviews in the process, retrieved using -22!. Views will be re-evaluated\nif required. Use .api.mem[0b] if you do not want to evaluate and\nreturn views.\n\n\nq).api.m[]                                                                                                                                                                                                                      \nvariable          size    sizeMB\n--------------------------------\n.tz.t             1587359 2     \n.help.TXT         15409   0     \n.api.detail       10678   0     \n.proc.usage       3610    0     \n.proc.configusage 1029    0     \n..\n\n\n\n.api.whereami[lambda] can be used to retrieve the name of a function\ngiven its definition. This can be useful in debugging.\n\n\nq)g:{x+y}                                                                                                                                                                                                                                                                     \nq)f:{20 + g[x;10]}                                                                                                                                                                                                                                                            \nq)f[10]                                                                                                                                                                                                                                                                       \n40\nq)f[`a]                                                                                                                                                                                                                                                                       \n{x+y}\n`type\n+\n`a\n10\nq)).api.whereami[.z.s]                                                                                                                                                                                                                                                        \n`..g\n\n\n\n\n\ntimer.q\n\n\nkdb+ provides a single timer function, .z.ts which is triggered with the\nfrequency specified by -t. We have provided an extension to allow\nmultiple functions to be added to the timer and fired when required. The\nbasic concept is that timer functions are registered in a table, with\n.z.ts periodically checking the table and running whichever functions\nare required. This is not a suitable mechanism where very high frequency\ntimers are required (e.g. sub 500ms).\n\n\nThere are two ways a function can be added to a timer- either as a\nrepeating timer, or to fire at a specific time. When a repeating timer\nis specified, there are three options as to how the timer can be\nrescheduled. Assuming that a timer function with period P is scheduled\nto fire at time T0, actually fires at time T1 and finishes at time T2,\nthen\n\n\n\n\n\n\nmode 0 will reschedule for T0+P;\n\n\n\n\n\n\nmode 1 will reschedule for T1+P;\n\n\n\n\n\n\nmode 2 will reschedule for T2+P.\n\n\n\n\n\n\nBoth mode 0 and mode 1 have the potential for causing the timer to back\nup if the finish time T2 is after the next schedule time. See\n.api.p\u201c.timer.*\u201dfor more details.\n\n\n\n\nasync.q\n\n\nkdb+ processes can communicate with each using either synchronous or\nasynchronous calls. Synchronous calls expect a response and so the\nserver must process the request when it is received to generate the\nresult and return it to the waiting client. Asynchronous calls do not\nexpect a response so allow for greater flexibility. The effect of\nsynchronous calls can be replicated with asynchronous calls in one of\ntwo ways (further details in section\u00a0gateway):\n\n\n\n\n\n\ndeferred synchronous: the client sends an async request, then blocks\n    on the handle waiting for the result. This allows the server more\n    flexibility as to how and when the query is processed;\n\n\n\n\n\n\nasynchronous postback: the client sends an async request which is\n      wrapped in a function to be posted back to the client when the\n      result is ready. This allows the server flexibility as to how and\n      when the query is processed, and allows the client to continue\n      processing while the server is generating the result.\n\n\n\n\n\n\nThe code for both of these can get a little tricky, largely due to the\namount of error trapping required. We have provided two functions to\nallow these methods to be used more easily. .async.deferred takes a list\nof handles and a query, and will return a two item list of\n(success;results).\n\n\nq).async.deferred[3 5;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                     \n1    1   \n9995 9996\nq).async.deferred[3 5;({x+y};1;2)]                                                                                                                                                                                                          \n1 1\n3 3\nq).async.deferred[3 5;({x+y};1;`a)]                                                                                                                                                                                                         \n0                         0                        \n\"error: server fail:type\" \"error: server fail:type\"\nq).async.deferred[3 5 87;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                  \n1     1     0                                       \n9995i 9996i \"error: comm fail: failed to send query\"\n\n\n\n.async.postback takes a list of handles, a query, and the name or lambda\nof the postback function to return the result to. It will immediately\nreturn a success vector, and the results will be posted back to the\nclient when ready.\n\n\nq).async.postback[3 5;({system\"sleep 1\";system\"p\"};());`showresult]                                                                                                                                                                         \n11b\nq)                                                                                                                                                                                                                                          \nq)9995i\n9996i\n\nq).async.postback[3 5;({x+y};1;2);`showresult]                                                                                                                                                                                              \n11b\nq)3\n3\n\nq).async.postback[3 5;({x+y};1;`a);`showresult]                                                                                                                                                                                             \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5;({x+y};1;`a);showresult]                                                                                                                                                                                              \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5 87;({x+y};1;2);showresult]                                                                                                                                                                                            \n110b\nq)3\n3\n\n\n\nFor more details, see .api.p\u201c.async.*\u201d.\n\n\n\n\ncache.q\n\n\ncache.q provides a mechanism for storing function results in a cache and\nreturning them from the cache if they are available and non stale. This\ncan greatly boost performance for frequently run queries.\n\n\nThe result set cache resides in memory and as such takes up space. It is\nup to the programmer to determine which functions are suitable for\ncaching. Likely candidates are those where some or all of the following\nconditions hold:\n\n\n\n\n\n\nthe function is run multiple times with the same parameters (perhaps\n    different clients all want the same result set);\n\n\n\n\n\n\nthe result set changes infrequently or the clients can accept\n      slightly out-of-date values;\n\n\n\n\n\n\nthe result set is not too large and/or is relatively expensive to\n      produce. For example, it does not make sense to cache raw data\n      extracts.\n\n\n\n\n\n\nThe cache has a maximum size and a minimum size for any individual\nresult set, both of which are defined in the configuration file. Size\nchecks are done with -22! which will give an approximation (but\nunderestimate) of the result set size. In the worst case the estimate\ncould be half the size of the actual size.\n\n\nIf a new result set is to be cached, the size is checked. Assuming it\ndoes not exceed the maximum individual size then it is placed in the\ncache. If the new cache size would exceed the maximum allowed space,\nother result sets are evicted from the cache. The current eviction\npolicy is to remove the least recently accessed result sets until the\nrequired space is freed. The cache performance is tracked in a table.\nCache adds, hits, fails, reruns and evictions are monitored.\n\n\nThe main function to use the cache is .cache.execute[function;\nstaletime]. If the function has been executed within the last\nstaletime, then the result is returned from the cache. Otherwise the\nfunction is executed and placed in the cache.\n\n\nThe function is run and the result placed in the cache:\n\n\nq)\\t r:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                     \n2023\nq)r                                                                                                                                                                                                                             \n3\n\n\n\nThe second time round, the result set is returned immediately from the\ncache as we are within the staletime value:\n\n\nq)\\t r1:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                    \n0\nq)r1                                                                                                                                                                                                                            \n3\n\n\n\nIf the time since the last execution is greater than the required stale\ntime, the function is re-run, the cached result is updated, and the\nresult returned:\n\n\nq)\\t r2:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:00]                                                                                                                                                                    \n2008\nq)r2                                                                                                                                                                                                                            \n3\n\n\n\nThe cache performance is tracked:\n\n\nq).cache.getperf[]                                                                                                                                                                                                              \ntime                          id status function                  \n------------------------------------------------------------------\n2013.11.06D12:41:53.103508000 2  add    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:01.647731000 2  hit    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:53.930404000 2  rerun  {system\"sleep 2\"; x+y} 1 2\n\n\n\nSee .api.p.cache.*for more details.\n\n\n\n\nemail.q\n\n\nA library file is provided to allow TorQ processes to send emails using\nan SMTP server. This is a wrapper around the standard libcurl library.\nThe library file is currently available for Windows (32 bit), Linux (32\nand 64 bit) and OSX (32 and 64 bit). The associated q script contains\ntwo main methods for creating a connection and sending emails. The email\nlibrary requires a modification to the path to find the required libs -\nsee the top of email.q for details.\n\n\nThe main connection method .email.connect takes a single dictionary\nparameter and returns 0i for success and -1i for failure.\n\n\n\n\n\n\n\n\nParameter\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nY\n\n\nsymbol\n\n\nURL of mail server e.g. smtp://mail.example.com\n\n\n\n\n\n\nuser\n\n\nY\n\n\nsymbol\n\n\nUsername of user to login as\n\n\n\n\n\n\npassword\n\n\nY\n\n\nsymbol\n\n\nPassword for user\n\n\n\n\n\n\nusessl\n\n\nN\n\n\nboolean\n\n\nConnect using SSL/TLS, defaults to false\n\n\n\n\n\n\nfrom\n\n\nN\n\n\nsymbol\n\n\nEmail from field, defaults to torq@aquaq.co.uk\n\n\n\n\n\n\ndebug\n\n\nN\n\n\ninteger\n\n\nDebug level. 0=no output, 1=normal output, 2=verbose output. Default is 1\n\n\n\n\n\n\n\n\nAn example is:\n\n\nq).email.connect[`url`user`password`from`usessl`debug!(`$\"smtp://mail.example.com:80\";`$\"torquser@aquaq.co.uk\";`hello;`$\"torquser@aquaq.co.uk\";0b;1i)]\n02 Jan 2015 11:45:19   emailConnect: url is set to smtp://mail.example.com:80\n02 Jan 2015 11:45:19   emailConnect: user is set to torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: password is set\n02 Jan 2015 11:45:19   emailConnect: from is set torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: trying to connect\n02 Jan 2015 11:45:19   emailConnect: connected, socket is 5\n0i\n\n\n\nThe email sending function .email.send takes a single dictionary\nparameter containing the details of the email to send. A connection must\nbe established before an email can be sent. The send function returns an\ninteger of the email length on success, or -1 on failure.\n\n\n\n\n\n\n\n\nParameter\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nto\n\n\nY\n\n\nsymbol (list)\n\n\naddresses to send to\n\n\n\n\n\n\nsubject\n\n\nY\n\n\nchar list\n\n\nemail subject\n\n\n\n\n\n\nbody\n\n\nY\n\n\nlist of char lists\n\n\nemail body\n\n\n\n\n\n\ncc\n\n\nN\n\n\nsymbol (list)\n\n\ncc list\n\n\n\n\n\n\nbodyType\n\n\nN\n\n\nsymbol\n\n\ntype of email body. Can be `text or `html. Default is `text\n\n\n\n\n\n\ndebug\n\n\nN\n\n\ninteger\n\n\nDebug level. 0=no output, 1=normal output,2=verbose output. Default is 1\n\n\n\n\n\n\n\n\nAn example is:\n\n\nq).email.send[`to`subject`body`debug!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\");1i)]\n02 Jan 2015 12:39:29   sending email with subject: test email\n02 Jan 2015 12:39:29   email size in bytes is 16682\n02 Jan 2015 12:39:30   emailSend: email sent\n16682i\n\n\n\nNote that if emails are sent infrequently the library must re-establish\nthe connection to the mail server (this will be done automatically after\nthe initial connection). In some circumstances it may be better to batch\nemails together to send, or to offload email sending to separate\nprocesses as communication with the SMTP server can take a little time.\n\n\nTwo further functions are available, .email.connectdefault and\n.email.senddefault. These are as above but will use the default\nconfiguration defined within the configuration files as the relevant\nparameters passed to the methods. In addition, .email.senddefault will\nautomatically establish a connection.\n\n\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:34.646336000|aquaq||discovery1|INF|email|sending email\n2015.01.02D12:43:35.743887000|aquaq||discovery1|INF|email|connection to mail server successful\n2015.01.02D12:43:37.250427000|aquaq|discovery1|INF|email|email sent\n16673i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n16675i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\");`\"$/home/ashortt/example.txt\")]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n47338i\n\n\n\n.email.test will attempt to establish a connection to the default\nconfigured email server and send a test email to the specified address.\ndebug should be set to 2i (verbose) to extract the full information.\n\n\nq).email.debug:2i\nq).email.test `$\"test@aquaq.co.uk\"\n...\n\n\n\nAdditionally functions are available within the email library. See\n.api.p.email.*for more details.\n\n\nEmails with SSL certificates from Windows\n\n\nIf you wish to send emails via an account which requires authentication\nfrom Windows (e.g. Hotmail, Gmail) then you have to do a few extra steps\nas usessl must be true and Windows does not usually find the correct\ncertificate. The steps are:\n\n\n\n\n\n\ndownload\n    \nthis\n\n    and save it to your PC\n\n\n\n\n\n\nset\n\n\n  CURLOPT_CAINFO=c:/path/to/cabundle_file/ca-bundle.crt\n\n\n\n\n\n\n\nMore information is available\n\nhere\n\nand \nhere\n\n\n\n\ntimezone.q\n\n\nA slightly customised version of the timezone conversion functionality\nfrom code.kx. It loads a table of timezone information from\n$KDBCONFIG. See .api.p.tz.*for more details.\n\n\n\n\ncompress.q\n\n\ncompress.q applies compression to any kdb+ database, handles all\npartition types including date, month, year, int, and can deal with top\nlevel splayed tables. It will also decompress files as required. Once\nthe compression/decompression is complete, summary statistics are\nreturned, with detailed statistics for each compressed or decompressed\nfile held in a table.\n\n\nThe utility is driven by the configuration specified within a csv file.\nDefault parameters can be given, and these can be used to compress all\nfiles within the database. However, the compress.q utility also provides\nthe flexibility to compress different tables with different compression\nparameters, and different columns within tables using different\nparameters. A function is provided which will return a table showing\neach file in the database to be compressed, and how, before the\ncompression is performed.\n\n\nCompression is performed using the -19! operator, which takes 3\nparameters; the compression algorithm to use (0 - none, 1 - kdb+ IPC, 2\n- gzip), the compression blocksize as a power of 2 (between 12 and 19),\n  and the level of compression to apply (from 0 - 9, applicable only for\n  gzip). (For further information on -19! and the parameters used, see\n  code.kx.com.)\n\n\nThe compressionconfig.csv file should have the following format:\n\n\ntable,minage,column,calgo,cblocksize,clevel\ndefault,20,default,2,17,6\ntrades,20,default,1,17,0\nquotes,20,asize,2,17,7\nquotes,20,bsize,2,17,7\n\n\n\nThis file can be placed in the config folder, or a path to the file\ngiven at run time.\n\n\nThe compression utility compresses all tables and columns present in the\nHDB but not specified in the driver file according the default\nparameters. In effect, to compress an entire HDB using the same\ncompression parameters, a single row with name default would suffice. To\nspecify that a particular table should be compressed in a certain\ndifferent manner, it should be listed in the table. If default is given\nas the column for this table, then all of the columns of that table will\nbe compressed accordingly. To specify the compression parameters for\nparticular columns, these should be listed individually. For example,\nthe file above will compress trades tables 20 days old or more with an\nalgorithm of 1, and a blocksize of 17. The asize and bsize columns of\nany quotes tables older than 20 days old will be compressed using\nalgorithm 2, blocksize 17 and level 7. All other files present will be\ncompressed according to the default, using an algorithm 2, blocksize 17\nand compression level 6. To leave files uncompressed, you must specify\nthem explicitly in the table with a calgo of 0. If the file is already\ncompressed, note that an algorithm of 0 will decompress the file.\n\n\nThis utility should be used with caution. Before running the compression\nit is recommended to run the function .cmp.showcomp, which takes three\nparameters - the path to the database, the path to the csv file, and the\nmaximum age of the files to be compressed:\n\n\n.cmp.showcomp[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.showcomp[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file\n\n\n\nThis function produces a table of the files to be compressed, the\nparameters with which they will be compressed, and the current size of\nthe file. Note that the current size column is calculated using hcount;\non a file which is already compressed this returns the uncompressed\nlength, i.e. this cannot be used as a signal as to whether the file is\ncompressed already.\n\n\nfullpath                        column table  partition  age calgo cblocksize clevel compressage currentsize\n-------------------------------------------------------------------------------------\n:/home/hdb/2013.11.05/depth/asize1 asize1 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize2 asize2 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize3 asize3 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/ask1   ask1   depth  2013.11.05 146 0     17         8      1           1575904\n....\n\n\n\nTo then run the compression function, use .cmp.compressmaxage with the\nsame parameters as .cmp.showcomp (hdb path, csv path, maximum age of\nfiles):\n\n\n.cmp.compressmaxage[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.compressmaxage[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file\n\n\n\nTo run compression on all files in the database disregarding the maximum\nage of the files (i.e. from minage as specified in the configuration\nfile to infinitely old), then use:\n\n\n.cmp.docompression[`:/full/path/to/HDB;.cmp.inputcsv]   \n        /- for using the csv file in the config folder\n.cmp.docompression[`:/full/path/to/HDB;`:/full/path/to/csvfile]    \n        /- to specify a file\n\n\n\nLogs are produced for each file which is compressed or decompressed.\nOnce the utility is complete, the statistics of the compression are also\nlogged. This includes the memory savings in MB from compression, the\nadditional memory usage in MB for decompression, the total compression\nratio, and the total decompression ratio:\n\n\n|comp1|INF|compression|Memory savings from compression: 34.48MB. Total compression ratio: 2.51.\n|comp1|INF|compression|Additional memory used from de-compression: 0.00MB. Total de-compression ratio: .\n|comp1|INF|compression|Check .cmp.statstab for info on each file.\n\n\n\nA table with the compressed and decompressed length for each individual\nfile, in descending order of compression ratio, is also produced. This\ncan be found in .cmp.statstab:\n\n\nfile                    algo compressedLength uncompressedLength compressionratio\n-----------------------------------------------------------------------------------\n:/hdb/2014.03.05/depth/asize1 2    89057            772600             8.675343\n:/hdb/2014.01.06/depth/asize1 2    114930           995532             8.662073\n:/hdb/2014.03.05/depth/bsize1 2    89210            772600             8.660464\n:/hdb/2014.03.12/depth/bsize1 2    84416            730928             8.658643\n:/hdb/2014.01.06/depth/bsize1 2    115067           995532             8.651759\n.....\n\n\n\nA note for windows users - windows supports compression only with a\ncompression blocksize of 16 or more.\n\n\n\n\ndataloader.q\n\n\nThis script contains some utility functions to assist in loading data\nfrom delimited files (e.g. comma separated, tab delimited). It is a more\ngeneric version of \nthe data loader example on\ncode.kx\n.\nThe supplied functions allow data to be read in configurable size chunks\nand written out to the database. When all the data is written, the\non-disk data is re-sorted and the attributes are applied. The main\nfunction is .loader.loadalldata which takes two parameters- a dictionary\nof loading parameters and a directory containing the files to read. The\ndictionary should/can have the following fields:\n\n\n\n\n\n\n\n\nParameter\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nheaders\n\n\nY\n\n\nsymbol list\n\n\nNames of the header columns in the file\n\n\n\n\n\n\ntypes\n\n\nY\n\n\nchar list\n\n\nData types to read from the file\n\n\n\n\n\n\nseparator\n\n\nY\n\n\nchar[list]\n\n\nDelimiting character. Enlist it if first line of file is header data\n\n\n\n\n\n\ntablename\n\n\nY\n\n\nsymbol\n\n\nName of table to write data to\n\n\n\n\n\n\ndbdir\n\n\nY\n\n\nsymbol\n\n\nDirectory to write data to\n\n\n\n\n\n\npartitiontype\n\n\nN\n\n\nsymbol\n\n\nPartitioning to use. Must be one of\n\n\n\n\n\n\n`date`month`year`int. Default is `date\n\n\n\n\n\n\n\n\n\n\n\n\npartitioncol\n\n\nN\n\n\nsymbol\n\n\nColumn to use to extract partition information.Default is `time\n\n\n\n\n\n\ndataprocessfunc\n\n\nN\n\n\nfunction\n\n\nDiadic function to process data after it has been read in. First argument is load parameters dictionary, second argument is data which has been read in. Default is {[x;y] y}\n\n\n\n\n\n\nchunksize\n\n\nN\n\n\nint\n\n\nData size in bytes to read in one chunk. Default is 100 MB\n\n\n\n\n\n\ncompression\n\n\nN\n\n\nint list\n\n\nCompression parameters to use e.g. 17 2 6. Default is empty list for no compression\n\n\n\n\n\n\ngc\n\n\nN\n\n\nboolean\n\n\nWhether to run garbage collection at appropriate points. Default is 0b (false)\n\n\n\n\n\n\n\n\nExample usage:\n\n\n.loader.loadallfiles[`headers`types`separator`tablename`dbdir!(`sym`time`price`volume;\"SP  FI\";\",\";`trade;`:hdb); `:TDC/toload]\n.loader.loadallfiles[`headers`types`separator`tablename`dbdir`dataprocessfunc`chunksize`partitiontype`partitioncol`compression`gc!(`sym`time`price`volume;\"SP  FI\";enlist\",\";`tradesummary;`:hdb;{[p;t] select sum size, max price by date:time.date from t};`int$500*2 xexp 20;`month;`date;16 1 0;1b); `:TDC/toload]\n\n\n\n\n\nsubscriptions.q\n\n\nThe subscription utilities allow multiple subscriptions to different\ndata sources to be managed and maintained. Automatic resubscriptions in\nthe event of failure are possible, along as specifying whether the\nprocess will get the schema and replay the log file from the remote\nsource (e.g. in the case of tickerplant subscriptions).\n\n\n.sub.getsubscriptionhandles is used to get a table of processes to\nsubscribe to. The following can be used to return a table of all\nconnected processes of type tickerplant:\n\n\n.sub.getsubscriptionhandles[`tickerplant;`;()!()]\n\n\n\n.sub.subscribe is used to subscribe to a process for the supplied list\nof tables and instruments. For example, to subscribe to instruments A, B\nand C for the quote table from all tickerplants:\n\n\n.sub.subscribe[`trthquote;`A`B;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]\n\n\n\nThe subscription method uses backtick for \u201call\u201d (which is the same as\nkdb+tick). To subscribe to all tables, all instruments, from all\ntickerplants:\n\n\n.sub.subscribe[`;`;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]\n\n\n\nSee .api.p\u201c.sub.*\u201d for more details.\n\n\n\n\npubsub.q\n\n\npubsub.q is essentially a placeholder script to allow publish and\nsubscribe functionality to be implemented. Licenced kdb+tick users can\nuse the publish and subscribe functionality implemented in u.[k|q]. If\nu.[k|q] is placed in the common code directory and loaded before\npubsub.q (make sure u.[k|q] is listed before pubsub.q in order.txt)\nthen publish and subscribe will be implemented. You can also build out\nthis file to add your own publish and subscribe routines as required.\n\n\n\n\ntplogutils.q\n\n\ntplogutils.q contains functions for recovering tickerplant log files.\nUnder certain circumstances the tickerplant log file can become corrupt\nby having an invalid sequence of bytes written to it. A log file can be\nrecovered using a simple recovery method. However, this will only\nrecover messages up to the first invalid message. The recovery functions\ndefined in tplogutils.q allow all valid messages to be recovered from\nthe tickerplant log file.\n\n\n\n\nmonitoringchecks.q\n\n\nmonitoringchecks.q implements a set of standard, basic monitoring\nchecks. They include checks to ensure:\n\n\n\n\n\n\ntable sizes are increasing during live capture\n\n\n\n\n\n\nthe HDB data saves down correctly\n\n\n\n\n\n\nthe allocated memory of a process does not increase past a certain\n      size\n\n\n\n\n\n\nthe size of the symbol list in memory doesn\u2019t grow to big\n\n\n\n\n\n\nthe process does not have too much on its pending subscriber queue\n\n\n\n\n\n\nThese checks are intended to be run by the reporter process on a\nschedule, and any alerts emailed to an appropriate recipient list.\n\n\n\n\nheartbeat.q\n\n\nheartbeat.q implements heartbeating, and relies on both timer.q and\npubsub.q. A table called heartbeat will be published periodically,\nallowing downstream processes to detect the availability of upstream\ncomponents. The heartbeat table contains a heartbeat time and counter.\nThe heartbeat script contains functions to handle and process heartbeats\nand manage upstream process failures. See .api.p.hb.*for details.\n\n\n\n\ndbwriteutils.q\n\n\nThis contains a set of utility functions for writing data to historic\ndatabases.\n\n\nSorting and Attributes\n\n\nThe sort utilities allow the sort order and attributes of tables to be\nglobally defined. This helps to manage the code base when the data can\npotentially be written from multiple locations (e.g. written from the\nRDB, loaded from flat file, replayed from the tickerplant log). The\nconfiguration is defined in a csv which defaults to $KDBCONFG/sort.csv.\nThe default setup is that every table is sorted by sym and time, with a\np attribute on sym (this is the standard kdb+ tick configuration).\n\n\naquaq$ tail config/sort.csv \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1\n\n\n\nAs an example, assume we have an optiontrade table which we want to be\ndifferent from the standard set up. We would like the table to be sorted\nby optionticker and then time, with a p attribute on optionticker. We\nalso have a column called underlyingticker which we can put an attribute\non as it is derived from optionticker (so there is an element of\nde-normalisation present in the table). We also have an exchange field\nwhich we would like to put a g attribute on. All other tables we want to\nbe sorted and parted in the standard way. The configuration file would\nlook like this (sort order is derived from the order within the file\ncombined with the sort flag being set to true):\n\n\naquaq$ tail config/sort.csv                \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1\noptiontrade,p,optionticker,1\noptiontrade,,exchtime,1\noptiontrade,p,underlyingticker,0\noptiontrade,g,exchange,0\n\n\n\nTo invoke the sort utilities, supply a list of (tablename; partitions)\ne.g.\n\n\nq).sort.sorttab(`trthtrade;`:hdb/2014.11.20/trthtrade`:hdb/2014.11.20/trthtrade)\n2014.12.03D09:56:19.214006000|aquaq|test|INF|sort|sorting the trthtrade table\n2014.12.03D09:56:19.214045000|aquaq|test|INF|sorttab|No sort parameters have been specified for : trthtrade. Using default parameters\n2014.12.03D09:56:19.214057000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.19/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.219716000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.19/trthtrade/\n2014.12.03D09:56:19.220846000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.20/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.226008000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.20/trthtrade/\n2014.12.03D09:56:19.226636000|aquaq|test|INF|sort|finished sorting the trthtrade table\n\n\n\nA different sort configuration file can be loaded with\n\n\n.sort.getsortcsv[`:file]\n\n\n\nGarbage Collection\n\n\nThe garbage collection utility prints some debug information before and\nafter the garbage collection.\n\n\nq).gc.run[]                                                                                                                                                      \n2014.12.03D10:22:51.688435000|aquaq|test|INF|garbagecollect|Starting garbage collect. mem stats: used=2 MB; heap=1984 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB\n2014.12.03D10:22:53.920656000|aquaq|test|INF|garbagecollect|Garbage collection returned 1472MB. mem stats: used=2 MB; heap=512 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB\n\n\n\nTable Manipulation\n\n\nThe table manipulation utilities allow table manipulation routines to be\ndefined in a single place. This is useful when data can be written from\nmutliple different processes e.g. RDB, WDB, or tickerplant log replay.\nInstead of having to create a separate definition of customised\nmanipulation in each process, it can be done in a single location and\ninvokved in each process.\n\n\n\n\nhelp.q\n\n\nThe standard help.q from code.kx provides help utilities in the console.\nThis should be kept up to date with\n[\ncode.kx\n].\n\n\nq)help`                                                                                                                                                                                                                         \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date \n time casts\nverbs     | verbs/functions\n\n\n\n\n\nhtml.q\n\n\nAn HTML utility has been added to accompany the HTML5 front end for the\nMonitoring process. It includes functions to format dates, tables to csv\nto configure the HTML file to work on the correct process. It is\naccessible from the \n.html\n namespace.\n\n\n\n\nAdditional Utilities\n\n\nThere are some additional user contributed utility scripts available on\ncode.kx which are good candidates for inclusion. These could either be\ndropped into the common code directory, or if not globally applicable\nthen in the code directory for either the process type or name. The full\nset of user contributed code is documented\n\nhere\n.\n\n\n\n\nFull API\n\n\nThe full public api can be found by running\n\n\nq).api.u`                                                                                                                                                                                                                       \nname             | vartype  namespace public descrip                 ..\n-----------------| --------------------------------------------------..\n.proc.createlog  | function .proc     1      \"Create the standard out..\n.proc.rolllogauto| function .proc     1      \"Roll the standard out/e..\n.proc.loadf      | function .proc     1      \"Load the specified file..\n.proc.loaddir    | function .proc     1      \"Load all the .q and .k ..\n.lg.o            | function .lg       1      \"Log to standard out\"   ..\n..\n\n\n\nCombined with the commented configuration file, this should give a good\noverview of the functionality available. A description of the individual\nnamespaces is below- run .api.u namespace*to list the functions.\n\n\n\n\n\n\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n.proc\n\n\nProcess API\n\n\n\n\n\n\n.lg\n\n\nStandard out/error logging API\n\n\n\n\n\n\n.err\n\n\nError throwing API\n\n\n\n\n\n\n.usage\n\n\nUsage logging API\n\n\n\n\n\n\n.access\n\n\nPermissions API\n\n\n\n\n\n\n.clients\n\n\nClient tracking API\n\n\n\n\n\n\n.servers\n\n\nServer tracking API\n\n\n\n\n\n\n.async\n\n\nAsync communication API\n\n\n\n\n\n\n.timer\n\n\nTimer API\n\n\n\n\n\n\n.cache\n\n\nCaching API\n\n\n\n\n\n\n.tz\n\n\nTimezone conversions API\n\n\n\n\n\n\n.checks\n\n\nMonitoring API\n\n\n\n\n\n\n.cmp\n\n\nCompression API\n\n\n\n\n\n\n.ps\n\n\nPublish and Subscribe API\n\n\n\n\n\n\n.hb\n\n\nHeartbeating API\n\n\n\n\n\n\n.loader\n\n\nData Loader API\n\n\n\n\n\n\n.sort\n\n\nData sorting and attribute setting API\n\n\n\n\n\n\n.sub\n\n\nSubscription API\n\n\n\n\n\n\n.gc\n\n\nGarbage Collection API\n\n\n\n\n\n\n.tplog\n\n\nTickerplant Log Replay API\n\n\n\n\n\n\n.api\n\n\nAPI management API\n\n\n\n\n\n\n\n\n\n\nModified u.q\n\n\nStarting in kdb+ v3.4, the new broadcast feature has some performance\nbenefits. It works by serialising a message once before sending it\nasynchronously to a list of subscribers whereas the previous method\nwould serialise it separately for each subscriber. To take advantage of\nthis, we\u2019ve modified u.q. This can be turned off by setting .u.broadcast\nto false. It is enabled by default, but will only override default\npublishing if the kdb+ version being used is 3.4 or after.", 
            "title": "Utilities"
        }, 
        {
            "location": "/utilities/#utilities", 
            "text": "We have provided several utility scripts, which either implement\ndeveloper aids or standard operations which are useful across processes.", 
            "title": "Utilities"
        }, 
        {
            "location": "/utilities/#apiq", 
            "text": "This provides a mechanism for documenting and publishing\nfunction/variable/table or view definitions within the kdb+ process. It\nprovides a search facility both by name and definition (in the case of\nfunctions). There is also a function for returning the approximate\nmemory usage of each variable in the process in descending order.  Definitions are added using the .api.add function. A variable can be\nmarked as public or private, and given a description, parameter list and\nreturn type. The search functions will return all the values found which\nmatch the pattern irrespective of them having a pre-defined definition.  Whether a value is public or private is defined in the definitions\ntable. If not found then by default all values are private, except those\nwhich live in the .q or top level namespace.  .api.f is used to find a function, variable, table or view based on a\ncase-insensitive pattern search. If a symbol parameter is supplied, a\nwildcard search of *[suppliedvalue]* is done. If a string is\nsupplied, the value is used as is, meaning other non-wildcard regex\npattern matching can be done.  \n    q).api.f`max                                                                                                                                                                                                                    \n    name                | vartype   namespace public descrip             ..\n    --------------------| -----------------------------------------------..\n    maxs                | function  .q        1                         ..\n    mmax                | function  .q        1                         ..\n    .clients.MAXIDLE    | variable  .clients  0                         ..\n    .access.MAXSIZE     | variable  .access   0                         ..\n    .cache.maxsize      | variable  .cache    1       The maximum size in..\n    .cache.maxindividual| variable  .cache    1       The maximum size in..\n    max                 | primitive           1                         ..\n    q).api.f max*                                                                                                                                                                                                                   \n    name| vartype   namespace public descrip params return\n    ----| ------------------------------------------------\n    maxs| function  .q        1                        \n    max | primitive           1                          .api.p is the same as .api.f, but only returns public functions. .api.u\nis as .api.p, but only includes user defined values i.e. it excludes q\nprimitives and values found in the .q, .Q, .h and .o namespaces.\n.api.find is a more general version of .api.f which can be used to do\ncase sensitive searches.  .api.s is used to search function definitions for specific values.  q).api.s\"*max*\"                                                                                                                                                                                                                 \nfunction            definition                                       ..\n---------------------------------------------------------------------..\n.Q.w                \"k){`used`heap`peak`wmax`mmap`mphy`syms`symw!(.\\\"..\n.clients.cleanup    \"{if[count w0:exec w from`.clients.clients where ..\n.access.validsize   \"{[x;y;z] $[superuser .z.u;x;MAXSIZE s:-22!x;x;'\\..\n.servers.getservers \"{[nameortype;lookups;req;autoopen;onlyone]\\n r:$..\n.cache.add          \"{[function;id;status]\\n \\n res:value function;\\n..  .api.m is used to return the approximate memory usage of variables and\nviews in the process, retrieved using -22!. Views will be re-evaluated\nif required. Use .api.mem[0b] if you do not want to evaluate and\nreturn views.  q).api.m[]                                                                                                                                                                                                                      \nvariable          size    sizeMB\n--------------------------------\n.tz.t             1587359 2     \n.help.TXT         15409   0     \n.api.detail       10678   0     \n.proc.usage       3610    0     \n.proc.configusage 1029    0     \n..  .api.whereami[lambda] can be used to retrieve the name of a function\ngiven its definition. This can be useful in debugging.  q)g:{x+y}                                                                                                                                                                                                                                                                     \nq)f:{20 + g[x;10]}                                                                                                                                                                                                                                                            \nq)f[10]                                                                                                                                                                                                                                                                       \n40\nq)f[`a]                                                                                                                                                                                                                                                                       \n{x+y}\n`type\n+\n`a\n10\nq)).api.whereami[.z.s]                                                                                                                                                                                                                                                        \n`..g", 
            "title": "api.q"
        }, 
        {
            "location": "/utilities/#timerq", 
            "text": "kdb+ provides a single timer function, .z.ts which is triggered with the\nfrequency specified by -t. We have provided an extension to allow\nmultiple functions to be added to the timer and fired when required. The\nbasic concept is that timer functions are registered in a table, with\n.z.ts periodically checking the table and running whichever functions\nare required. This is not a suitable mechanism where very high frequency\ntimers are required (e.g. sub 500ms).  There are two ways a function can be added to a timer- either as a\nrepeating timer, or to fire at a specific time. When a repeating timer\nis specified, there are three options as to how the timer can be\nrescheduled. Assuming that a timer function with period P is scheduled\nto fire at time T0, actually fires at time T1 and finishes at time T2,\nthen    mode 0 will reschedule for T0+P;    mode 1 will reschedule for T1+P;    mode 2 will reschedule for T2+P.    Both mode 0 and mode 1 have the potential for causing the timer to back\nup if the finish time T2 is after the next schedule time. See\n.api.p\u201c.timer.*\u201dfor more details.", 
            "title": "timer.q"
        }, 
        {
            "location": "/utilities/#asyncq", 
            "text": "kdb+ processes can communicate with each using either synchronous or\nasynchronous calls. Synchronous calls expect a response and so the\nserver must process the request when it is received to generate the\nresult and return it to the waiting client. Asynchronous calls do not\nexpect a response so allow for greater flexibility. The effect of\nsynchronous calls can be replicated with asynchronous calls in one of\ntwo ways (further details in section\u00a0gateway):    deferred synchronous: the client sends an async request, then blocks\n    on the handle waiting for the result. This allows the server more\n    flexibility as to how and when the query is processed;    asynchronous postback: the client sends an async request which is\n      wrapped in a function to be posted back to the client when the\n      result is ready. This allows the server flexibility as to how and\n      when the query is processed, and allows the client to continue\n      processing while the server is generating the result.    The code for both of these can get a little tricky, largely due to the\namount of error trapping required. We have provided two functions to\nallow these methods to be used more easily. .async.deferred takes a list\nof handles and a query, and will return a two item list of\n(success;results).  q).async.deferred[3 5;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                     \n1    1   \n9995 9996\nq).async.deferred[3 5;({x+y};1;2)]                                                                                                                                                                                                          \n1 1\n3 3\nq).async.deferred[3 5;({x+y};1;`a)]                                                                                                                                                                                                         \n0                         0                        \n\"error: server fail:type\" \"error: server fail:type\"\nq).async.deferred[3 5 87;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                  \n1     1     0                                       \n9995i 9996i \"error: comm fail: failed to send query\"  .async.postback takes a list of handles, a query, and the name or lambda\nof the postback function to return the result to. It will immediately\nreturn a success vector, and the results will be posted back to the\nclient when ready.  q).async.postback[3 5;({system\"sleep 1\";system\"p\"};());`showresult]                                                                                                                                                                         \n11b\nq)                                                                                                                                                                                                                                          \nq)9995i\n9996i\n\nq).async.postback[3 5;({x+y};1;2);`showresult]                                                                                                                                                                                              \n11b\nq)3\n3\n\nq).async.postback[3 5;({x+y};1;`a);`showresult]                                                                                                                                                                                             \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5;({x+y};1;`a);showresult]                                                                                                                                                                                              \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5 87;({x+y};1;2);showresult]                                                                                                                                                                                            \n110b\nq)3\n3  For more details, see .api.p\u201c.async.*\u201d.", 
            "title": "async.q"
        }, 
        {
            "location": "/utilities/#cacheq", 
            "text": "cache.q provides a mechanism for storing function results in a cache and\nreturning them from the cache if they are available and non stale. This\ncan greatly boost performance for frequently run queries.  The result set cache resides in memory and as such takes up space. It is\nup to the programmer to determine which functions are suitable for\ncaching. Likely candidates are those where some or all of the following\nconditions hold:    the function is run multiple times with the same parameters (perhaps\n    different clients all want the same result set);    the result set changes infrequently or the clients can accept\n      slightly out-of-date values;    the result set is not too large and/or is relatively expensive to\n      produce. For example, it does not make sense to cache raw data\n      extracts.    The cache has a maximum size and a minimum size for any individual\nresult set, both of which are defined in the configuration file. Size\nchecks are done with -22! which will give an approximation (but\nunderestimate) of the result set size. In the worst case the estimate\ncould be half the size of the actual size.  If a new result set is to be cached, the size is checked. Assuming it\ndoes not exceed the maximum individual size then it is placed in the\ncache. If the new cache size would exceed the maximum allowed space,\nother result sets are evicted from the cache. The current eviction\npolicy is to remove the least recently accessed result sets until the\nrequired space is freed. The cache performance is tracked in a table.\nCache adds, hits, fails, reruns and evictions are monitored.  The main function to use the cache is .cache.execute[function;\nstaletime]. If the function has been executed within the last\nstaletime, then the result is returned from the cache. Otherwise the\nfunction is executed and placed in the cache.  The function is run and the result placed in the cache:  q)\\t r:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                     \n2023\nq)r                                                                                                                                                                                                                             \n3  The second time round, the result set is returned immediately from the\ncache as we are within the staletime value:  q)\\t r1:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                    \n0\nq)r1                                                                                                                                                                                                                            \n3  If the time since the last execution is greater than the required stale\ntime, the function is re-run, the cached result is updated, and the\nresult returned:  q)\\t r2:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:00]                                                                                                                                                                    \n2008\nq)r2                                                                                                                                                                                                                            \n3  The cache performance is tracked:  q).cache.getperf[]                                                                                                                                                                                                              \ntime                          id status function                  \n------------------------------------------------------------------\n2013.11.06D12:41:53.103508000 2  add    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:01.647731000 2  hit    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:53.930404000 2  rerun  {system\"sleep 2\"; x+y} 1 2  See .api.p.cache.*for more details.", 
            "title": "cache.q"
        }, 
        {
            "location": "/utilities/#emailq", 
            "text": "A library file is provided to allow TorQ processes to send emails using\nan SMTP server. This is a wrapper around the standard libcurl library.\nThe library file is currently available for Windows (32 bit), Linux (32\nand 64 bit) and OSX (32 and 64 bit). The associated q script contains\ntwo main methods for creating a connection and sending emails. The email\nlibrary requires a modification to the path to find the required libs -\nsee the top of email.q for details.  The main connection method .email.connect takes a single dictionary\nparameter and returns 0i for success and -1i for failure.     Parameter  Req  Type  Description      url  Y  symbol  URL of mail server e.g. smtp://mail.example.com    user  Y  symbol  Username of user to login as    password  Y  symbol  Password for user    usessl  N  boolean  Connect using SSL/TLS, defaults to false    from  N  symbol  Email from field, defaults to torq@aquaq.co.uk    debug  N  integer  Debug level. 0=no output, 1=normal output, 2=verbose output. Default is 1     An example is:  q).email.connect[`url`user`password`from`usessl`debug!(`$\"smtp://mail.example.com:80\";`$\"torquser@aquaq.co.uk\";`hello;`$\"torquser@aquaq.co.uk\";0b;1i)]\n02 Jan 2015 11:45:19   emailConnect: url is set to smtp://mail.example.com:80\n02 Jan 2015 11:45:19   emailConnect: user is set to torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: password is set\n02 Jan 2015 11:45:19   emailConnect: from is set torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: trying to connect\n02 Jan 2015 11:45:19   emailConnect: connected, socket is 5\n0i  The email sending function .email.send takes a single dictionary\nparameter containing the details of the email to send. A connection must\nbe established before an email can be sent. The send function returns an\ninteger of the email length on success, or -1 on failure.     Parameter  Req  Type  Description      to  Y  symbol (list)  addresses to send to    subject  Y  char list  email subject    body  Y  list of char lists  email body    cc  N  symbol (list)  cc list    bodyType  N  symbol  type of email body. Can be `text or `html. Default is `text    debug  N  integer  Debug level. 0=no output, 1=normal output,2=verbose output. Default is 1     An example is:  q).email.send[`to`subject`body`debug!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\");1i)]\n02 Jan 2015 12:39:29   sending email with subject: test email\n02 Jan 2015 12:39:29   email size in bytes is 16682\n02 Jan 2015 12:39:30   emailSend: email sent\n16682i  Note that if emails are sent infrequently the library must re-establish\nthe connection to the mail server (this will be done automatically after\nthe initial connection). In some circumstances it may be better to batch\nemails together to send, or to offload email sending to separate\nprocesses as communication with the SMTP server can take a little time.  Two further functions are available, .email.connectdefault and\n.email.senddefault. These are as above but will use the default\nconfiguration defined within the configuration files as the relevant\nparameters passed to the methods. In addition, .email.senddefault will\nautomatically establish a connection.  q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:34.646336000|aquaq||discovery1|INF|email|sending email\n2015.01.02D12:43:35.743887000|aquaq||discovery1|INF|email|connection to mail server successful\n2015.01.02D12:43:37.250427000|aquaq|discovery1|INF|email|email sent\n16673i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n16675i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\");`\"$/home/ashortt/example.txt\")]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n47338i  .email.test will attempt to establish a connection to the default\nconfigured email server and send a test email to the specified address.\ndebug should be set to 2i (verbose) to extract the full information.  q).email.debug:2i\nq).email.test `$\"test@aquaq.co.uk\"\n...  Additionally functions are available within the email library. See\n.api.p.email.*for more details.", 
            "title": "email.q"
        }, 
        {
            "location": "/utilities/#emails-with-ssl-certificates-from-windows", 
            "text": "If you wish to send emails via an account which requires authentication\nfrom Windows (e.g. Hotmail, Gmail) then you have to do a few extra steps\nas usessl must be true and Windows does not usually find the correct\ncertificate. The steps are:    download\n     this \n    and save it to your PC    set    CURLOPT_CAINFO=c:/path/to/cabundle_file/ca-bundle.crt    More information is available here \nand  here", 
            "title": "Emails with SSL certificates from Windows"
        }, 
        {
            "location": "/utilities/#timezoneq", 
            "text": "A slightly customised version of the timezone conversion functionality\nfrom code.kx. It loads a table of timezone information from\n$KDBCONFIG. See .api.p.tz.*for more details.", 
            "title": "timezone.q"
        }, 
        {
            "location": "/utilities/#compressq", 
            "text": "compress.q applies compression to any kdb+ database, handles all\npartition types including date, month, year, int, and can deal with top\nlevel splayed tables. It will also decompress files as required. Once\nthe compression/decompression is complete, summary statistics are\nreturned, with detailed statistics for each compressed or decompressed\nfile held in a table.  The utility is driven by the configuration specified within a csv file.\nDefault parameters can be given, and these can be used to compress all\nfiles within the database. However, the compress.q utility also provides\nthe flexibility to compress different tables with different compression\nparameters, and different columns within tables using different\nparameters. A function is provided which will return a table showing\neach file in the database to be compressed, and how, before the\ncompression is performed.  Compression is performed using the -19! operator, which takes 3\nparameters; the compression algorithm to use (0 - none, 1 - kdb+ IPC, 2\n- gzip), the compression blocksize as a power of 2 (between 12 and 19),\n  and the level of compression to apply (from 0 - 9, applicable only for\n  gzip). (For further information on -19! and the parameters used, see\n  code.kx.com.)  The compressionconfig.csv file should have the following format:  table,minage,column,calgo,cblocksize,clevel\ndefault,20,default,2,17,6\ntrades,20,default,1,17,0\nquotes,20,asize,2,17,7\nquotes,20,bsize,2,17,7  This file can be placed in the config folder, or a path to the file\ngiven at run time.  The compression utility compresses all tables and columns present in the\nHDB but not specified in the driver file according the default\nparameters. In effect, to compress an entire HDB using the same\ncompression parameters, a single row with name default would suffice. To\nspecify that a particular table should be compressed in a certain\ndifferent manner, it should be listed in the table. If default is given\nas the column for this table, then all of the columns of that table will\nbe compressed accordingly. To specify the compression parameters for\nparticular columns, these should be listed individually. For example,\nthe file above will compress trades tables 20 days old or more with an\nalgorithm of 1, and a blocksize of 17. The asize and bsize columns of\nany quotes tables older than 20 days old will be compressed using\nalgorithm 2, blocksize 17 and level 7. All other files present will be\ncompressed according to the default, using an algorithm 2, blocksize 17\nand compression level 6. To leave files uncompressed, you must specify\nthem explicitly in the table with a calgo of 0. If the file is already\ncompressed, note that an algorithm of 0 will decompress the file.  This utility should be used with caution. Before running the compression\nit is recommended to run the function .cmp.showcomp, which takes three\nparameters - the path to the database, the path to the csv file, and the\nmaximum age of the files to be compressed:  .cmp.showcomp[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.showcomp[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file  This function produces a table of the files to be compressed, the\nparameters with which they will be compressed, and the current size of\nthe file. Note that the current size column is calculated using hcount;\non a file which is already compressed this returns the uncompressed\nlength, i.e. this cannot be used as a signal as to whether the file is\ncompressed already.  fullpath                        column table  partition  age calgo cblocksize clevel compressage currentsize\n-------------------------------------------------------------------------------------\n:/home/hdb/2013.11.05/depth/asize1 asize1 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize2 asize2 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize3 asize3 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/ask1   ask1   depth  2013.11.05 146 0     17         8      1           1575904\n....  To then run the compression function, use .cmp.compressmaxage with the\nsame parameters as .cmp.showcomp (hdb path, csv path, maximum age of\nfiles):  .cmp.compressmaxage[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.compressmaxage[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file  To run compression on all files in the database disregarding the maximum\nage of the files (i.e. from minage as specified in the configuration\nfile to infinitely old), then use:  .cmp.docompression[`:/full/path/to/HDB;.cmp.inputcsv]   \n        /- for using the csv file in the config folder\n.cmp.docompression[`:/full/path/to/HDB;`:/full/path/to/csvfile]    \n        /- to specify a file  Logs are produced for each file which is compressed or decompressed.\nOnce the utility is complete, the statistics of the compression are also\nlogged. This includes the memory savings in MB from compression, the\nadditional memory usage in MB for decompression, the total compression\nratio, and the total decompression ratio:  |comp1|INF|compression|Memory savings from compression: 34.48MB. Total compression ratio: 2.51.\n|comp1|INF|compression|Additional memory used from de-compression: 0.00MB. Total de-compression ratio: .\n|comp1|INF|compression|Check .cmp.statstab for info on each file.  A table with the compressed and decompressed length for each individual\nfile, in descending order of compression ratio, is also produced. This\ncan be found in .cmp.statstab:  file                    algo compressedLength uncompressedLength compressionratio\n-----------------------------------------------------------------------------------\n:/hdb/2014.03.05/depth/asize1 2    89057            772600             8.675343\n:/hdb/2014.01.06/depth/asize1 2    114930           995532             8.662073\n:/hdb/2014.03.05/depth/bsize1 2    89210            772600             8.660464\n:/hdb/2014.03.12/depth/bsize1 2    84416            730928             8.658643\n:/hdb/2014.01.06/depth/bsize1 2    115067           995532             8.651759\n.....  A note for windows users - windows supports compression only with a\ncompression blocksize of 16 or more.", 
            "title": "compress.q"
        }, 
        {
            "location": "/utilities/#dataloaderq", 
            "text": "This script contains some utility functions to assist in loading data\nfrom delimited files (e.g. comma separated, tab delimited). It is a more\ngeneric version of  the data loader example on\ncode.kx .\nThe supplied functions allow data to be read in configurable size chunks\nand written out to the database. When all the data is written, the\non-disk data is re-sorted and the attributes are applied. The main\nfunction is .loader.loadalldata which takes two parameters- a dictionary\nof loading parameters and a directory containing the files to read. The\ndictionary should/can have the following fields:     Parameter  Req  Type  Description      headers  Y  symbol list  Names of the header columns in the file    types  Y  char list  Data types to read from the file    separator  Y  char[list]  Delimiting character. Enlist it if first line of file is header data    tablename  Y  symbol  Name of table to write data to    dbdir  Y  symbol  Directory to write data to    partitiontype  N  symbol  Partitioning to use. Must be one of    `date`month`year`int. Default is `date       partitioncol  N  symbol  Column to use to extract partition information.Default is `time    dataprocessfunc  N  function  Diadic function to process data after it has been read in. First argument is load parameters dictionary, second argument is data which has been read in. Default is {[x;y] y}    chunksize  N  int  Data size in bytes to read in one chunk. Default is 100 MB    compression  N  int list  Compression parameters to use e.g. 17 2 6. Default is empty list for no compression    gc  N  boolean  Whether to run garbage collection at appropriate points. Default is 0b (false)     Example usage:  .loader.loadallfiles[`headers`types`separator`tablename`dbdir!(`sym`time`price`volume;\"SP  FI\";\",\";`trade;`:hdb); `:TDC/toload]\n.loader.loadallfiles[`headers`types`separator`tablename`dbdir`dataprocessfunc`chunksize`partitiontype`partitioncol`compression`gc!(`sym`time`price`volume;\"SP  FI\";enlist\",\";`tradesummary;`:hdb;{[p;t] select sum size, max price by date:time.date from t};`int$500*2 xexp 20;`month;`date;16 1 0;1b); `:TDC/toload]", 
            "title": "dataloader.q"
        }, 
        {
            "location": "/utilities/#subscriptionsq", 
            "text": "The subscription utilities allow multiple subscriptions to different\ndata sources to be managed and maintained. Automatic resubscriptions in\nthe event of failure are possible, along as specifying whether the\nprocess will get the schema and replay the log file from the remote\nsource (e.g. in the case of tickerplant subscriptions).  .sub.getsubscriptionhandles is used to get a table of processes to\nsubscribe to. The following can be used to return a table of all\nconnected processes of type tickerplant:  .sub.getsubscriptionhandles[`tickerplant;`;()!()]  .sub.subscribe is used to subscribe to a process for the supplied list\nof tables and instruments. For example, to subscribe to instruments A, B\nand C for the quote table from all tickerplants:  .sub.subscribe[`trthquote;`A`B;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]  The subscription method uses backtick for \u201call\u201d (which is the same as\nkdb+tick). To subscribe to all tables, all instruments, from all\ntickerplants:  .sub.subscribe[`;`;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]  See .api.p\u201c.sub.*\u201d for more details.", 
            "title": "subscriptions.q"
        }, 
        {
            "location": "/utilities/#pubsubq", 
            "text": "pubsub.q is essentially a placeholder script to allow publish and\nsubscribe functionality to be implemented. Licenced kdb+tick users can\nuse the publish and subscribe functionality implemented in u.[k|q]. If\nu.[k|q] is placed in the common code directory and loaded before\npubsub.q (make sure u.[k|q] is listed before pubsub.q in order.txt)\nthen publish and subscribe will be implemented. You can also build out\nthis file to add your own publish and subscribe routines as required.", 
            "title": "pubsub.q"
        }, 
        {
            "location": "/utilities/#tplogutilsq", 
            "text": "tplogutils.q contains functions for recovering tickerplant log files.\nUnder certain circumstances the tickerplant log file can become corrupt\nby having an invalid sequence of bytes written to it. A log file can be\nrecovered using a simple recovery method. However, this will only\nrecover messages up to the first invalid message. The recovery functions\ndefined in tplogutils.q allow all valid messages to be recovered from\nthe tickerplant log file.", 
            "title": "tplogutils.q"
        }, 
        {
            "location": "/utilities/#monitoringchecksq", 
            "text": "monitoringchecks.q implements a set of standard, basic monitoring\nchecks. They include checks to ensure:    table sizes are increasing during live capture    the HDB data saves down correctly    the allocated memory of a process does not increase past a certain\n      size    the size of the symbol list in memory doesn\u2019t grow to big    the process does not have too much on its pending subscriber queue    These checks are intended to be run by the reporter process on a\nschedule, and any alerts emailed to an appropriate recipient list.", 
            "title": "monitoringchecks.q"
        }, 
        {
            "location": "/utilities/#heartbeatq", 
            "text": "heartbeat.q implements heartbeating, and relies on both timer.q and\npubsub.q. A table called heartbeat will be published periodically,\nallowing downstream processes to detect the availability of upstream\ncomponents. The heartbeat table contains a heartbeat time and counter.\nThe heartbeat script contains functions to handle and process heartbeats\nand manage upstream process failures. See .api.p.hb.*for details.", 
            "title": "heartbeat.q"
        }, 
        {
            "location": "/utilities/#dbwriteutilsq", 
            "text": "This contains a set of utility functions for writing data to historic\ndatabases.", 
            "title": "dbwriteutils.q"
        }, 
        {
            "location": "/utilities/#sorting-and-attributes", 
            "text": "The sort utilities allow the sort order and attributes of tables to be\nglobally defined. This helps to manage the code base when the data can\npotentially be written from multiple locations (e.g. written from the\nRDB, loaded from flat file, replayed from the tickerplant log). The\nconfiguration is defined in a csv which defaults to $KDBCONFG/sort.csv.\nThe default setup is that every table is sorted by sym and time, with a\np attribute on sym (this is the standard kdb+ tick configuration).  aquaq$ tail config/sort.csv \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1  As an example, assume we have an optiontrade table which we want to be\ndifferent from the standard set up. We would like the table to be sorted\nby optionticker and then time, with a p attribute on optionticker. We\nalso have a column called underlyingticker which we can put an attribute\non as it is derived from optionticker (so there is an element of\nde-normalisation present in the table). We also have an exchange field\nwhich we would like to put a g attribute on. All other tables we want to\nbe sorted and parted in the standard way. The configuration file would\nlook like this (sort order is derived from the order within the file\ncombined with the sort flag being set to true):  aquaq$ tail config/sort.csv                \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1\noptiontrade,p,optionticker,1\noptiontrade,,exchtime,1\noptiontrade,p,underlyingticker,0\noptiontrade,g,exchange,0  To invoke the sort utilities, supply a list of (tablename; partitions)\ne.g.  q).sort.sorttab(`trthtrade;`:hdb/2014.11.20/trthtrade`:hdb/2014.11.20/trthtrade)\n2014.12.03D09:56:19.214006000|aquaq|test|INF|sort|sorting the trthtrade table\n2014.12.03D09:56:19.214045000|aquaq|test|INF|sorttab|No sort parameters have been specified for : trthtrade. Using default parameters\n2014.12.03D09:56:19.214057000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.19/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.219716000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.19/trthtrade/\n2014.12.03D09:56:19.220846000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.20/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.226008000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.20/trthtrade/\n2014.12.03D09:56:19.226636000|aquaq|test|INF|sort|finished sorting the trthtrade table  A different sort configuration file can be loaded with  .sort.getsortcsv[`:file]", 
            "title": "Sorting and Attributes"
        }, 
        {
            "location": "/utilities/#garbage-collection", 
            "text": "The garbage collection utility prints some debug information before and\nafter the garbage collection.  q).gc.run[]                                                                                                                                                      \n2014.12.03D10:22:51.688435000|aquaq|test|INF|garbagecollect|Starting garbage collect. mem stats: used=2 MB; heap=1984 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB\n2014.12.03D10:22:53.920656000|aquaq|test|INF|garbagecollect|Garbage collection returned 1472MB. mem stats: used=2 MB; heap=512 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB", 
            "title": "Garbage Collection"
        }, 
        {
            "location": "/utilities/#table-manipulation", 
            "text": "The table manipulation utilities allow table manipulation routines to be\ndefined in a single place. This is useful when data can be written from\nmutliple different processes e.g. RDB, WDB, or tickerplant log replay.\nInstead of having to create a separate definition of customised\nmanipulation in each process, it can be done in a single location and\ninvokved in each process.", 
            "title": "Table Manipulation"
        }, 
        {
            "location": "/utilities/#helpq", 
            "text": "The standard help.q from code.kx provides help utilities in the console.\nThis should be kept up to date with\n[ code.kx ].  q)help`                                                                                                                                                                                                                         \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date   time casts\nverbs     | verbs/functions", 
            "title": "help.q"
        }, 
        {
            "location": "/utilities/#htmlq", 
            "text": "An HTML utility has been added to accompany the HTML5 front end for the\nMonitoring process. It includes functions to format dates, tables to csv\nto configure the HTML file to work on the correct process. It is\naccessible from the  .html  namespace.", 
            "title": "html.q"
        }, 
        {
            "location": "/utilities/#additional-utilities", 
            "text": "There are some additional user contributed utility scripts available on\ncode.kx which are good candidates for inclusion. These could either be\ndropped into the common code directory, or if not globally applicable\nthen in the code directory for either the process type or name. The full\nset of user contributed code is documented here .", 
            "title": "Additional Utilities"
        }, 
        {
            "location": "/utilities/#full-api", 
            "text": "The full public api can be found by running  q).api.u`                                                                                                                                                                                                                       \nname             | vartype  namespace public descrip                 ..\n-----------------| --------------------------------------------------..\n.proc.createlog  | function .proc     1      \"Create the standard out..\n.proc.rolllogauto| function .proc     1      \"Roll the standard out/e..\n.proc.loadf      | function .proc     1      \"Load the specified file..\n.proc.loaddir    | function .proc     1      \"Load all the .q and .k ..\n.lg.o            | function .lg       1      \"Log to standard out\"   ..\n..  Combined with the commented configuration file, this should give a good\noverview of the functionality available. A description of the individual\nnamespaces is below- run .api.u namespace*to list the functions.     Namespace  Description      .proc  Process API    .lg  Standard out/error logging API    .err  Error throwing API    .usage  Usage logging API    .access  Permissions API    .clients  Client tracking API    .servers  Server tracking API    .async  Async communication API    .timer  Timer API    .cache  Caching API    .tz  Timezone conversions API    .checks  Monitoring API    .cmp  Compression API    .ps  Publish and Subscribe API    .hb  Heartbeating API    .loader  Data Loader API    .sort  Data sorting and attribute setting API    .sub  Subscription API    .gc  Garbage Collection API    .tplog  Tickerplant Log Replay API    .api  API management API", 
            "title": "Full API"
        }, 
        {
            "location": "/utilities/#modified-uq", 
            "text": "Starting in kdb+ v3.4, the new broadcast feature has some performance\nbenefits. It works by serialising a message once before sending it\nasynchronously to a list of subscribers whereas the previous method\nwould serialise it separately for each subscriber. To take advantage of\nthis, we\u2019ve modified u.q. This can be turned off by setting .u.broadcast\nto false. It is enabled by default, but will only override default\npublishing if the kdb+ version being used is 3.4 or after.", 
            "title": "Modified u.q"
        }, 
        {
            "location": "/handlers/", 
            "text": "Message Handlers\n\n\nThere is a separate code directory containing message handler\ncustomizations. This is found at $KDBCODE/handlers. Much of the code is\nderived from Simon Garland\u2019s contributions to\n\ncode.kx\n.\n\n\nEvery external interaction with a process goes through a message\nhandler, and these can be modified to, for example, log or restrict\naccess. Passing through a bespoke function defined in a message handler\nwill add extra processing time and therefore latency to the message. All\nthe customizations we have provided aim to minimise additional latency,\nbut if a bespoke process is latency sensitive then some or all of the\ncustomizations could be switched off. We would argue though that\ngenerally it is better to switch on all the message handler functions\nwhich provide diagnostic information, as for most non-latency sensitive\nprocesses (HDBs, Gateways, some RDBs etc.) the extra information upon\nfailure is worth the cost. The message handlers can be globally switched\noff by setting .proc.loadhandlers to 0b in the configuration file.\n\n\n\n\n\n\n\n\nScript\n\n\nNS\n\n\nDiag\n\n\nFunction\n\n\nModifies\n\n\n\n\n\n\n\n\n\n\nlogusage.q\n\n\n.usage\n\n\nY\n\n\nLog all client interaction to an ascii log file and/or in-memory table. Messages can be logged before and after they are processed. Timer calls are also logged. Exclusion function list can be applied to .z.ps to disable logging of asynchronous real time updates\n\n\npw, po, pg, ps, pc, ws, ph, pp, pi, exit, timer\n\n\n\n\n\n\ncontrolaccess.q\n\n\n.access\n\n\nN\n\n\nRestrict access for set of users/user groups to a list of functions, and from a defined set of servers\n\n\npw, pg, ps, ws, ph, pp, pi\n\n\n\n\n\n\ntrackclients.q\n\n\n.clients\n\n\nY\n\n\nTrack client process details including then number of requests and cumulative data size returned\n\n\npo, pg, ps, ws, pc\n\n\n\n\n\n\ntrackservers.q\n\n\n.servers\n\n\nY\n\n\nDiscover and track server processes including name, type and attribute information. This also contains the core of the code which can be used in conjunction with the discovery service.\n\n\npc, timer\n\n\n\n\n\n\nzpsignore.q\n\n\n.zpsignore\n\n\nN\n\n\nOverride async message handler based on certain message patterns\n\n\nps\n\n\n\n\n\n\nwriteaccess.q\n\n\n.readonly\n\n\nN\n\n\nRestrict client write access to prevent any modification to data in place. Also disables all HTTP access.\n\n\npg, ps, ws, ph, pp\n\n\n\n\n\n\n\n\nEach customization can be turned on or off individually from the\nconfiguration file(s). Each script can be extensively customised using\nthe configuration file. Example customization for logusage.q, taken from\n$KDBCONFIG/settings/default.q is below. Please see default.q for the\nremaining configuration of the other message handler files.\n\n\n/- Configuration used by the usage functions - logging of client interaction\n\\d .usage\nenabled:1b      /- whether the usage logging is enabled\nlogtodisk:1b        /- whether to log to disk or not\nlogtomemory:1b      /- write query logs to memory\nignore:1b       /- check the ignore list for functions to ignore\nignorelist:(`upd;\"upd\") /- the list of functions to ignore in async calls\nflushtime:1D00      /- default value for how long to persist the\n            /- in-memory logs. Set to 0D for no flushing\nsuppressalias:0b    /- whether to suppress the log file alias creation\nlogtimestamp:{[].z.d}   /- function to generate the log file timestamp suffix\nLEVEL:3         /- log level. 0=none;1=errors;2=errors+complete\n            /- queries;3=errors+before a query+after\nlogroll:1b      /- Whether or not to roll the log file\n            /- automatically (on a daily schedule)\n\n\n\n\n\nlogusage.q\n\n\nlogusage.q is probably the most important of the scripts from a\ndiagnostic perspective. It is a modified version of the logusage.q\nscript on code.kx.\n\n\nIn its most verbose mode it will log information to an in-memory table\n(.usage.usage) and an on-disk ASCII file, both before and after every\nclient interaction and function executed on the timer. These choices\nwere made because:\n\n\n\n\n\n\nlogging to memory enables easy interrogation of client interaction;\n\n\n\n\n\n\nlogging to disk allows persistence if the process fails or locks up.\n      ASCII text files allow interrogation using OS tools such as vi, grep\n      or tail;\n\n\n\n\n\n\nlogging before a query ensures any query that adversely effects the\n      process is definitely captured, as well as capturing some state\n      information before the query execution;\n\n\n\n\n\n\nlogging after a query captures the time taken, result set size and\n      resulting state;\n\n\n\n\n\n\nlogging timer calls ensures a full history of what the process is\n      actually doing. Also, timer call performance degradation over time\n      is a common source of problems in kdb+ systems.\n\n\n\n\n\n\nThe following fields are logged in .usage.usage:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntime\n\n\nTime the row was added to the table\n\n\n\n\n\n\nid\n\n\nID of the query. Normally before and complete rows will be consecutive but it might not be the case if the incoming call invokes further external communication\n\n\n\n\n\n\ntimer\n\n\nExecution time. Null for rows with status=b (before)\n\n\n\n\n\n\nzcmd\n\n\n.z handler the query arrived through\n\n\n\n\n\n\nstatus\n\n\nQuery status. One of b, c or e (before, complete, error)\n\n\n\n\n\n\na\n\n\nAddress of sender. .dotz.ipa can be used to convert from the integer format to a hostname\n\n\n\n\n\n\nu\n\n\nUsername of sender\n\n\n\n\n\n\nw\n\n\nHandle of sender\n\n\n\n\n\n\ncmd\n\n\nCommand sent\n\n\n\n\n\n\nmem\n\n\nMemory statistics\n\n\n\n\n\n\nsz\n\n\nSize of result. Null for rows with status of b or e\n\n\n\n\n\n\nerror\n\n\nError message\n\n\n\n\n\n\n\n\n\n\ncontrolaccess.q\n\n\ncontrolaccess.q is used to restrict client access to the process. It is\nmodified version of controlaccess.q from code.kx. The script allows\ncontrol of several aspects:\n\n\n\n\n\n\nthe host/ip address of the servers which are allowed to access the\n    process;\n\n\n\n\n\n\ndefinition of three user groups (default, poweruser and superuser)\n      and the actions each group is allowed to do;\n\n\n\n\n\n\nthe group(s) each user is a member of, and any additional actions an\n      individual user is allowed/disallowed outside of the group\n      permissions;\n\n\n\n\n\n\nthe maximum size of the result set returned to a client.\n\n\n\n\n\n\nThe access restrictions are loaded from csv files. The permissions files\nare stored in $KDBCONFIG/permissions.\n\n\n\n\n\n\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n*_hosts.csv\n\n\nContains hostname and ip address (patterns) for servers which are allowed or disallowed access. If a server is not found in the list, it is disallowed\n\n\n\n\n\n\n*_users.csv\n\n\nContains individual users and the user groups they are are a member of\n\n\n\n\n\n\n*_functions.csv\n\n\nContains individual functions and whether each user group is allowed to execute them. ; separated user list enables functions to be allowed by individual users\n\n\n\n\n\n\n\n\nThe permissions files are loaded using a similar hierarchical approach\nas for the configuration and code loading. Three files can be provided-\ndefault_.csv, [proctype]_.csv, and [procname]_.csv. All of the\nfiles will be loaded, but permissions for the same entity (hostpattern,\nuser, or function) defined in [procname]_.csv will override those in\n[proctype]_.csv which will in turn override [procname]_.csv.\n\n\nWhen a client makes a query which is refused by the permissioning layer,\nan error will be raised and logged in .usuage.usage if it is enabled.\n\n\n\n\ntrackclients.q\n\n\ntrackclients.q is used to track client interaction. It is a slightly\nmodified version of trackclients.q from code.kx, and extends the\nfunctionality to handle interaction with the discovery service.\n\n\nWhenever a client opens a connection to the q process, it will be\nregistered in the .clients.clients table. Various details are logged,\nbut from a diagnostic perspective the most important information are the\nclient details, the number of queries it has run, the last time it ran a\nquery, the number of failed queries and the cumulative size of results\nreturned to it.\n\n\n\n\ntrackservers.q\n\n\ntrackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. It is explained more in section\u00a0connectionmanagement.\n\n\n\n\nzpsignore.q\n\n\nzpsignore.q is used to check incoming async calls for certain patterns\nand to bypass all further message handler checks for messages matching\nthe pattern. This is useful for handling update messages published to a\nprocess from a data source.\n\n\n\n\nwriteaccess.q\n\n\nwriteaccess.q is used to restrict client write access to data within a\nprocess. The script uses the reval function, released in KDB+ 3.3, to\nprevent client queries from modifying any data in place. At present only\nqueries in the form of strings are passed through the reval function.\nAdditonally the script disables any form of HTTP access. If using\nversions of KDB+ prior to 3.3, this feature must be disabled. An attempt\nto use this feature on previous KDB+ versions will result in an error\nand the relevant process exiting.\n\n\npermissions.q\n\n\npermissions.q is used to control client access to a server process. It\nallows:\n\n\n\n\n\n\nAccess control via username/password access, either in combination\n    with the -u/U process flags or in place of them.\n\n\n\n\n\n\nDefinition of user groups, which control variable access.\n\n\n\n\n\n\nDefinition of user roles, which allow control over function\n    execution.\n\n\n\n\n\n\nDeeper control over table subsetting through the use of \u201cvirtual\n    tables\u201d, using enforced where clauses.\n\n\n\n\n\n\nAccess restriction in TorQ can be enabled on all processes, each of\nwhich can then load the default.q in $KDBCONFIG/permissions/, which\nadds users, groups and roles allowing standard operation of TorQ. The\nadmin user and role by default can access all functions, and each of the\nsystem processes has access only to the required system functions.\n\n\nPermissions are enabled or disabled on a per-process basis through\nsetting .pm.enabled as 1b or 0b at process load (set to 0b by default).\nA permissioned process can safely interact with a non-permissioned\nprocess while still controlling access to itself.\n\n\nThe access schema consists of 7 control tables:\n\n\n\n\n\n\n\n\nName\n\n\nDescriptions\n\n\n\n\n\n\n\n\n\n\nUser\n\n\nUsername, locality, encryption type and password hash\n\n\n\n\n\n\nUsergroup\n\n\nUser and their group.\n\n\n\n\n\n\nUserrole\n\n\nUser and role.\n\n\n\n\n\n\nFunctiongroup\n\n\nFunctions and their group\n\n\n\n\n\n\nFunction\n\n\nFunction names, the roles which can access them, and a lambda checking the parameters those roles can use.\n\n\n\n\n\n\nAccess\n\n\nVariable names, the groups which can access them, and the read or write access level.\n\n\n\n\n\n\nVirtualtable\n\n\nVirtual table name, main table name, and the where clause it enforces on access to that table.\n\n\n\n\n\n\n\n\nIn addition to groupinfo and roleinfo tables, which contain the\ngroup/role name and a string describing each group and role. A user can\nbelong to multiple groups, and have multiple roles. In particular the\nschema supports group hierarchy, where a user group can be listed as a\nuser in the group table, and inherit all the permissions from another\nother group, effectively inheriting the second group itself.\n\n\nA user belonging to a group listed in the access table will have the\nspecified level of access (read or write) to that group\u2019s variables,\ne.g.\n\n\n\n\n\n\n\n\nTable\n\n\nGroup\n\n\nLevel\n\n\n\n\n\n\n\n\n\n\nquote\n\n\nheadtrader\n\n\nwrite\n\n\n\n\n\n\ntrade\n\n\njuniortrader\n\n\nread\n\n\n\n\n\n\n\n\nHere, users in headtrader will have write access to the quote table,\nwhile juniortrader group has read access to the trade table. If\nheadtraders have been set to inherit the juniortrader group, they will\nalso have read access to trade. Note that read access is distinct from\nwrite access. Headtraders in this circumstance do not have implicit read\naccess to the quote table. This control is for direct name access only.\nSelects, execs and updates are controlled via the function table, as\nbelow.\n\n\nThe permissions script can be set to have permissive mode enabled with\npermissivemode:1b (disabled by default). When enabled at script loading,\nthis bypasses access checks on variables which are not listed in the\naccess table, effectively auto-whitelisting any variables not listed in\nthe access table for all users, which may be useful in partly restricted\ndevelopment environments.\n\n\nFunction access is controlled through non-hierarchical roles. A user\nattempting to run a named function will have their access checked\nagainst the function table through their role, for example, trying to\nrun a function timedata[syms;bkttype], which selects from a table by a\ntime bucket type bkttype on xbar:\n\n\n\n\n\n\n\n\nFunction\n\n\nRole\n\n\nParam. Check\n\n\n\n\n\n\n\n\n\n\ntimedata\n\n\nquant\n\n\n{1b}\n\n\n\n\n\n\ntimedata\n\n\nnormal user\n\n\n{x[`bkttype] in `hh}\n\n\n\n\n\n\nselect\n\n\nquant\n\n\n{1b}\n\n\n\n\n\n\n\n\nThe parameter check in the third column must be a lambda accepting a\ndictionary of parameters and their values, which can then return a\nboolean if some parameter condition is met. Here, any normal user must\nhave their bucket type as an hour. If they try anything else, the\nfunction is not permitted. This could be extended to restriction to\ncertain syms as well, in this example, the quant can run this function\nwith any parameters. Anything passed to the param. check function\nreturns 1b. A quant having general select access is listed as having\n1b in the param. check.\n\n\nFurther restriction of data can be achieved with virtual tables, via\nwhich users can be restricted to having a certain subset of data from a\nmain table available. To avoid the need to replicate a potentially large\nsubset of a table into a separately-controlled variable, this is done\nthrough pointing to the table under a different name via a where clause,\ne.g.\n\n\n\n\n\n\n\n\nVirtual Table\n\n\nTable\n\n\nWhere Clause\n\n\n\n\n\n\n\n\n\n\ntrade_lse\n\n\ntrade\n\n\n,(in;`src;\u201cL\u201d)\n\n\n\n\n\n\nquote_new\n\n\nquote\n\n\n,(\n;`time;(-;`.z.p;01:00))\n\n\n\n\n\n\n\n\nWhen a select from trade_lse is performed, a select on trade is\nmodified to contain the where clause above. Access to virtual tables can\nbe controlled identically to access to real tables through the access\ntable.\n\n\nIf the process is given the flag \u201c-public 1\u201d, it will run in public\naccess mode. This allows a user to log in without a password and be\ngiven the publicuser role and membership of the public group, which can\nbe configured as any other group or role.\n\n\nThe permissions control has a default size restriction of 2GB, set (as\nbytes) on .pm.maxsize. This is a global restriction and is not affected\nby user permissions.\n\n\nAdding to the groups and roles is handled by the functions:\n\n\nadduser[`user;`locality;`hash type; md5\"password\"]\nremoveuser[`user]\naddgroup[`groupname; \"description\"]\nremovegroup[`groupname]\naddrole[`rolename; \"description\"]\nremoverole[`rolename]\naddtogroup[`user;`groupname]\nremovefromgroup[`user; `groupname]\nassignrole[`user; `rolename]\nunassignrole[`user; `rolename]\naddfunction[`function; `functiongroup]\nremovefunction[`function; `functiongroup]\ngrantaccess[`variable; `groupname; `level]\nrevokeaccess[`variable; `groupname; `level]\ngrantfunction[`function; `rolename; {paramCheckFn}]\nrevokefunction[`function; `rolename]\ncreatevirtualtable[`vtablename; `table; ,(whereclause)]\nremovevirtualtable[`vtablename]\ncloneuser[`user;`newuser;\"password\"]\n\n\n\nwhich are further explained in the script API.\n\n\nPermission control operates identically on the gateway. A user connected\nto the gateway must have access to the gateway, and their roles must\nhave access to the .gw.syncexec or .gw.asyncexec functions.\n\n\nUsage Example\n\n\nTo connect to a permissioned RDB in the TorQ system, a group and role\nfor the user must be established. If the RDB contains the tables trade,\nquote, and depth, and the process contains the functions getdata[syms,\nbkttype,bktsize] and hloc[table], restricted access would be\nconfigured like so:\n\n\n.pm.adduser[`adam;`local;`md5;md5\"pass\"]\n.pm.adduser[`bob;`local;`md5;md5\"pass\"]\n\n.pm.addtogroup[`adam;`fulluser]\n.pm.addtogroup[`bob;`partuser]\n.pm.addtogroup[`fulluser;`partuser]\n.pm.grantaccess[`quote;`fulluser;`read]\n.pm.grantaccess[`trade;`partuser;`read]\n\n.pm.createvirtualtable[`quotenew;`quote;enlist(\n;`time;(-;`.z.p;01:00))]\n.pm.grantaccess[`quotenew;`partuser;`read]\n\n.pm.assignrole[`adam;`toplevel]\n.pm.assignrole[`bob;`lowlevel]\n.pm.grantfunction[`getdata;`toplevel;{1b}]\n.pm.grantfunction[`getdata;`lowlevel;{x[`syms] in `GOOG}]\n.pm.grantfunction[`hloc;`toplevel;{1b}]\n.pm.grantfunction[`hloc;`lowlevel;{x[`table] in `trade}]\n\n\n\nThis provides a system in which Bob can access only the trade table,\nwhile Adam has access to the trade table and quote table (through\ninheritance from Bob\u2019s group). Through a virtual table, if Bob runs\n\u201cselect from quotenew\u201d, he is able to get a table of the last hour of\nquotes. When the system is started in normal mode, there is no IPC\naccess to the depth table, however if the system was started in\npermissive mode, in this case any user who could log in could access\ndepth.\n\n\nAdam can run the getdata function however he wants, and Bob can only run\nit against sym GOOG. Similarly Adam can run hloc against any table, but\nBob can only look at trade with it.\n\n\nAdditionally, any system calls would need to be actively permissioned in\nthe same way, after defining a systemuser role (or expanding the default\nrole in TorQ). The superuser is given global function access by\nassigning them .pm.ALL in the function table, for example a tickerplant\npushing to the RDB would need to have a user and role defined:\n\n\n.pm.adduser[`ticker;`local;`md5;md5\"plant\"]\n.pm.assignrole[`ticker;`tp]\n\n\n\nAnd then grant that role access to the .u.upd function:\n\n\n.pm.grantfunction[`.u.upd;`tp;{1b}]\n\n\n\nAlthough the .u.upd function updates to a table, there is no need to\ngrant direct access to that table.\n\n\nGateway Example\n\n\nThe gateway user will have superuser role by default. The execution of a\nfunction passed through the gateway is checked against the user who sent\nthe call. This should not be modified.\n\n\nWithin the gateway itself, access to target processes can be controlled\nvia the function table. For example, if Adam in the previous example was\nallowed to access only the RDB with .gw.syncexec, you could use:\n\n\n.pm.grantfunction[`.gw.syncexec;`toplevel;{x[`1] in `rdb}]\n\n\n\nSince .gw.syncexec is a projection, the arguments supplied are checked\nin order, with dictionary keys `0`1`2... etc. This could be further\nextended to restrict access to queries with the\n.pm.allowed[user;query] function, which checks permissions of the\ncurrent user as listed on the gateway permission tables:\n\n\n.pm.grantfunction[`.gw.syncexec;`toplevel;\n    {.pm.allowed[.z.u;x[`0]] and x[`1] in `rdb}]\n\n\n\n\n\nDiagnostic Reporting\n\n\nThe message handler modifications provide a wealth of diagnostic\ninformation including:\n\n\n\n\n\n\nthe timings and memory usage for every query run on a process;\n\n\n\n\n\n\nfailed queries;\n\n\n\n\n\n\nclients trying to do things they are not permissioned for;\n\n\n\n\n\n\nthe clients which are querying often and/or regularly extracting\n      large datasets;\n\n\n\n\n\n\nthe number of clients currently connected;\n\n\n\n\n\n\ntimer calls and how long they take.\n\n\n\n\n\n\nAlthough not currently implemented, it would be straightforward to use\nthis information to implement reports on the behaviour of each process\nand the overall health of the system. Similarly it would be\nstraightforward to set up periodic publication to a central repository\nto have a single point for system diagnostic statistics.", 
            "title": "Handlers"
        }, 
        {
            "location": "/handlers/#message-handlers", 
            "text": "There is a separate code directory containing message handler\ncustomizations. This is found at $KDBCODE/handlers. Much of the code is\nderived from Simon Garland\u2019s contributions to code.kx .  Every external interaction with a process goes through a message\nhandler, and these can be modified to, for example, log or restrict\naccess. Passing through a bespoke function defined in a message handler\nwill add extra processing time and therefore latency to the message. All\nthe customizations we have provided aim to minimise additional latency,\nbut if a bespoke process is latency sensitive then some or all of the\ncustomizations could be switched off. We would argue though that\ngenerally it is better to switch on all the message handler functions\nwhich provide diagnostic information, as for most non-latency sensitive\nprocesses (HDBs, Gateways, some RDBs etc.) the extra information upon\nfailure is worth the cost. The message handlers can be globally switched\noff by setting .proc.loadhandlers to 0b in the configuration file.     Script  NS  Diag  Function  Modifies      logusage.q  .usage  Y  Log all client interaction to an ascii log file and/or in-memory table. Messages can be logged before and after they are processed. Timer calls are also logged. Exclusion function list can be applied to .z.ps to disable logging of asynchronous real time updates  pw, po, pg, ps, pc, ws, ph, pp, pi, exit, timer    controlaccess.q  .access  N  Restrict access for set of users/user groups to a list of functions, and from a defined set of servers  pw, pg, ps, ws, ph, pp, pi    trackclients.q  .clients  Y  Track client process details including then number of requests and cumulative data size returned  po, pg, ps, ws, pc    trackservers.q  .servers  Y  Discover and track server processes including name, type and attribute information. This also contains the core of the code which can be used in conjunction with the discovery service.  pc, timer    zpsignore.q  .zpsignore  N  Override async message handler based on certain message patterns  ps    writeaccess.q  .readonly  N  Restrict client write access to prevent any modification to data in place. Also disables all HTTP access.  pg, ps, ws, ph, pp     Each customization can be turned on or off individually from the\nconfiguration file(s). Each script can be extensively customised using\nthe configuration file. Example customization for logusage.q, taken from\n$KDBCONFIG/settings/default.q is below. Please see default.q for the\nremaining configuration of the other message handler files.  /- Configuration used by the usage functions - logging of client interaction\n\\d .usage\nenabled:1b      /- whether the usage logging is enabled\nlogtodisk:1b        /- whether to log to disk or not\nlogtomemory:1b      /- write query logs to memory\nignore:1b       /- check the ignore list for functions to ignore\nignorelist:(`upd;\"upd\") /- the list of functions to ignore in async calls\nflushtime:1D00      /- default value for how long to persist the\n            /- in-memory logs. Set to 0D for no flushing\nsuppressalias:0b    /- whether to suppress the log file alias creation\nlogtimestamp:{[].z.d}   /- function to generate the log file timestamp suffix\nLEVEL:3         /- log level. 0=none;1=errors;2=errors+complete\n            /- queries;3=errors+before a query+after\nlogroll:1b      /- Whether or not to roll the log file\n            /- automatically (on a daily schedule)", 
            "title": "Message Handlers"
        }, 
        {
            "location": "/handlers/#logusageq", 
            "text": "logusage.q is probably the most important of the scripts from a\ndiagnostic perspective. It is a modified version of the logusage.q\nscript on code.kx.  In its most verbose mode it will log information to an in-memory table\n(.usage.usage) and an on-disk ASCII file, both before and after every\nclient interaction and function executed on the timer. These choices\nwere made because:    logging to memory enables easy interrogation of client interaction;    logging to disk allows persistence if the process fails or locks up.\n      ASCII text files allow interrogation using OS tools such as vi, grep\n      or tail;    logging before a query ensures any query that adversely effects the\n      process is definitely captured, as well as capturing some state\n      information before the query execution;    logging after a query captures the time taken, result set size and\n      resulting state;    logging timer calls ensures a full history of what the process is\n      actually doing. Also, timer call performance degradation over time\n      is a common source of problems in kdb+ systems.    The following fields are logged in .usage.usage:     Field  Description      time  Time the row was added to the table    id  ID of the query. Normally before and complete rows will be consecutive but it might not be the case if the incoming call invokes further external communication    timer  Execution time. Null for rows with status=b (before)    zcmd  .z handler the query arrived through    status  Query status. One of b, c or e (before, complete, error)    a  Address of sender. .dotz.ipa can be used to convert from the integer format to a hostname    u  Username of sender    w  Handle of sender    cmd  Command sent    mem  Memory statistics    sz  Size of result. Null for rows with status of b or e    error  Error message", 
            "title": "logusage.q"
        }, 
        {
            "location": "/handlers/#controlaccessq", 
            "text": "controlaccess.q is used to restrict client access to the process. It is\nmodified version of controlaccess.q from code.kx. The script allows\ncontrol of several aspects:    the host/ip address of the servers which are allowed to access the\n    process;    definition of three user groups (default, poweruser and superuser)\n      and the actions each group is allowed to do;    the group(s) each user is a member of, and any additional actions an\n      individual user is allowed/disallowed outside of the group\n      permissions;    the maximum size of the result set returned to a client.    The access restrictions are loaded from csv files. The permissions files\nare stored in $KDBCONFIG/permissions.     File  Description      *_hosts.csv  Contains hostname and ip address (patterns) for servers which are allowed or disallowed access. If a server is not found in the list, it is disallowed    *_users.csv  Contains individual users and the user groups they are are a member of    *_functions.csv  Contains individual functions and whether each user group is allowed to execute them. ; separated user list enables functions to be allowed by individual users     The permissions files are loaded using a similar hierarchical approach\nas for the configuration and code loading. Three files can be provided-\ndefault_.csv, [proctype]_.csv, and [procname]_.csv. All of the\nfiles will be loaded, but permissions for the same entity (hostpattern,\nuser, or function) defined in [procname]_.csv will override those in\n[proctype]_.csv which will in turn override [procname]_.csv.  When a client makes a query which is refused by the permissioning layer,\nan error will be raised and logged in .usuage.usage if it is enabled.", 
            "title": "controlaccess.q"
        }, 
        {
            "location": "/handlers/#trackclientsq", 
            "text": "trackclients.q is used to track client interaction. It is a slightly\nmodified version of trackclients.q from code.kx, and extends the\nfunctionality to handle interaction with the discovery service.  Whenever a client opens a connection to the q process, it will be\nregistered in the .clients.clients table. Various details are logged,\nbut from a diagnostic perspective the most important information are the\nclient details, the number of queries it has run, the last time it ran a\nquery, the number of failed queries and the cumulative size of results\nreturned to it.", 
            "title": "trackclients.q"
        }, 
        {
            "location": "/handlers/#trackserversq", 
            "text": "trackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. It is explained more in section\u00a0connectionmanagement.", 
            "title": "trackservers.q"
        }, 
        {
            "location": "/handlers/#zpsignoreq", 
            "text": "zpsignore.q is used to check incoming async calls for certain patterns\nand to bypass all further message handler checks for messages matching\nthe pattern. This is useful for handling update messages published to a\nprocess from a data source.", 
            "title": "zpsignore.q"
        }, 
        {
            "location": "/handlers/#writeaccessq", 
            "text": "writeaccess.q is used to restrict client write access to data within a\nprocess. The script uses the reval function, released in KDB+ 3.3, to\nprevent client queries from modifying any data in place. At present only\nqueries in the form of strings are passed through the reval function.\nAdditonally the script disables any form of HTTP access. If using\nversions of KDB+ prior to 3.3, this feature must be disabled. An attempt\nto use this feature on previous KDB+ versions will result in an error\nand the relevant process exiting.", 
            "title": "writeaccess.q"
        }, 
        {
            "location": "/handlers/#permissionsq", 
            "text": "permissions.q is used to control client access to a server process. It\nallows:    Access control via username/password access, either in combination\n    with the -u/U process flags or in place of them.    Definition of user groups, which control variable access.    Definition of user roles, which allow control over function\n    execution.    Deeper control over table subsetting through the use of \u201cvirtual\n    tables\u201d, using enforced where clauses.    Access restriction in TorQ can be enabled on all processes, each of\nwhich can then load the default.q in $KDBCONFIG/permissions/, which\nadds users, groups and roles allowing standard operation of TorQ. The\nadmin user and role by default can access all functions, and each of the\nsystem processes has access only to the required system functions.  Permissions are enabled or disabled on a per-process basis through\nsetting .pm.enabled as 1b or 0b at process load (set to 0b by default).\nA permissioned process can safely interact with a non-permissioned\nprocess while still controlling access to itself.  The access schema consists of 7 control tables:     Name  Descriptions      User  Username, locality, encryption type and password hash    Usergroup  User and their group.    Userrole  User and role.    Functiongroup  Functions and their group    Function  Function names, the roles which can access them, and a lambda checking the parameters those roles can use.    Access  Variable names, the groups which can access them, and the read or write access level.    Virtualtable  Virtual table name, main table name, and the where clause it enforces on access to that table.     In addition to groupinfo and roleinfo tables, which contain the\ngroup/role name and a string describing each group and role. A user can\nbelong to multiple groups, and have multiple roles. In particular the\nschema supports group hierarchy, where a user group can be listed as a\nuser in the group table, and inherit all the permissions from another\nother group, effectively inheriting the second group itself.  A user belonging to a group listed in the access table will have the\nspecified level of access (read or write) to that group\u2019s variables,\ne.g.     Table  Group  Level      quote  headtrader  write    trade  juniortrader  read     Here, users in headtrader will have write access to the quote table,\nwhile juniortrader group has read access to the trade table. If\nheadtraders have been set to inherit the juniortrader group, they will\nalso have read access to trade. Note that read access is distinct from\nwrite access. Headtraders in this circumstance do not have implicit read\naccess to the quote table. This control is for direct name access only.\nSelects, execs and updates are controlled via the function table, as\nbelow.  The permissions script can be set to have permissive mode enabled with\npermissivemode:1b (disabled by default). When enabled at script loading,\nthis bypasses access checks on variables which are not listed in the\naccess table, effectively auto-whitelisting any variables not listed in\nthe access table for all users, which may be useful in partly restricted\ndevelopment environments.  Function access is controlled through non-hierarchical roles. A user\nattempting to run a named function will have their access checked\nagainst the function table through their role, for example, trying to\nrun a function timedata[syms;bkttype], which selects from a table by a\ntime bucket type bkttype on xbar:     Function  Role  Param. Check      timedata  quant  {1b}    timedata  normal user  {x[`bkttype] in `hh}    select  quant  {1b}     The parameter check in the third column must be a lambda accepting a\ndictionary of parameters and their values, which can then return a\nboolean if some parameter condition is met. Here, any normal user must\nhave their bucket type as an hour. If they try anything else, the\nfunction is not permitted. This could be extended to restriction to\ncertain syms as well, in this example, the quant can run this function\nwith any parameters. Anything passed to the param. check function\nreturns 1b. A quant having general select access is listed as having\n1b in the param. check.  Further restriction of data can be achieved with virtual tables, via\nwhich users can be restricted to having a certain subset of data from a\nmain table available. To avoid the need to replicate a potentially large\nsubset of a table into a separately-controlled variable, this is done\nthrough pointing to the table under a different name via a where clause,\ne.g.     Virtual Table  Table  Where Clause      trade_lse  trade  ,(in;`src;\u201cL\u201d)    quote_new  quote  ,( ;`time;(-;`.z.p;01:00))     When a select from trade_lse is performed, a select on trade is\nmodified to contain the where clause above. Access to virtual tables can\nbe controlled identically to access to real tables through the access\ntable.  If the process is given the flag \u201c-public 1\u201d, it will run in public\naccess mode. This allows a user to log in without a password and be\ngiven the publicuser role and membership of the public group, which can\nbe configured as any other group or role.  The permissions control has a default size restriction of 2GB, set (as\nbytes) on .pm.maxsize. This is a global restriction and is not affected\nby user permissions.  Adding to the groups and roles is handled by the functions:  adduser[`user;`locality;`hash type; md5\"password\"]\nremoveuser[`user]\naddgroup[`groupname; \"description\"]\nremovegroup[`groupname]\naddrole[`rolename; \"description\"]\nremoverole[`rolename]\naddtogroup[`user;`groupname]\nremovefromgroup[`user; `groupname]\nassignrole[`user; `rolename]\nunassignrole[`user; `rolename]\naddfunction[`function; `functiongroup]\nremovefunction[`function; `functiongroup]\ngrantaccess[`variable; `groupname; `level]\nrevokeaccess[`variable; `groupname; `level]\ngrantfunction[`function; `rolename; {paramCheckFn}]\nrevokefunction[`function; `rolename]\ncreatevirtualtable[`vtablename; `table; ,(whereclause)]\nremovevirtualtable[`vtablename]\ncloneuser[`user;`newuser;\"password\"]  which are further explained in the script API.  Permission control operates identically on the gateway. A user connected\nto the gateway must have access to the gateway, and their roles must\nhave access to the .gw.syncexec or .gw.asyncexec functions.", 
            "title": "permissions.q"
        }, 
        {
            "location": "/handlers/#usage-example", 
            "text": "To connect to a permissioned RDB in the TorQ system, a group and role\nfor the user must be established. If the RDB contains the tables trade,\nquote, and depth, and the process contains the functions getdata[syms,\nbkttype,bktsize] and hloc[table], restricted access would be\nconfigured like so:  .pm.adduser[`adam;`local;`md5;md5\"pass\"]\n.pm.adduser[`bob;`local;`md5;md5\"pass\"]\n\n.pm.addtogroup[`adam;`fulluser]\n.pm.addtogroup[`bob;`partuser]\n.pm.addtogroup[`fulluser;`partuser]\n.pm.grantaccess[`quote;`fulluser;`read]\n.pm.grantaccess[`trade;`partuser;`read]\n\n.pm.createvirtualtable[`quotenew;`quote;enlist( ;`time;(-;`.z.p;01:00))]\n.pm.grantaccess[`quotenew;`partuser;`read]\n\n.pm.assignrole[`adam;`toplevel]\n.pm.assignrole[`bob;`lowlevel]\n.pm.grantfunction[`getdata;`toplevel;{1b}]\n.pm.grantfunction[`getdata;`lowlevel;{x[`syms] in `GOOG}]\n.pm.grantfunction[`hloc;`toplevel;{1b}]\n.pm.grantfunction[`hloc;`lowlevel;{x[`table] in `trade}]  This provides a system in which Bob can access only the trade table,\nwhile Adam has access to the trade table and quote table (through\ninheritance from Bob\u2019s group). Through a virtual table, if Bob runs\n\u201cselect from quotenew\u201d, he is able to get a table of the last hour of\nquotes. When the system is started in normal mode, there is no IPC\naccess to the depth table, however if the system was started in\npermissive mode, in this case any user who could log in could access\ndepth.  Adam can run the getdata function however he wants, and Bob can only run\nit against sym GOOG. Similarly Adam can run hloc against any table, but\nBob can only look at trade with it.  Additionally, any system calls would need to be actively permissioned in\nthe same way, after defining a systemuser role (or expanding the default\nrole in TorQ). The superuser is given global function access by\nassigning them .pm.ALL in the function table, for example a tickerplant\npushing to the RDB would need to have a user and role defined:  .pm.adduser[`ticker;`local;`md5;md5\"plant\"]\n.pm.assignrole[`ticker;`tp]  And then grant that role access to the .u.upd function:  .pm.grantfunction[`.u.upd;`tp;{1b}]  Although the .u.upd function updates to a table, there is no need to\ngrant direct access to that table.", 
            "title": "Usage Example"
        }, 
        {
            "location": "/handlers/#gateway-example", 
            "text": "The gateway user will have superuser role by default. The execution of a\nfunction passed through the gateway is checked against the user who sent\nthe call. This should not be modified.  Within the gateway itself, access to target processes can be controlled\nvia the function table. For example, if Adam in the previous example was\nallowed to access only the RDB with .gw.syncexec, you could use:  .pm.grantfunction[`.gw.syncexec;`toplevel;{x[`1] in `rdb}]  Since .gw.syncexec is a projection, the arguments supplied are checked\nin order, with dictionary keys `0`1`2... etc. This could be further\nextended to restrict access to queries with the\n.pm.allowed[user;query] function, which checks permissions of the\ncurrent user as listed on the gateway permission tables:  .pm.grantfunction[`.gw.syncexec;`toplevel;\n    {.pm.allowed[.z.u;x[`0]] and x[`1] in `rdb}]", 
            "title": "Gateway Example"
        }, 
        {
            "location": "/handlers/#diagnostic-reporting", 
            "text": "The message handler modifications provide a wealth of diagnostic\ninformation including:    the timings and memory usage for every query run on a process;    failed queries;    clients trying to do things they are not permissioned for;    the clients which are querying often and/or regularly extracting\n      large datasets;    the number of clients currently connected;    timer calls and how long they take.    Although not currently implemented, it would be straightforward to use\nthis information to implement reports on the behaviour of each process\nand the overall health of the system. Similarly it would be\nstraightforward to set up periodic publication to a central repository\nto have a single point for system diagnostic statistics.", 
            "title": "Diagnostic Reporting"
        }, 
        {
            "location": "/conn/", 
            "text": "Connection Management\n\n\ntrackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. All the options are described in the default config file. All\nconnections are tracked in the .servers.SERVERS table. When the handle\nis used the count and last query time are updated.\n\n\nq).servers.SERVERS \nprocname     proctype  hpup                            w  hits startp                        lastp                         endp                          attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996    0                                  2014.01.08D11:13:10.583056000                               ()!()                        \ndiscovery2   discovery :aquaq:9995 6  0    2014.01.07D16:44:47.175757000 2014.01.07D16:44:47.174408000                               ()!()                        \nrdb_europe_1 rdb       :aquaq:9998 12 0    2014.01.07D16:46:47.897910000 2014.01.07D16:46:47.892901000 2014.01.07D16:46:44.626293000 `datacentre`country!`essex`uk\nrdb1         rdb       :aquaq:5011 7  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nrdb_europe_1 hdb       :aquaq:9997    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb1         hdb       :aquaq:9999    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb2         hdb       :aquaq:5013 8  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nhdb1         hdb       :aquaq:5012 9  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\n\nq)last .servers.SERVERS \nprocname  | `hdb2\nproctype  | `hdb\nhpup      | `:aquaq:5013\nw         | 8i\nhits      | 0i\nstartp    | 2014.01.08D11:51:01.928045000\nlastp     | 2014.01.08D11:51:01.925078000\nendp      | 0Np\nattributes| `datacentre`country!`essex`uk\n\n\n\nConnections\n\n\nProcesses locate other processes based on their process type. The\nlocation is done either statically using the process.csv file or\ndynamically using a discovery service. It is recommended to use the\ndiscovery service as it allows the process to be notified as new\nprocesses become available.\n\n\nThe main configuration variable is .servers.CONNECTIONS, which dictates\nwhich process type(s) to create connections to. .servers.startup[]\nmust be called to initialise the connections. When connections are\nclosed, the connection table is automatically updated. The process can\nbe set to periodically retry connections.\n\n\nProcess Attributes\n\n\nEach process can report a set of attributes. When process A connects to\nprocess B, process A will try to retrieve the attributes of process B.\nThe attributes are defined by the result of the .proc.getattributes\nfunction, which is by default an empty dictionary. Attributes are used\nto retrieve more detail about the capabilities of each process, rather\nthan relying on the broad brush process type and process name\ncategorization. Attributes can be used for intelligent query routing.\nPotential fields for attributes include:\n\n\n\n\n\n\nrange of data contained in the process;\n\n\n\n\n\n\navailable tables;\n\n\n\n\n\n\ninstrument universe;\n\n\n\n\n\n\nphysical location;\n\n\n\n\n\n\nany other fields of relevance.\n\n\n\n\n\n\nConnection Passwords\n\n\nThe password used by a process to connect to external processes is\nretrieved using the .servers.loadpassword function call. By default,\nthis will read the password from a txt file contained in\n$KDBCONFIG/passwords. A default password can be used, which is\noverridden by one for the process type, which is itself overridden by\none for the process name. For greater security, the\n.servers.loadpassword function should be modified.\n\n\nRetrieving and Using Handles\n\n\nA function .servers.getservers is supplied to return a table of handle\ninformation. .servers.getservers takes five parameters:\n\n\n\n\n\n\ntype-or-name: whether the lookup is to be done by type or name (can\n    be either proctype or procname);\n\n\n\n\n\n\ntypes-or-names: the types or names to retrieve e.g. hdb;\n\n\n\n\n\n\nrequired-attributes: the dictionary of attributes to match on;\n\n\n\n\n\n\nopen-dead-connections: whether to re-open dead connections;\n\n\n\n\n\n\nonly-one: whether we only require one handle. So for example if 3\n      services of the supplied type are registered, and we have an open\n      handle to 1 of them, the open handle will be returned and the others\n      left closed irrespective of the open-dead-connections parameter.\n\n\n\n\n\n\n.servers.getservers will compare the required parameters with the\navailable parameters for each handle. The resulting table will have an\nextra column called attribmatch which can be used to determine how good\na match the service is with the required attributes. attribmatch is a\ndictionary of (required attribute key) ! (Boolean full match;\nintersection of attributes).\n\n\nq).servers.SERVERS \nprocname     proctype  hpup                            w hits startp                        lastp                         endp attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996   0                                  2014.01.08D11:51:01.922390000      ()!()                        \ndiscovery2   discovery :aquaq:9995 6 0    2014.01.08D11:51:01.923812000 2014.01.08D11:51:01.922390000      ()!()                        \nrdb_europe_1 rdb       :aquaq:9998   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb_europe_2 rdb       :aquaq:9997   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb1         rdb       :aquaq:5011 7 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\nhdb3         hdb       :aquaq:5012 9 0    2014.01.08D11:51:38.349472000 2014.01.08D11:51:38.347598000      `datacentre`country!`essex`uk\nhdb2         hdb       :aquaq:5013 8 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\n\n/- pull back hdbs.  Leave the attributes empty\nq).servers.getservers[`proctype;`hdb;()!();1b;f0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk ()!()      \nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk ()!()\n\n/- supply some attributes\nq).servers.getservers[`proctype;`hdb;(enlist`country)!enlist`uk;1b;0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch           \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nq).servers.getservers[`proctype;`hdb;`country`datacentre!`uk`slough;1b;0b]                                                                                                                                                                                                    \nprocname proctype lastp                         w hpup        attributes                    attribmatch                                    \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))\n\n\n\n.servers.getservers will try to automatically re-open connections if\nrequired.\n\n\nq).servers.getservers[`proctype;`rdb;()!();1b;0b] \n2014.01.08D12:01:06.023146000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9998\n2014.01.08D12:01:06.023581000|aquaq|gateway1|INF|conn|connection to :aquaq:9998 failed: hop: Connection refused\n2014.01.08D12:01:06.023597000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9997\n2014.01.08D12:01:06.023872000|aquaq|gateway1|INF|conn|connection to :aquaq:9997 failed: hop: Connection refused\nprocname proctype lastp                         w hpup         attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()\n\n/- If we only require one connection, and we have one open,then it doesn't retry connections\nq).servers.getservers[`proctype;`rdb;()!();1b;1b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()\n\n\n\nThere are two other functions supplied for retrieving server details,\nboth of which are based on .servers.getservers. .servers.gethandlebytype\nreturns a single handle value, .servers.gethpupbytype returns a single\nhost:port value. Both will re-open connections if there are not any\nvalid connections. Both take two parameters:\n\n\n\n\n\n\ntypes: the type to retrieve e.g. hdb;\n\n\n\n\n\n\nselection-algorithm: can be one of any, last or roundrobin.\n\n\n\n\n\n\nConnecting To Non-TorQ Processes\n\n\nConnections to non-torq (external) processes can also be established.\nThis is useful if you wish to integrate TorQ with an existing\ninfrastructure. Any process can connect to external processes, or it can\nbe managed by the discovery service only. Every external process should\nhave a type and name in the same way as TorQ processes, to enable them\nto be located and used as required.\n\n\nNon-TorQ processes need to be listed by default in\n$KDBCONFIG/settings/nontorqprocess.csv. This file has the same format\nas the standard process.csv file. The location of the non-TorQ process\nfile can be adjusted using the .servers.NONTORQPROCESSFILE variable. To\nenable connections, set .servers.TRACKNONTORQPROCESS to 1b.\n\n\nExample of nontorqprocess.csv file:\n\n\nhost,port,proctype,procname\naquaq,5533,hdb,extproc01\naquaq,5577,hdb,extproc02\n\n\n\nManually Adding And Using Connections\n\n\nConnections can also be manually added and used. See .api.p\u201c.servers.*\u201d\nfor details.\n\n\nIPC types\n\n\nIn version kdb+ v3.4, two new IPC connection types were added. These new\ntypes are unix domain sockets and SSL/TLS (tcps). The incoming\nconnections to a proctype can be set by updating .servers.SOCKETTYPE.\n\n\nIn the settings example below, everything that connects to the\ntickerplant will use unix domain sockets.\n\n\n\\d .servers \nSOCKETTYPE:enlist[`tickerplant]!enlist `unix\n\n\n\nAttempting to open a unix domain socket connection to a process which\nhas an older kdb+ version will fail. We allow for processes to fallback\nto tcp if this happens by setting .servers.SOCKETFALLBACK to true. It\nwill not fallback if the connection error message returned is one of the\nfollowing : timeout, access. It will also not fallback for SSL/TLS\n(tcps) due to security concerns.\n\n\nAt the time of writing, using unix domain sockets syntax on windows will\nappear to work whilst it\u2019s actually falling back to tcp in the\nbackground. This can be misleading so we disabled using them on windows.", 
            "title": "Connection Management"
        }, 
        {
            "location": "/conn/#connection-management", 
            "text": "trackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. All the options are described in the default config file. All\nconnections are tracked in the .servers.SERVERS table. When the handle\nis used the count and last query time are updated.  q).servers.SERVERS \nprocname     proctype  hpup                            w  hits startp                        lastp                         endp                          attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996    0                                  2014.01.08D11:13:10.583056000                               ()!()                        \ndiscovery2   discovery :aquaq:9995 6  0    2014.01.07D16:44:47.175757000 2014.01.07D16:44:47.174408000                               ()!()                        \nrdb_europe_1 rdb       :aquaq:9998 12 0    2014.01.07D16:46:47.897910000 2014.01.07D16:46:47.892901000 2014.01.07D16:46:44.626293000 `datacentre`country!`essex`uk\nrdb1         rdb       :aquaq:5011 7  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nrdb_europe_1 hdb       :aquaq:9997    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb1         hdb       :aquaq:9999    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb2         hdb       :aquaq:5013 8  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nhdb1         hdb       :aquaq:5012 9  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\n\nq)last .servers.SERVERS \nprocname  | `hdb2\nproctype  | `hdb\nhpup      | `:aquaq:5013\nw         | 8i\nhits      | 0i\nstartp    | 2014.01.08D11:51:01.928045000\nlastp     | 2014.01.08D11:51:01.925078000\nendp      | 0Np\nattributes| `datacentre`country!`essex`uk", 
            "title": "Connection Management"
        }, 
        {
            "location": "/conn/#connections", 
            "text": "Processes locate other processes based on their process type. The\nlocation is done either statically using the process.csv file or\ndynamically using a discovery service. It is recommended to use the\ndiscovery service as it allows the process to be notified as new\nprocesses become available.  The main configuration variable is .servers.CONNECTIONS, which dictates\nwhich process type(s) to create connections to. .servers.startup[]\nmust be called to initialise the connections. When connections are\nclosed, the connection table is automatically updated. The process can\nbe set to periodically retry connections.", 
            "title": "Connections"
        }, 
        {
            "location": "/conn/#process-attributes", 
            "text": "Each process can report a set of attributes. When process A connects to\nprocess B, process A will try to retrieve the attributes of process B.\nThe attributes are defined by the result of the .proc.getattributes\nfunction, which is by default an empty dictionary. Attributes are used\nto retrieve more detail about the capabilities of each process, rather\nthan relying on the broad brush process type and process name\ncategorization. Attributes can be used for intelligent query routing.\nPotential fields for attributes include:    range of data contained in the process;    available tables;    instrument universe;    physical location;    any other fields of relevance.", 
            "title": "Process Attributes"
        }, 
        {
            "location": "/conn/#connection-passwords", 
            "text": "The password used by a process to connect to external processes is\nretrieved using the .servers.loadpassword function call. By default,\nthis will read the password from a txt file contained in\n$KDBCONFIG/passwords. A default password can be used, which is\noverridden by one for the process type, which is itself overridden by\none for the process name. For greater security, the\n.servers.loadpassword function should be modified.", 
            "title": "Connection Passwords"
        }, 
        {
            "location": "/conn/#retrieving-and-using-handles", 
            "text": "A function .servers.getservers is supplied to return a table of handle\ninformation. .servers.getservers takes five parameters:    type-or-name: whether the lookup is to be done by type or name (can\n    be either proctype or procname);    types-or-names: the types or names to retrieve e.g. hdb;    required-attributes: the dictionary of attributes to match on;    open-dead-connections: whether to re-open dead connections;    only-one: whether we only require one handle. So for example if 3\n      services of the supplied type are registered, and we have an open\n      handle to 1 of them, the open handle will be returned and the others\n      left closed irrespective of the open-dead-connections parameter.    .servers.getservers will compare the required parameters with the\navailable parameters for each handle. The resulting table will have an\nextra column called attribmatch which can be used to determine how good\na match the service is with the required attributes. attribmatch is a\ndictionary of (required attribute key) ! (Boolean full match;\nintersection of attributes).  q).servers.SERVERS \nprocname     proctype  hpup                            w hits startp                        lastp                         endp attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996   0                                  2014.01.08D11:51:01.922390000      ()!()                        \ndiscovery2   discovery :aquaq:9995 6 0    2014.01.08D11:51:01.923812000 2014.01.08D11:51:01.922390000      ()!()                        \nrdb_europe_1 rdb       :aquaq:9998   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb_europe_2 rdb       :aquaq:9997   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb1         rdb       :aquaq:5011 7 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\nhdb3         hdb       :aquaq:5012 9 0    2014.01.08D11:51:38.349472000 2014.01.08D11:51:38.347598000      `datacentre`country!`essex`uk\nhdb2         hdb       :aquaq:5013 8 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\n\n/- pull back hdbs.  Leave the attributes empty\nq).servers.getservers[`proctype;`hdb;()!();1b;f0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk ()!()      \nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk ()!()\n\n/- supply some attributes\nq).servers.getservers[`proctype;`hdb;(enlist`country)!enlist`uk;1b;0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch           \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nq).servers.getservers[`proctype;`hdb;`country`datacentre!`uk`slough;1b;0b]                                                                                                                                                                                                    \nprocname proctype lastp                         w hpup        attributes                    attribmatch                                    \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))  .servers.getservers will try to automatically re-open connections if\nrequired.  q).servers.getservers[`proctype;`rdb;()!();1b;0b] \n2014.01.08D12:01:06.023146000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9998\n2014.01.08D12:01:06.023581000|aquaq|gateway1|INF|conn|connection to :aquaq:9998 failed: hop: Connection refused\n2014.01.08D12:01:06.023597000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9997\n2014.01.08D12:01:06.023872000|aquaq|gateway1|INF|conn|connection to :aquaq:9997 failed: hop: Connection refused\nprocname proctype lastp                         w hpup         attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()\n\n/- If we only require one connection, and we have one open,then it doesn't retry connections\nq).servers.getservers[`proctype;`rdb;()!();1b;1b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()  There are two other functions supplied for retrieving server details,\nboth of which are based on .servers.getservers. .servers.gethandlebytype\nreturns a single handle value, .servers.gethpupbytype returns a single\nhost:port value. Both will re-open connections if there are not any\nvalid connections. Both take two parameters:    types: the type to retrieve e.g. hdb;    selection-algorithm: can be one of any, last or roundrobin.", 
            "title": "Retrieving and Using Handles"
        }, 
        {
            "location": "/conn/#connecting-to-non-torq-processes", 
            "text": "Connections to non-torq (external) processes can also be established.\nThis is useful if you wish to integrate TorQ with an existing\ninfrastructure. Any process can connect to external processes, or it can\nbe managed by the discovery service only. Every external process should\nhave a type and name in the same way as TorQ processes, to enable them\nto be located and used as required.  Non-TorQ processes need to be listed by default in\n$KDBCONFIG/settings/nontorqprocess.csv. This file has the same format\nas the standard process.csv file. The location of the non-TorQ process\nfile can be adjusted using the .servers.NONTORQPROCESSFILE variable. To\nenable connections, set .servers.TRACKNONTORQPROCESS to 1b.  Example of nontorqprocess.csv file:  host,port,proctype,procname\naquaq,5533,hdb,extproc01\naquaq,5577,hdb,extproc02", 
            "title": "Connecting To Non-TorQ Processes"
        }, 
        {
            "location": "/conn/#manually-adding-and-using-connections", 
            "text": "Connections can also be manually added and used. See .api.p\u201c.servers.*\u201d\nfor details.", 
            "title": "Manually Adding And Using Connections"
        }, 
        {
            "location": "/conn/#ipc-types", 
            "text": "In version kdb+ v3.4, two new IPC connection types were added. These new\ntypes are unix domain sockets and SSL/TLS (tcps). The incoming\nconnections to a proctype can be set by updating .servers.SOCKETTYPE.  In the settings example below, everything that connects to the\ntickerplant will use unix domain sockets.  \\d .servers \nSOCKETTYPE:enlist[`tickerplant]!enlist `unix  Attempting to open a unix domain socket connection to a process which\nhas an older kdb+ version will fail. We allow for processes to fallback\nto tcp if this happens by setting .servers.SOCKETFALLBACK to true. It\nwill not fallback if the connection error message returned is one of the\nfollowing : timeout, access. It will also not fallback for SSL/TLS\n(tcps) due to security concerns.  At the time of writing, using unix domain sockets syntax on windows will\nappear to work whilst it\u2019s actually falling back to tcp in the\nbackground. This can be misleading so we disabled using them on windows.", 
            "title": "IPC types"
        }, 
        {
            "location": "/Processes/", 
            "text": "Processes\n\n\nA set of processes is included. These processes build upon AquaQ TorQ,\nproviding specific functionality. All the process scripts are contained\nin $KDBCODE/processes. All processes should have an entry in\n$KDBCONFIG/process.csv. All processes can have any type and name,\nexcept for discovery services which must have a process type of\n\u201cdiscovery\u201d. An example process.csv is:\n\n\naquaq$ cat config/process.csv\nhost,port,proctype,procname\naquaq,9998,rdb,rdb_europe_1\naquaq,9997,hdb,rdb_europe_1aquaq,9999,hdb,hdb1\naquaq,9996,discovery,discovery1\naquaq,9995,discovery,discovery2\naquaq,8000,gateway,gateway1\naquaq,5010,tickerplant,tickerplant1\naquaq,5011,rdb,rdb1\naquaq,5012,hdb,hdb1\naquaq,5013,hdb,hdb2\naquaq,9990,tickerlogreplay,tpreplay1\naquaq,20000,kill,killhdbs\naquaq,20001,monitor,monitor1\naquaq,20002,housekeeping,hk1\n\n\n\n\n\nDiscovery Service\n\n\nOverview\n\n\nProcesses use the discovery service to register their own availability,\nfind other processes (by process type) and subscribe to receive updates\nfor new process availability (by process type). The discovery service\ndoes not manage connections- it simply returns tables of registered\nprocesses, irrespective of their current availability. It is up to each\nindividual process to manage its own connections.\n\n\nThe discovery service uses the process.csv file to make connections to\nprocesses on start up. After start up it is up to each individual\nprocess to attempt connections and register with the discovery service.\nThis is done automatically, depending on the configuration parameters.\nMultiple discovery services can be run in which case each process will\ntry to register and retrieve process details from each discovery process\nit finds in its process.csv file. Discovery services do not replicate\nbetween themselves. A discovery process must have its process type\nlisted as discovery.\n\n\nTo run the discovery service, use a start line such as:\n\n\naquaq $ q torq.q -load code/processes/discovery.q -p 9995\n\n\n\nModify the configuration as required.\n\n\nOperation\n\n\n\n\n\n\nProcesses register with the discovery service.\n\n\n\n\n\n\n\n\nProcesses use the discovery service to locate other processes.\n\n\n\n\n\n\n\n\nWhen new services register, any processes which have registered an\n     interest in that process type are notified.\n\n\n\n\n\n\n\n\nAvailable Processes\n\n\nThe list of available processes can be found in the .servers.SERVERS\ntable.\n\n\nq).servers.SERVERS                                                                                                                                                                                                                                                            \nprocname     proctype        hpup            w  hits startp                        lastp                         endp attributes                                                                   \n-------------------------------------------------------------------------------------\ndiscovery1   discovery       :aquaq:9995     0                                  2014.01.22D17:00:40.947470000      ()!()                                                                        \ndiscovery2   discovery       :aquaq:9996     0                                  2014.01.22D17:00:40.947517000      ()!()                                                                        \nhdb2         hdb             :aquaq:5013     0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \nkilltick     kill            :aquaq:20000    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntpreplay1    tickerlogreplay :aquaq:20002    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntickerplant1 tickerplant     :aquaq:5010  6  0    2014.01.22D17:00:40.967699000 2014.01.22D17:00:40.967698000      ()!()                                                                        \nmonitor1     monitor         :aquaq:20001 9  0    2014.01.22D17:00:40.971344000 2014.01.22D17:00:40.971344000      ()!()                                                                        \nrdb1         rdb             :aquaq:5011  7  0    2014.01.22D17:06:13.032883000 2014.01.22D17:06:13.032883000      `date`tables!(,2014.01.22;`fxquotes`heartbeat`logmsg`quotes`trades)          \nhdb3         hdb             :aquaq:5012  8  0    2014.01.22D17:06:18.647349000 2014.01.22D17:06:18.647349000      `date`tables!(2014.01.13 2014.01.14;`fxquotes`heartbeat`logmsg`quotes`trades)\ngateway1     gateway         :aquaq:5020  10 0    2014.01.22D17:06:32.152836000 2014.01.22D17:06:32.152836000      ()!()\n\n\n\n\n\nGateway\n\n\nA synchronous and asynchronous gateway is provided. The gateway can be\nused for load balancing and/or to join the results of queries across\nheterogeneous servers (e.g. an RDB and HDB). Ideally the gateway should\nonly be used with asynchronous calls. Synchronous calls cause the\ngateway to block so limits the gateway to serving one query at a time\n(although if querying across multiple backend servers the backend\nqueries will be run in parallel). When using asynchronous calls the\nclient can either block and wait for the result (deferred synchronous)\nor post a call back function which the gateway will call back to the\nclient with. With both asynchronous and synchronous queries the backend\nservers to execute queries against are selected using process type. The\ngateway API can be seen by querying .api.p\u201c.gw.*\u201d within a gateway\nprocess.\n\n\n\n\nAsynchronous Behaviour\n\n\nAsynchronous queries allow much greater flexibility. They allow multiple\nqueries to be serviced at once, prioritisation, and queries to be timed\nout. When an asynchronous query is received the following happens:\n\n\n\n\n\n\nthe query is placed in a queue;\n\n\n\n\n\n\nthe list of available servers is retrieved;\n\n\n\n\n\n\nthe queue is prioritised, so those queries with higher priority are\n      serviced first;\n\n\n\n\n\n\nqueries are sent to back end servers as they become available. Once\n      the backend server returns its result, it is given another query;\n\n\n\n\n\n\nwhen all the partial results from the query are returned the results\n      are aggregated and returned to the client. They are either returned\n      directly, or wrapped in a callback and posted back asynchronously to\n      the client.\n\n\n\n\n\n\nThe two main customisable features of the gateway are the selection of\navailable servers (.gw.availableservers) and the queue prioritisation\n(.gw.getnextqueryid). With default configuration, the available servers\nare those servers which are not currently servicing a query from the\ngateway, and the queue priority is a simple FIFO queue. The available\nservers could be extended to handle process attributes, such as the\navailable datasets or the location of the process, and the queue\nprioritisation could be modified to anything required e.g. based on the\nquery itself, the username, host of the client etc.\n\n\nAn asynchronous query can be timed out using a timeout defined by the\nclient. The gateway will periodically check if any client queries have\nnot completed in the alotted time, and return a timeout error to the\nclient. If the query is already running on any backend servers then they\ncannot be timed out other than by using the standard -T flag.\n\n\nSynchronous Behaviour\n\n\nWhen using synchronous queries the gateway can only handle one query at\na time and cannot timeout queries other than with the standard -T flag.\nAll synchronous queries will be immediately dispatched to the back end\nprocesses. They will be dispatched using an asyhcnronous call, allowing\nthem to run in parallel rather than serially. When the results are\nreceived they are aggregated and returned to the client.\n\n\nProcess Discovery\n\n\nThe gateway uses the discovery service to locate processes to query\nacross. The discovery service will notify the gateway when new processes\nbecome available and the gateway will automatically connect and start\nusing them. The gateway can also use the static information in\nprocess.csv, but this limits the gateway to a predefined list of\nprocesses rather than allowing new services to come online as demand\nrequires.\n\n\nError Handling\n\n\nWhen synchronous calls are used, q errors are returned to clients as\nthey are encountered. When using asynchronous calls there is no way to\nreturn actual errors and appropriately prefixed strings must be used\ninstead. It is up to the client to check the type of the received result\nand if it is a string then whether it contains the error prefix. The\nerror prefix can be changed, but the default is \u201cerror: \u201d. Errors will\nbe returned when:\n\n\n\n\n\n\nthe client requests a query against a server type which the gateway\n    does not currently have any active instances of (this error is\n    returned immediately);\n\n\n\n\n\n\nthe query is timed out;\n\n\n\n\n\n\na back end server returns an error;\n\n\n\n\n\n\na back end server fails;\n\n\n\n\n\n\nthe join function fails.\n\n\n\n\n\n\nIf postback functions are used, the error string will be posted back\nwithin the postback function (i.e. it will be packed the same way as a\nvalid result).\n\n\nClient Calls\n\n\nThere are four main client calls. The .gw.sync* methods should only be\ninvoked synchronously, and the .gw.async* methods should only be\ninvoked asynchronously. Each of these are documented more extensively in\nthe gateway api. Use .api.p\u201c.gw.*\u201d for more details.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n.gw.syncexec[query; servertypes]\n\n\nExecute the specified query synchronously against the required list of servers. If more than one server, the results will be razed.\n\n\n\n\n\n\n.gw.syncexecj[query; servertypes; joinfunction]\n\n\nExecute the specified query against the required list of servers. Use the specified join function to aggregate the results.\n\n\n\n\n\n\n.gw.asyncexec[query; servertypes]\n\n\nExecute the specified query against the required list of servers. If more than one server, the results will be razed. The client must block and wait for the results.\n\n\n\n\n\n\n.gw.asyncexecjpt[query; servertypes; joinfunction; postback; timeout]\n\n\nExecute the specified query against the required list of servers. Use the specified join function to aggregate the results. If the postback function is not set, the client must block and wait for the results. If it is set, the result will be wrapped in the specified postback function and returned asynchronously to the client. The query will be timed out if the timeout value is exceeded.\n\n\n\n\n\n\n\n\nFor the purposes of demonstration, assume that the queries must be run\nacross an RDB and HDB process, and the gateway has one RDB and two HDB\nprocesses available to it.\n\n\nq).gw.servers                                                                                                                                                                                                                                                                 \nhandle| servertype inuse active querycount lastquery                     usage                attributes                   \n------| --------------------------------------------------------------------\n7     | rdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:00:52.149069000 `datacentre`country!`essex`uk\n8     | hdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:01:26.143564000 `datacentre`country!`essex`uk\n9     | hdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:08.019862000 `datacentre`country!`essex`uk\n12    | rdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:04.018349000 `datacentre`country!`essex`uk\n\n\n\nBoth the RDB and HDB processes have a function f and table t defined. f\nwill run for 2 seconds longer on the HDB processes then it will the RDB.\n\n\nq)f                                                                                                                                                                                                                                                                           \n{system\"sleep \",string x+$[`hdb=.proc.proctype;2;0]; t}\nq)t                                                                                                                                                                                                                                                                           \na   \n----\n5013\n5014\n5015\n5016\n5017\n\n\n\nRun the gateway. The main parameter which should be set is the\n.servers.CONNECTIONS parameter, which dictates the process types the\ngateway queries across. Also, we need to explicitly allow sync calls. We\ncan do this from the config or from the command line.\n\n\nq torq.q -load code/processes/gateway.q -p 8000 -.gw.synccallsallowed 1 -.servers.CONNECTIONS hdb rdb\n\n\n\nStart a client and connect to the gateway. Start with a sync query. The\nHDB query should take 4 seconds and the RDB query should take 2 seconds.\nIf the queries run in parallel, the total query time should be 4\nseconds.\n\n\nq)h:hopen 8000                                                                                                                                                                                                                                                                \nq)h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                            \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                         \n4009\n\n\n\nIf a query is done for a server type which is not registered, an error\nis returned:\n\n\nq)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb`other)                                                                                                                                                                                                                                   \n`not all of the requested server types are available; missing other\n\n\n\nCustom join functions can be specified:\n\n\nq)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by a from x} each x})                                                                                                                                                                                                  \na   | x\n----| -\n5014| 2\n5015| 2\n5016| 2\n5017| 1\n5018| 1\n5012| 1\n5013| 1\n\n\n\nCustom joins can fail with appropriate errors:\n\n\nq)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by b from x} each x})                                                                                                                                                                                                  \n`failed to apply supplied join function to results: b\n\n\n\nAsynchronous queries must be sent in async and blocked:\n\n\nq)(neg h)(`.gw.asyncexec;(`f;2);`hdb`rdb); r:h(::)                                                                                                                                                                                                                          \n    /- This white space is from pressing return\n    /- the client is blocked and unresponsive\n\nq)q)q)                                                                                                                                                                                                                                                                        \nq)                                                                                                                                                                                                                                                                            \nq)r                                                                                                                                                                                                                                                                           \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)\n\n\n\nWe can send multiple async queries at once. Given the gateway has two\nRDBs and two HDBs avaialble to it, it should be possible to service two\nof these queries at the same time.\n\n\nq)h:hopen each 8000 8000                                                                                                                                                                                                                                                      \nq)\\t (neg h)@\\:(`.gw.asyncexec;(`f;2);`hdb`rdb); (neg h)@\\:(::); r:h@\\:(::)\n4012\nq)r                                                                                                                                                                                                                                                                           \n+(,`a)!,5014 5015 5016 5017 5018 5012 5013 5014 5015 5016\n+(,`a)!,5013 5014 5015 5016 5017 9999 10000 10001 10002 10003\n\n\n\nAlternatively async queries can specify a postback so the client does\nnot have to block and wait for the result. The postback function must\ntake two parameters- the first is the function that was sent up, the\nsecond is the results. The postback can either be a lambda, or the name\nof a function.\n\n\nq)h:hopen 8000                                                                                                                                                                                                                                                                \nq)handleresults:{-1(string .z.z),\" got results\"; -3!x; show y}                                                                                                                                                                                                                \nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;handleresults;0Wn)                                                                                                                                                                                                           \nq)\nq)  /- These q prompts are from pressing enter\nq)  /- The q client is not blocked, unlike the previous example\nq)\nq)2014.01.07T16:53:42.481 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\n\n/- Can also use a named function rather than a lambda\nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;`handleresults;0Wn)\nq)\nq)              \nq)2014.01.07T16:55:12.235 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\n\n\n\nAsynchronous queries can also be timed out. This query will run for 22\nseconds, but should be timed out after 5 seconds. There is a tolerance\nof +5 seconds on the timeout value, as that is how often the query list\nis checked. This can be reduced as required.\n\n\nq)(neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)\n\nq)q)q)r                                                                                                                                                                                                                                                                       \n\"error: query has exceeded specified timeout value\"\nq)\\t (neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)                                                                                                                                                                                                  \n6550\n\n\n\nNon kdb+ Clients\n\n\nAll the examples in the previous section are from clients written in q.\nHowever it should be possible to do most of the above from non kdb+\nclients. The officially supported APIs for Java, C# and C allow the\nasynchronous methods above. For example, we can modify the try block in\nthe main function of the \nJava Grid\nViewer\n:\n\n\nimport java.awt.BorderLayout;\nimport java.awt.Color;\nimport java.io.IOException;\nimport java.lang.reflect.Array;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTable;\nimport javax.swing.table.AbstractTableModel;\nimport kx.c;\n\npublic class Main {\n    public static class KxTableModel extends AbstractTableModel {\n        private c.Flip flip;\n        public void setFlip(c.Flip data) {\n            this.flip = data;\n        }\n\n        public int getRowCount() {\n            return Array.getLength(flip.y[0]);\n        }\n\n        public int getColumnCount() {\n            return flip.y.length;\n        }\n\n        public Object getValueAt(int rowIndex, int columnIndex) {\n            return c.at(flip.y[columnIndex], rowIndex);\n        }\n\n        public String getColumnName(int columnIndex) {\n            return flip.x[columnIndex];\n        }\n    };\n\n    public static void main(String[] args) {\n        KxTableModel model = new KxTableModel();\n        c c = null;\n        try {\n            c = new c(\"localhost\", 8000,\"username:password\");\n            // Create the query to send\n        String query=\".gw.asyncexec[(`f;2);`hdb`rdb]\";\n            // Send the query \n        c.ks(query);\n            // Block on the socket and wait for the result\n        model.setFlip((c.Flip) c.k());\n        } catch (Exception ex) {\n            Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex);\n        } finally {\n            if (c != null) {try{c.close();} catch (IOException ex) {}\n          }\n        }\n        JTable table = new JTable(model);\n        table.setGridColor(Color.BLACK);\n        String title = \"kdb+ Example - \"+model.getRowCount()+\" Rows\";\n        JFrame frame = new JFrame(title);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        frame.getContentPane().add(new JScrollPane(table), BorderLayout.CENTER);\n        frame.setSize(300, 300);\n        frame.setVisible(true);\n    }\n}\n\n\n\nSome of the unofficially supported APIs may only allow synchronous calls\nto be made.\n\n\n\n\nReal Time Database (RDB)\n\n\nThe Real Time Database is a modified version of r.q found in kdb+tick.\nThe modifications from the standard r.q include:\n\n\n\n\n\n\nTickerplant (data source) and HDB location derived from processes\n    defined by the discovery service or from config file;\n\n\n\n\n\n\nAutomatic re-connection and resubscription to tickerplant;\n\n\n\n\n\n\nList of tables to subscribe to supplied as configuration setting;\n\n\n\n\n\n\nMore pre-built flexibility in end-of-day;\n\n\n\n\n\n\nMore verbose end-of-day logging;\n\n\n\n\n\n\nReload multiple authenticated HDBs after end-of-day;\n\n\n\n\n\n\nEnd-of-day save down manipulation code is shared between RDB, WDB\n      and tickerplant log replay\n\n\n\n\n\n\nSee the top of the file for more information.\n\n\n\n\nWrite Database (WDB)\n\n\nThe Write Database or WDB is based on w.q. This process features a\nnumber of modifications and enhancements over w.q:\n\n\n\n\n\n\nProvides the option to write down to a custom partition scheme,\n    defined by parted columns in sort.csv, which removes the need for\n    end of day sorting;\n\n\n\n\n\n\nGreater configuration options; max rows on a per table basis, list\n     subscription tables, upd function etc. See the top of the process\n     file for the options;\n\n\n\n\n\n\nUse of common code with the RDB and Tickerplant Log Replay process\n     to manipulate tables before save, sort and apply attributes;\n\n\n\n\n\n\nChecks whether to persist data to disk on a timer rather than on\n     each tick;\n\n\n\n\n\n\nInforms other RDB, HDB and GW processes that end of day save and\n     sort has completed;\n\n\n\n\n\n\nMore log information supplied.\n\n\n\n\n\n\nThe WDB process can broken down into two main functions:\n\n\n\n\n\n\nPeriodically saving data to disk and\n\n\n\n\n\n\nSorting data at end of day\n\n\n\n\n\n\nThe WDB process provides flexibility so it can be set-up as a\nstand-alone process that will both save and sort data or two separate\nprocesses (one that saves the data and another that will sort the data\non disk). This allows greater flexibility around the end of day event as\nsorting data can be time consuming. It is also helps when implementing\nseemless rollovers (i.e. no outage window at end-of-day).\n\n\nThe behaviour of the WDB process is controlled by the \n.wdb.mode\n\nparameter. This should be set to one of following three values:\n\n\n\n\n\n\nsaveandsort - the process will subscribe for data, periodically\n    write data to disk and at EOD it will flush remaining data to disk\n    before sorting it and informing GWs, RDBs and HDBs etc.\n\n\n\n\n\n\nsave - the process will subscribe for data, periodically write data\n      to disk and at EOD it will flush remaining data to disk. It will\n      then inform its respective sort mode process to sort the data\n\n\n\n\n\n\nsort - the process will wait to get a trigger from its respective\n      save mode process. When this is triggered it will sort the data on\n      disk, apply attributes and the trigger a reload on the RDB, HDB and\n      GW processes\n\n\n\n\n\n\nWhen running a system with separate save and sort process, the sort\nprocess should be configured in the processes.csv file with a proctype\nof sort. The save process will check for processes with a proctype of\nsort when it attempts to trigger the end of day sort of the data.\n\n\nThe wdb process provides two methods for persisting data to disk and\nsorting at the end of the day.\n\n\n\n\n\n\ndefault - Data is persisted into a partition defined by the\n    [partitiontype] variable, similar to the hdb partition scheme. The\n    general scheme is of the form\n    [wdbdir]/[partitiontype]/[table]/. And a typical partition\n    directory would be similar to wdb/database/2015.11.26/trades/. At\n    the end of the day, before being moved to the hdb, the data is\n    sorted according to parameters defined in sort.csv. For each table,\n    sort.csv will specify the columns to sort (using xasc) and apply\n    attributes to.\n\n\n\n\n\n\npartbyattr - Data is persisted to a custom partition scheme, derived\n      from parameters in the sort.csv file. The write down scheme is taken\n      from sort.csv, to reflect the effect of using xasc at the end of\n      day. For each table, the columns defined in sort.csv, with the\n      parted attribute, are used to create custom partitions in the wdb.\n      Multiple columns can be defined with the parted attribute and\n      distinct combinations of each are generated for custom partitions.\n      The general partition scheme is of the form\n      [wdbdir]/[partitiontype]/[table]/[parted column(s)]/. And a\n      typical partition directory would be similar to\n      wdb/database/2015.11.26/trade/MSFT_N. In the above example, the\n      data is parted by sym and source, and so a unique partition\n      directory MSFT_N is created in the wdb directory.\n\n\nAt the end of the day, data is upserted into the hdb without the\n  need for sorting. The number of rows that are joined at once is\n  limited by the mergenumrows and mergenumtab parameters.\n\n\n\n\n\n\nThe optional partbyattr method may provide a significant saving in time\nat the end of day, allowing the hdb to be accessed sooner. For large\ndata sets with a low cardinality (ie. small number of distinct elements)\nthe optional method may provide a significant time saving, upwards of\n50%. The optional method should also reduce the memory usage at the end\nof day event, as joining data is generally less memory intensive than\nsorting.\n\n\n\n\nTickerplant Log Replay\n\n\nThe Tickerplant Log Replay script is for replaying tickerplant logs.\nThis is useful for:\n\n\n\n\n\n\nhandling end of day save down failures;\n\n\n\n\n\n\nhandling large volumes of data (larger than can fit into RAM).\n\n\n\n\n\n\nThe process takes as the main input either an individual log file to\nreplay, or a directory containing a set of log files. Amongst other\nfunctionality, the process can:\n\n\n\n\n\n\nreplay specific message ranges;\n\n\n\n\n\n\nreplay in manageable message chunks;\n\n\n\n\n\n\nrecover as many messages as possible from a log file rather than\n      just stopping at the first bad message;\n\n\n\n\n\n\nignore specific tables;\n\n\n\n\n\n\nmodify the tables before or after they are saved;\n\n\n\n\n\n\napply sorting and parting after all the data is written out.\n\n\n\n\n\n\nThe process must have some variables set (the tickerplant log file or\ndirectory, the schema file, and the on-disk database directory to write\nto) or it will fail on startup. These can either be set in the config\nfile, or overridden from the command line in the usual way. An example\nstart line would be:\n\n\nq torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.tplogfile ../test/tplogs/marketdata2013.12.17 -.replay.schemafile ../test/marketdata.q -.replay.hdbdir ../test/hdb1\n\n\n\nThe tickerplant log replay script has extended usage information which\ncan be accessed with -.replay.usage.\n\n\nq torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.usage\n\n\n\n\n\nHousekeeping\n\n\nThe housekeeping process is used to undertake periodic system\nhousekeeping and maintenance, such as compressing or removing files\nwhich are no longer required. The process will run the housekeeping jobs\nperiodically on a timer. Amongst other functionality the process:\n\n\n\n\n\n\nAllows for removing and zipping of directory files;\n\n\n\n\n\n\nProvides an inbuilt search utility and selectively searches using a\n      \u2018find\u2019 and \u2018exclude\u2019 string, and an \u2018older than\u2019 parameter;\n\n\n\n\n\n\nReads all tasks from a single CSV;\n\n\n\n\n\n\nRuns on a user defined timer;\n\n\n\n\n\n\nCan be run immediately from command line or within the process;\n\n\n\n\n\n\nCan be easily extended to include new user defined housekeeping\n      tasks.\n\n\n\n\n\n\nThe process has two main parameters that should be set prior to use;\nruntimes and inputcsv.\u2018Runtimes\u2019 sets the timer to run housekeeping at\nthe set time(s), and \u2018Inputcsv\u2019 provides the location of the\nhousekeeping csv file. These can either be set in the config file, or\noverridden via the command line. If these are not set, then default\nparameters are used; 12.00 and \u2018KDBCONFIG/housekeeping.csv\u2019\nrespectively. The process is designed to run from a single csv file with\nfive headings:\n\n\n\n\n\n\nFunction details the action that you wish to be carried out on the\n    files, initially, this can be rm (remove) and zip (zipping);\n\n\n\n\n\n\nPath specifies the directory that the files are in;\n\n\n\n\n\n\nMatch provides the search string to the find function, files\n      returned will have names that match this string;\n\n\n\n\n\n\nExclude provides a second string to the find function, and these\n      files are excluded from the match list;\n\n\n\n\n\n\nAge is the \u2018older than\u2019 parameter, and the function will only be\n      carried out on files older than the age given (in days).\n\n\n\n\n\n\nAn example csv file would be:\n\n\nfunction,path,match,exclude,age\nzip,./logs/,*.log,*tick*,2\nrm,./logs/,*.log*,*tick*,4\nzip,./logs/,*tick*,,1\nrm,./logs/,*tick*,,3\n\nfunction path      match    exclude  age\n----------------------------------------\nzip      \"./logs/\" \"*.log\"  \"*tick*\" 2\nrm       \"./logs/\" \"*.log*\" \"*tick*\" 4\nzip      \"./logs/\" \"*tick*\" \"\"       1\nrm       \"./logs/\" \"*tick*\" \"\"       3\n\n\n\nThe process reads in the csv file, and passes it line by line to a\n\u2018find\u2019 function; providing a dictionary of values that can be used to\nlocate the files required. The find function takes advantage of system\ncommands to search for the files according to the specifications in the\ndictionary. A search is performed for both the match string and the\nexclude string, and cross referenced to produce a list of files that\nmatch the parameters given. The files are then each passed to a further\nset of system commands to perform the task of either zipping or\nremoving. Note that an incomplete csv or non-existant path will throw an\nerror.\n\n\nThe remove and zipping functions form only basic implimentations of the\nhousekeeping process; it is designed to be exended to include more\nactions than those provided. Any user function defined in the\nhousekeeping code can be employed in the same fashion by providing the\nname of the function,search string and age of files to the csv.\n\n\nAs well as being scheduled on a timer, the process can also be run\nimmediately. Adding \u2018-hk.runnow 1\u2019 to the command line when starting the\nprocess will force immediate running of the actions in the housekeeping\ncsv. Likewise, setting runnow to 1b in the config file will immediately\nrun the cleaning process. Both methods will cause the process to exit\nupon completion. Calling hkrun[] from within the q process will also\nrun the csv instructions immediately. This will not affect any timer\nscheduling and the process will remain open upon completion.\n\n\nHousekeeping works both on windows and unix based systems. Since the\nprocess utilizes inbuilt system commands to perform maintenances, a\nunix/windows switch detects the operating system of the host and applies\neither unix or widows functions appropriately. Extensions need only be\nmade in the namespace of the hosting operating system (i.e. if you are\nusing a unix system, and wish to add a new function, you do not need to\nadd the function to the windows namespace to). Usage information can be\naccessed using the \u2018-hkusage\u2019 flag:\n\n\nq torq.q -load code/processes/housekeeping.q -p 9999 -proctype housekeeping -procname hk1 -debug -hkusage\n\n\n\n\n\nFile Alerter\n\n\nThe file alerter process is a long-running process which periodically\nscans a set of directories for user-specified files. If a matching file\nis found it will then carry out a user-defined function on it. The files\nto search for and the functions to run are read in from a csv file.\nAdditionally, the file alerter process can:\n\n\n\n\n\n\nrun more than one function on the specified file.\n\n\n\n\n\n\noptionally move the file to a new directory after running the\n      function.\n\n\n\n\n\n\nstore a table of files that have already been processed.\n\n\n\n\n\n\nrun the function only on new files or run it every time the file is\n      modified.\n\n\n\n\n\n\nignore any matching files already on the system when the process\n      starts and only run a function if a new file is added or a file is\n      modified.\n\n\n\n\n\n\nThe file alerter process has four parameters which should be set prior\nto use. These parameters can either be set in the config file or\noverridden on the command-line. If they are not set, the default\nparameters will be used. The parameters are as follows.\n\n\ninputcsv\n - The name and location of the csv file which defines the\nbehaviour of the process. The default is KDBCONFIG/filealerter.csv.\n\n\npolltime\n - How often the process will scan for matching files. The\ndefault is 0D:00:01, i.e., every minute.\n\n\nalreadyprocessed\n - The name and location of the already-processed\ntable. The default is KDBCONFIG/filealerterprocessed. This table will\nbe created automatically the first time the process is ran.\n\n\nskipallonstart\n - If this is set to 1, it will ignore all files\nalready on the system; if it is set to 0, it will not. The default\nvalue is 0.\n\n\nThe files to find and the functions to run are read in from a csv file\ncreated by the user. This file has five columns, which are detailed\nbelow.\n\n\npath\n - This is the path to the directory that will be scanned for\nthe file.\n\n\nmatch\n - This is a search string matching the name of the file to be\nfound. Wildcards can be used in this search, for example, \u201cfile*\u201d will\nfind all files starting with \u201cfil\u201d.\n\n\nfunction\n - This is the name of the function to be run on the file.\nThis function must be defined in the script\nKDBCODE/processes/filealerter.q. If the function is not defined or fails\nto run, the process will throw an error and ignore that file from then\non.\n\n\nnewonly\n - This is a boolean value. If it is set to 1, it will\nonly run the function on the file if it has been newly created. If it is\nset to 0, then it will run the function every time the file is\nmodified.\n\n\nmovetodirectory\n - This is the path of the directory you would like\nto move the file to after it has been processed. If this value is left\nblank, the file will not be moved.\n\n\nIt is possible to run two separate functions on the same file by adding\nthem as separate lines in the csv file. If the file is to be moved after\nit is processed, the file alerter will run both functions on the file\nand then attempt to move it. A typical csv file to configure the file\nalerter would look like:\n\n\npath,match,function,newonly,movetodirectory\n/path/to/dirA,fileA.*,copy,0,/path/to/newDir\n/path/to/dirB,fileB.txt,email,1,\n/path/to/dirA,fileA.*,delete,0,/path/to/newDir\n\npath        match      function  newonly    movetodirectory\n---------------------------------------------------\n\"/path/to/dirA\" \"fileA.*\"   copy    0   \"/path/to/newDir\"\n\"/path/to/dirB\" \"fileB.txt\" email   1   \"\"\n\"/path/to/dirA\" \"fileA.*\"   delete  0   \"/path/to/newDir\"\n\n\n\nThe file alerter process reads in each line of the csv file and searches\nfiles matching the search string specified in that line. Note that there\nmay be more than one file found if a wildcard is used in the search\nstring. If it finds any files, it will check that they are not in the\nalready processed table. If newonly is set to 1, it only checks if\nthe filename is already in the table. If newonly is set to 0, it\nchecks against the filename, filesize and a md5 hash of the file. The\nmd5 hash and the filesize are used to determine if the file has been\nmodified since it was processed last. If the found files have not been\nprocessed already, it then attempts to run the specified function to\nthese files.\n\n\nAfter the process has run through each line of the csv, it generates a\ntable of all files that were processed on that run. These files are\nappended to the already processed table which is then saved to disk. The\nfile alerter will attempt to move the files to the \u2018movetodirectory\u2019, if\nspecified. If the file has already been moved during the process (for\nexample, if the function to run on it was \u2018delete\u2019), the file alerter\nwill not attempt to move it.\n\n\nThe file alerter is designed to be extended by the user. Customised\nfunctions should be defined within the filealerter.q script. They should\nbe diadic functions, i.e., they take two parameters: the path and the\nfilename. As an example, a simple function to make a copy of a file in\nanother directory could be:\n\n\ncopy:{[path;file] system \"cp \", path,\"/\", file, \" /path/to/newDir\"}\n\n\n\nAlthough the process is designed to run at regular intervals throughout\nthe day, it can be called manually by invoking the FArun[] command\nfrom within the q session. Similarly, if new lines are added to the csv\nfile, then it can be re-loaded by calling the loadcsv[] command\nfrom the q session.\n\n\nEach stage of the process, along with any errors which may occur, are\nappropriately logged in the usual manner.\n\n\nThe file alerter process is designed to work on both Windows and Unix\nbased systems. Since many of the functions defined will use inbuilt\nsystem command they will be need to written to suit the operating system\nin use. It should also be noted that Windows does not have an inbuilt\nmd5 hashing function so the file alerter will only detect different\nversions of files if the filename or filesize changes.\n\n\n\n\nReporter\n\n\nOverview\n\n\nThe reporter process is used to run periodic reports on specific\nprocesses. A report is the result of a query that is run on a process at\na specific time. The result of the query is then handled by one of the\ninbuilt result handlers, with the ability to add custom result handlers.\n\n\n\n\nFeatures:\n\n\n\n\n\n\nEasily create a report for information that you want;\n\n\n\n\n\n\nFully customizable scheduling such as start time, end time and days\n      of the week;\n\n\n\n\n\n\nRun reports repeatedly with a custom period between them;\n\n\n\n\n\n\nAsynchronous querying with custom timeout intervals;\n\n\n\n\n\n\nInbuilt result handlers allow reports to be written to file or\n      published;\n\n\n\n\n\n\nCustom result handlers can be defined;\n\n\n\n\n\n\nLogs each step of the report process;\n\n\n\n\n\n\nFully integrated with the TorQ gateway to allow reports to be run\n      across backend processes.\n\n\n\n\n\n\nThe reporter process has three parameters that are read in on\ninitialisation from the reporter.q file found in the\n$KDBCONFIG/settings directory. These settings are the string filepath\nof the input csv file, a boolean to output log messages and timestamp\nfor flushing the query log table.\n\n\nTo run the reporter process:\n\n\nq torq.q -load code/processes/reporter.q -p 20004\n\n\n\nOnce the reporter process has been initiated, the reports will be\nscheduled and no further input is required from the user.\n\n\nReport Configuration\n\n\nBy default, the process takes its inputs from a file called reporter.csv\nwhich is found in the $KDBCONFIG directory. This allows the user\ncomplete control over the configuration of the reports. As the queries\nare evaluated on the target process, local variables can be referenced\nor foreign functions can be run. Table [table:reportertable] shows the\nmeaning of the csv schema.\n\n\n\n\n\n\n\n\nColumn Header\n\n\nDescription and Example\n\n\n\n\n\n\n\n\n\n\nname\n\n\nReport name e.g. Usage\n\n\n\n\n\n\nquery\n\n\nQuery to be evaluated on that process. It can be a string query or function\n\n\n\n\n\n\nresulthandler\n\n\nResult handlers are run on the returned result. Custom result handlers can be added. The result handler must be a monadic function with the result data being passed in e.g. writetofile[\u201c./output\u201d;\u201cusage\u201d]\n\n\n\n\n\n\ngateway\n\n\nIf non null the reporter will query processes route the query to the proctype specified in this field. The values in the proctype field will be the process types on which the gateway runs the backend query. e.g. `gateway\n\n\n\n\n\n\njoinfunction\n\n\nUsed to join the results when a gateway query is being used. The choice of joinfunction must take into account the result that will be received. The function must be monadic and the parameter will be the list of results returned from the backend processes e.g. raze\n\n\n\n\n\n\nproctype\n\n\nThe type of process that the report will be run on. If the gateway field is not empty this may be a list of process types, otherwise the reporter will throw an error on startup. e.g. `rdb\n\n\n\n\n\n\nprocname\n\n\nThe name of a specific process to run the report on. If left null, the reporter process will select a random process with the specified proctype. If the gateway field is not null, this field specifies the specific gateway process name to run the query against e.g. `hdb1\n\n\n\n\n\n\nstart\n\n\nTime on that day to start at e.g. 12:00\n\n\n\n\n\n\nend\n\n\nTime on that day that the report will stop at e.g. 23:00\n\n\n\n\n\n\nperiod\n\n\nThe period between each report query e.g. 00:00:10\n\n\n\n\n\n\ntimeoutinterval\n\n\nThe amount of time the reporter waits before timing out a report e.g. 00:00:30\n\n\n\n\n\n\ndaysofweek\n\n\nNumeric value required for the day of the week. Where 0 is Saturday and 2 is Monday\n\n\n\n\n\n\n\n\nWhen running a report on a gateway, the gateway field must be set to the\nproctype of the gateway that will be queried. It will then run the\nreport on the processes which are listed in the proctype field and join\nthe results by using the function specified in the joinfunction field.\nIf there is no join function then the reporter process will not start.\nMultiple entries in the proctype field must be separated by a space and\nare only allowed when the gateway field is not empty. If gateway field\nis empty and there are multiple entries in the proctype field then the\nreporter process will not load.\n\n\nListing [code:csvschema] shows an example of the schema needed in the\ninput csv file.\n\n\nname|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nusage|10#.usage.usage|writetofiletype[\"./output/\";\"usage\";\"csv\"]|||rdb||00:01|23:50|00:01|00:00:01|0 1 2 3 4 5 6\nmemory|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|||rdb|rdb1|00:05|18:00|00:01|00:00:08|0 1 2 3 4 5 6\nusage_gateway|10#.usage.usage||gateway|raze|rdb hdb||00:02|22:00|00:01|00:00:10|0 1 2 3 4 5 6\n\n\n\nResult Handlers\n\n\nThere are several default result handlers which are listed below. Custom\nresult handlers can be defined as required. The result handler will be\ninvoked with a single parameter (the result of the query).\n\n\nwritetofiletype\n - Accepts 3 parameters: path, filename, filetype and\ndata. When writing to file it uses a date time suffix so the resultant\nfilename will be \nusage_rdb_2014_01_02_15_00_12.txt\n e.g.\n\n\nwritetofiletype[\"./output/\";\"usage\";\"csv\"]\n\n\n\nsplaytable\n - This accepts 3 parameters: path, file and data. This\nsplays the result to a directory. The result must be a table in order to\nuse this function e.g.\n\n\nsplaytable[\"./output/\";\"usage\"]\n\n\n\nemailalert\n - This accepts 3 parameters: period, recipient list and\ndata. The period dictates the throttle i.e. emails will be sent at most\nevery period. The result of the report must be a table with a single\ncolumn called messages which contains the character list of the email\nmessage. This is used with the monitoring checks to raise alerts, but\ncan be used with other functions.\n\n\nemailalert[0D00:30;(\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\")]\n\n\n\nemailreport\n - This accepts 3 parameters: temporary path, recipient\nlist, file name, file type and data. The data is written out as the file\ntype (e.g. csv, xml, txt, xls, json) with the given file name to the\ntemporary path. It is then emailed to the recipient list, and the\ntemporary file removed.\n\n\nemailreport[\"./tempdir/\"; (\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\"); \"EndOfDayReport\"; \"csv\"]\n\n\n\npublishresult\n - Accepts 1 parameter and that is the data. This is\ndiscussed later in the subsection\u00a0subresults.\nCustom result handlers can be added to $KDBCODE/processes/reporter.q .\nIt is important to note that the result handler is referencing local\nfunctions as it is executed in the reporter process and not the target\nprocess. When the query has been successful the result handler will be\npassed a dictionary with the following keys: queryid, time, name,\nprocname, proctype and result.\n\n\nReport Process Tracking\n\n\nEach step of the query is logged by the reporter process. Each query is\ngiven a unique id and regular system messages are given the id 0. The\nstage column specifies what stage the query is in and these are shown in\ntable [table:stagetable]. An appropriate log message is also shown so\nany problems can easily be diagnosed. The in memory table is flushed\nevery interval depending on the value of the flushqueryloginterval\nvariable in the reporter.q file found in the $KDBCONFIG/settings\ndirectory. \n\n\n\n\n\n\n\n\nStage symbol\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nR\n\n\nThe query is currently running\n\n\n\n\n\n\nE\n\n\nAn error has occurred during the query\n\n\n\n\n\n\nC\n\n\nThe query has been completed with no errors\n\n\n\n\n\n\nT\n\n\nThe query has exceeded the timeout interval\n\n\n\n\n\n\nS\n\n\nSystem message e.g. \u201cReporter Process Initialised\u201d\n\n\n\n\n\n\n\n\ntime                         | queryid stage message\n-----------------------------| ------------------------------------------------------------------------\n2014.10.20D22:20:06.597035000| 37 R \"Received result\"\n2014.10.20D22:20:06.600692000| 37 R \"Running resulthandler\"\n2014.10.20D22:20:06.604455000| 37 C \"Finished report\"\n2014.10.20D22:30:00.984572000| 38 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:00.991862000| 38 R \"Received result\"\n2014.10.20D22:30:00.995527000| 38 R \"Running resulthandler\"\n2014.10.20D22:30:00.999236000| 38 C \"Finished report\"\n2014.10.20D22:30:06.784419000| 39 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:06.796431000| 39 R \"Received result\"\n\n\n\nSubscribing for Results\n\n\nTo publish the results of the report, the reporter process uses the pub\nsub functionality of TorQ. This is done by using the using the inbuilt\nresult handler called publishresult. In order to subscribe to this feed,\nconnect to the reporter process and send the function shown below over\nthe handle. To subscribe to all reports use a backtick as the second\nparameter and to subscribe to a specific reports results include the\nreporter name as a symbol.\n\n\n/- define a upd function\nupd:insert\n\n/- handle to reporter process\nh: hopen 20004\n\n/- Subscribe to all results that use the publishresult handler\nh(`.ps.subscribe;`reporterprocessresults;`)\n\n/- Subscribe to a specific report called testreport\nh(`.ps.subscribe;`reporterprocessresults;`testreport)\n\n\n\nExample reports\n\n\nThe following are examples of reports that could be used in the reporter\nprocess. The rdbtablecount report will run hourly and return the count\nof all the tables in a rdb process. The memoryusage report will run\nevery 10 minutes against the gateway for multiple processes and will\nreturn the \n.Q.w[]\n information. Both of these reports run between\n9:30am to 4:00pm during the weekdays. The report onetimequery is an\nexample of a query that is run one time, in order to run a query once,\nthe period must be the same as the difference between the start and end\ntime.\n\n\nname|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nrdbtablecount|ts!count each value each ts:tables[]|{show x`result}|||rdb|rdb1|09:30|16:00|01:00|00:00:10|2 3 4 5 6\nmemoryusage|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|gateway1|{enlist raze x}|rdb hdb||09:30|16:00|00:10|00:00:10|2 3 4 5 6\nonetimequery|10#.usage.usage|writetofile[\"./output/\";\"onetime.csv\"]|||rdb||10:00|10:01|00:01|00:00:10|2 3 4 5 6\n\n\n\n\n\nMonitor\n\n\nThe Monitor process is a simple process to monitor the health of the\nother processes in the system. It connects to each process that it finds\n(by default using the discovery service, though can use the static file\nas well) and subscribes to both heartbeats and log messages. It\nmaintains a keyed table of heartbeats, and a table of all log messages\nreceived.\n\n\nRun it with:\n\n\naquaq $ q torq.q -load code/processes/monitor.q -p 20001\n\n\n\nIt is probably advisable to run the monitor process with the -trap flag,\nas there may be some start up errors if the processes it is connecting\nto do not have the necessary heartbeating or publish/subscribe code\nloaded.\n\n\naquaq $ q torq.q -load code/processes/monitor.q -p 20001 -trap\n\n\n\nThe current heartbeat statuses are tracked in .hb.hb, and the log\nmessages in logmsg\n\n\nq)show .hb.hb                                                                                                                                                                                                                                                                 \nsym       procname    | time                          counter warning error\n----------------------| ---------------------------------------------------\ndiscovery discovery2  | 2014.01.07D13:24:31.848257000 893     0       0    \nhdb       hdb1        | 2014.01.07D13:24:31.866459000 955     0       0    \nrdb       rdb_europe_1| 2014.01.07D13:23:31.507203000 901     1       0    \nrdb       rdb1        | 2014.01.07D13:24:31.848259000 34      0       0\n\nq)show select from logmsg where loglevel=`ERR                                                                                                              \ntime                          sym  host  loglevel id      message                               \n-------------------------------------------------------------------------------------\n2014.01.07D12:25:17.457535000 hdb1 aquaq ERR      reload  \"failed to reload database\"           \n2014.01.07D13:29:28.784333000 rdb1 aquaq ERR      eodsave \"failed to save tables : trade, quote\"\n\n\n\nHTML5 front end\n\n\nA HTML5 front end has been built to display important process\ninformation that is sent from the monitor process. It uses HTML5,\nWebSockets and JavaScript on the front end and interacts with the\nmonitor process in the kdb+ side. The features of the front end include:\n\n\n\n\n\n\nHeartbeat table with processes that have warnings highlighted in\n    orange and errors in red\n\n\n\n\n\n\nLog message table displaying the last 30 errors\n\n\n\n\n\n\nLog message error chart that is by default displayed in 5 minute\n      bins\n\n\n\n\n\n\nChart\u2019s bin value can be changed on the fly\n\n\n\n\n\n\nResponsive design so works on all main devices i.e. phones, tablets\n      and desktop\n\n\n\n\n\n\nIt is accessible by going to the url \nhttp://HOST:PORT/.non?monitorui\n\n\n\n\nCompression\n\n\nThe compression process is a thin wrapper around the compression utility\nlibrary. It allows periodic compression of whole or parts of databases\n(e.g. data is written out uncompressed and then compressed after a\ncertain period of time). It uses four variables defined in\nKDBCONFIG/settings/compression.q which specify\n\n\n\n\n\n\nthe compression configuration file to use\n\n\n\n\n\n\nthe database directory to compress\n\n\n\n\n\n\nthe maximum age of data to attempt to compress\n\n\n\n\n\n\nwhether the process should exit upon completion\n\n\n\n\n\n\nThe process is run like other TorQ processes:\n\n\nq torq.q -load code/processes/compression.q -p 20005\n\n\n\nModify the settings file or override variables from the command line as\nappropriate.\n\n\n\n\nKill\n\n\nThe kill process is used to connect to and terminate currently running\nprocesses. It kills the process by sending the exit command therefore\nthe kill process must have appropriate permissions to send the command,\nand it must be able to create a connection (i.e. it will not be able to\nkill a blocked process in the same way that the unix command kill -9\nwould). By default, the kill process will connect to the discovery\nservice(s), and kill the processes of the specified types. The kill\nprocess can be modified to not use the discovery service and instead use\nthe process.csv file via the configuration in the standard way.\n\n\nIf run without any command line parameters, kill.q will try to kill each\nprocess it finds with type defined by its .servers.CONNECTIONS variable.\n\n\nq torq.q -load code/processes/kill.q -p 20000\n\n\n\n.servers.CONNECTIONS can optionally be overridden from the command line\n(as can any other process variable):\n\n\nq torq.q -load code/processes/kill.q -p 20000 -.servers.CONNECTIONS rdb tickerplant\n\n\n\nThe kill process can also be used to kill only specific named processes\nwithin the process types:\n\n\nq torq.q -load code/processes/kill.q -p 20000 -killnames hdb1 hdb2\n\n\n\n\n\nChained Tickerplant\n\n\nIn tick+ architecture the main tickerplant is the most important\ncomponent, as it is relied upon by all the real time subscribers. When\nthe tickerplant goes down data will be lost, compare this to an rdb\nwhich can be recovered after it fails. The chained tickerplant process\nis an additional tickerplant that is a real time subscriber to the main\ntickerplant but replicates its behaviour. It will have its own real time\nsubscribers and can be recovered when it fails. This is the recommended\napproach when users want to perform their own custom real time analysis.\n\n\nThe chained tickerplant can:\n\n\n\n\n\n\nsubscribe to specific tables and syms\n\n\n\n\n\n\nbatch publish at an interval or publish tick by tick\n\n\n\n\n\n\ncreate a tickerplant log that its real time subscribers can replay\n\n\n\n\n\n\nreplay the source tickerplant log\n\n\n\n\n\n\nTo launch the chained tickerplant\n\n\nq torq.q -load code/processes/chainedtp.q -p 12009\n\n\n\nChained tickerplant settings are found in \nconfig/settings/chainedtp.q\n\nand are under the \n.ctp\n namespace.\n\n\n\n\n\n\n\n\nSetting\n\n\nExplanation\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\ntickerplantname\n\n\nlist of tickerplant names to try and make a connection to\n\n\n`tickerplant1\n\n\n\n\n\n\npubinterval\n\n\npublish batch updates at this interval. If the value is 0D00:00:00 then it will publish tick by tick\n\n\n0D00:00:00\n\n\n\n\n\n\ntpconnsleep\n\n\nnumber of seconds between attempts to connect to the source tickerplant\n\n\n10\n\n\n\n\n\n\ncreatelogfile\n\n\ncreate a log file\n\n\n0b\n\n\n\n\n\n\nlogdir\n\n\ndirectory containing chained tickerplant logs\n\n\n`:hdb\n\n\n\n\n\n\nsubscribeto\n\n\nsubscribe to these tables only (null for all)\n\n\n`\n\n\n\n\n\n\nsubscribesyms\n\n\nsubscribe to these syms only (null for all)\n\n\n`\n\n\n\n\n\n\nreplay\n\n\nreplay the tickerplant log file\n\n\n0b\n\n\n\n\n\n\nschema\n\n\nretrieve schema from tickerplant\n\n\n1b\n\n\n\n\n\n\nclearlogonsubscription\n\n\nclear log on subscription, only called if createlogfile is also enabled\n\n\n0b\n\n\n\n\n\n\n\n\n\n\nIntegration with kdb+tick\n\n\nAquaQ TorQ can be fully integrated with kdb+tick. For further details,\nuse one of the AquaQ TorQ Starter packs to set up a production kdb+ data\ncapture system.", 
            "title": "Processes"
        }, 
        {
            "location": "/Processes/#processes", 
            "text": "A set of processes is included. These processes build upon AquaQ TorQ,\nproviding specific functionality. All the process scripts are contained\nin $KDBCODE/processes. All processes should have an entry in\n$KDBCONFIG/process.csv. All processes can have any type and name,\nexcept for discovery services which must have a process type of\n\u201cdiscovery\u201d. An example process.csv is:  aquaq$ cat config/process.csv\nhost,port,proctype,procname\naquaq,9998,rdb,rdb_europe_1\naquaq,9997,hdb,rdb_europe_1aquaq,9999,hdb,hdb1\naquaq,9996,discovery,discovery1\naquaq,9995,discovery,discovery2\naquaq,8000,gateway,gateway1\naquaq,5010,tickerplant,tickerplant1\naquaq,5011,rdb,rdb1\naquaq,5012,hdb,hdb1\naquaq,5013,hdb,hdb2\naquaq,9990,tickerlogreplay,tpreplay1\naquaq,20000,kill,killhdbs\naquaq,20001,monitor,monitor1\naquaq,20002,housekeeping,hk1", 
            "title": "Processes"
        }, 
        {
            "location": "/Processes/#discovery-service", 
            "text": "", 
            "title": "Discovery Service"
        }, 
        {
            "location": "/Processes/#overview", 
            "text": "Processes use the discovery service to register their own availability,\nfind other processes (by process type) and subscribe to receive updates\nfor new process availability (by process type). The discovery service\ndoes not manage connections- it simply returns tables of registered\nprocesses, irrespective of their current availability. It is up to each\nindividual process to manage its own connections.  The discovery service uses the process.csv file to make connections to\nprocesses on start up. After start up it is up to each individual\nprocess to attempt connections and register with the discovery service.\nThis is done automatically, depending on the configuration parameters.\nMultiple discovery services can be run in which case each process will\ntry to register and retrieve process details from each discovery process\nit finds in its process.csv file. Discovery services do not replicate\nbetween themselves. A discovery process must have its process type\nlisted as discovery.  To run the discovery service, use a start line such as:  aquaq $ q torq.q -load code/processes/discovery.q -p 9995  Modify the configuration as required.", 
            "title": "Overview"
        }, 
        {
            "location": "/Processes/#operation", 
            "text": "Processes register with the discovery service.     Processes use the discovery service to locate other processes.     When new services register, any processes which have registered an\n     interest in that process type are notified.", 
            "title": "Operation"
        }, 
        {
            "location": "/Processes/#available-processes", 
            "text": "The list of available processes can be found in the .servers.SERVERS\ntable.  q).servers.SERVERS                                                                                                                                                                                                                                                            \nprocname     proctype        hpup            w  hits startp                        lastp                         endp attributes                                                                   \n-------------------------------------------------------------------------------------\ndiscovery1   discovery       :aquaq:9995     0                                  2014.01.22D17:00:40.947470000      ()!()                                                                        \ndiscovery2   discovery       :aquaq:9996     0                                  2014.01.22D17:00:40.947517000      ()!()                                                                        \nhdb2         hdb             :aquaq:5013     0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \nkilltick     kill            :aquaq:20000    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntpreplay1    tickerlogreplay :aquaq:20002    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntickerplant1 tickerplant     :aquaq:5010  6  0    2014.01.22D17:00:40.967699000 2014.01.22D17:00:40.967698000      ()!()                                                                        \nmonitor1     monitor         :aquaq:20001 9  0    2014.01.22D17:00:40.971344000 2014.01.22D17:00:40.971344000      ()!()                                                                        \nrdb1         rdb             :aquaq:5011  7  0    2014.01.22D17:06:13.032883000 2014.01.22D17:06:13.032883000      `date`tables!(,2014.01.22;`fxquotes`heartbeat`logmsg`quotes`trades)          \nhdb3         hdb             :aquaq:5012  8  0    2014.01.22D17:06:18.647349000 2014.01.22D17:06:18.647349000      `date`tables!(2014.01.13 2014.01.14;`fxquotes`heartbeat`logmsg`quotes`trades)\ngateway1     gateway         :aquaq:5020  10 0    2014.01.22D17:06:32.152836000 2014.01.22D17:06:32.152836000      ()!()", 
            "title": "Available Processes"
        }, 
        {
            "location": "/Processes/#gateway", 
            "text": "A synchronous and asynchronous gateway is provided. The gateway can be\nused for load balancing and/or to join the results of queries across\nheterogeneous servers (e.g. an RDB and HDB). Ideally the gateway should\nonly be used with asynchronous calls. Synchronous calls cause the\ngateway to block so limits the gateway to serving one query at a time\n(although if querying across multiple backend servers the backend\nqueries will be run in parallel). When using asynchronous calls the\nclient can either block and wait for the result (deferred synchronous)\nor post a call back function which the gateway will call back to the\nclient with. With both asynchronous and synchronous queries the backend\nservers to execute queries against are selected using process type. The\ngateway API can be seen by querying .api.p\u201c.gw.*\u201d within a gateway\nprocess.", 
            "title": "Gateway"
        }, 
        {
            "location": "/Processes/#asynchronous-behaviour", 
            "text": "Asynchronous queries allow much greater flexibility. They allow multiple\nqueries to be serviced at once, prioritisation, and queries to be timed\nout. When an asynchronous query is received the following happens:    the query is placed in a queue;    the list of available servers is retrieved;    the queue is prioritised, so those queries with higher priority are\n      serviced first;    queries are sent to back end servers as they become available. Once\n      the backend server returns its result, it is given another query;    when all the partial results from the query are returned the results\n      are aggregated and returned to the client. They are either returned\n      directly, or wrapped in a callback and posted back asynchronously to\n      the client.    The two main customisable features of the gateway are the selection of\navailable servers (.gw.availableservers) and the queue prioritisation\n(.gw.getnextqueryid). With default configuration, the available servers\nare those servers which are not currently servicing a query from the\ngateway, and the queue priority is a simple FIFO queue. The available\nservers could be extended to handle process attributes, such as the\navailable datasets or the location of the process, and the queue\nprioritisation could be modified to anything required e.g. based on the\nquery itself, the username, host of the client etc.  An asynchronous query can be timed out using a timeout defined by the\nclient. The gateway will periodically check if any client queries have\nnot completed in the alotted time, and return a timeout error to the\nclient. If the query is already running on any backend servers then they\ncannot be timed out other than by using the standard -T flag.", 
            "title": "Asynchronous Behaviour"
        }, 
        {
            "location": "/Processes/#synchronous-behaviour", 
            "text": "When using synchronous queries the gateway can only handle one query at\na time and cannot timeout queries other than with the standard -T flag.\nAll synchronous queries will be immediately dispatched to the back end\nprocesses. They will be dispatched using an asyhcnronous call, allowing\nthem to run in parallel rather than serially. When the results are\nreceived they are aggregated and returned to the client.", 
            "title": "Synchronous Behaviour"
        }, 
        {
            "location": "/Processes/#process-discovery", 
            "text": "The gateway uses the discovery service to locate processes to query\nacross. The discovery service will notify the gateway when new processes\nbecome available and the gateway will automatically connect and start\nusing them. The gateway can also use the static information in\nprocess.csv, but this limits the gateway to a predefined list of\nprocesses rather than allowing new services to come online as demand\nrequires.", 
            "title": "Process Discovery"
        }, 
        {
            "location": "/Processes/#error-handling", 
            "text": "When synchronous calls are used, q errors are returned to clients as\nthey are encountered. When using asynchronous calls there is no way to\nreturn actual errors and appropriately prefixed strings must be used\ninstead. It is up to the client to check the type of the received result\nand if it is a string then whether it contains the error prefix. The\nerror prefix can be changed, but the default is \u201cerror: \u201d. Errors will\nbe returned when:    the client requests a query against a server type which the gateway\n    does not currently have any active instances of (this error is\n    returned immediately);    the query is timed out;    a back end server returns an error;    a back end server fails;    the join function fails.    If postback functions are used, the error string will be posted back\nwithin the postback function (i.e. it will be packed the same way as a\nvalid result).", 
            "title": "Error Handling"
        }, 
        {
            "location": "/Processes/#client-calls", 
            "text": "There are four main client calls. The .gw.sync* methods should only be\ninvoked synchronously, and the .gw.async* methods should only be\ninvoked asynchronously. Each of these are documented more extensively in\nthe gateway api. Use .api.p\u201c.gw.*\u201d for more details.     Function  Description      .gw.syncexec[query; servertypes]  Execute the specified query synchronously against the required list of servers. If more than one server, the results will be razed.    .gw.syncexecj[query; servertypes; joinfunction]  Execute the specified query against the required list of servers. Use the specified join function to aggregate the results.    .gw.asyncexec[query; servertypes]  Execute the specified query against the required list of servers. If more than one server, the results will be razed. The client must block and wait for the results.    .gw.asyncexecjpt[query; servertypes; joinfunction; postback; timeout]  Execute the specified query against the required list of servers. Use the specified join function to aggregate the results. If the postback function is not set, the client must block and wait for the results. If it is set, the result will be wrapped in the specified postback function and returned asynchronously to the client. The query will be timed out if the timeout value is exceeded.     For the purposes of demonstration, assume that the queries must be run\nacross an RDB and HDB process, and the gateway has one RDB and two HDB\nprocesses available to it.  q).gw.servers                                                                                                                                                                                                                                                                 \nhandle| servertype inuse active querycount lastquery                     usage                attributes                   \n------| --------------------------------------------------------------------\n7     | rdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:00:52.149069000 `datacentre`country!`essex`uk\n8     | hdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:01:26.143564000 `datacentre`country!`essex`uk\n9     | hdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:08.019862000 `datacentre`country!`essex`uk\n12    | rdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:04.018349000 `datacentre`country!`essex`uk  Both the RDB and HDB processes have a function f and table t defined. f\nwill run for 2 seconds longer on the HDB processes then it will the RDB.  q)f                                                                                                                                                                                                                                                                           \n{system\"sleep \",string x+$[`hdb=.proc.proctype;2;0]; t}\nq)t                                                                                                                                                                                                                                                                           \na   \n----\n5013\n5014\n5015\n5016\n5017  Run the gateway. The main parameter which should be set is the\n.servers.CONNECTIONS parameter, which dictates the process types the\ngateway queries across. Also, we need to explicitly allow sync calls. We\ncan do this from the config or from the command line.  q torq.q -load code/processes/gateway.q -p 8000 -.gw.synccallsallowed 1 -.servers.CONNECTIONS hdb rdb  Start a client and connect to the gateway. Start with a sync query. The\nHDB query should take 4 seconds and the RDB query should take 2 seconds.\nIf the queries run in parallel, the total query time should be 4\nseconds.  q)h:hopen 8000                                                                                                                                                                                                                                                                \nq)h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                            \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                         \n4009  If a query is done for a server type which is not registered, an error\nis returned:  q)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb`other)                                                                                                                                                                                                                                   \n`not all of the requested server types are available; missing other  Custom join functions can be specified:  q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by a from x} each x})                                                                                                                                                                                                  \na   | x\n----| -\n5014| 2\n5015| 2\n5016| 2\n5017| 1\n5018| 1\n5012| 1\n5013| 1  Custom joins can fail with appropriate errors:  q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by b from x} each x})                                                                                                                                                                                                  \n`failed to apply supplied join function to results: b  Asynchronous queries must be sent in async and blocked:  q)(neg h)(`.gw.asyncexec;(`f;2);`hdb`rdb); r:h(::)                                                                                                                                                                                                                          \n    /- This white space is from pressing return\n    /- the client is blocked and unresponsive\n\nq)q)q)                                                                                                                                                                                                                                                                        \nq)                                                                                                                                                                                                                                                                            \nq)r                                                                                                                                                                                                                                                                           \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)  We can send multiple async queries at once. Given the gateway has two\nRDBs and two HDBs avaialble to it, it should be possible to service two\nof these queries at the same time.  q)h:hopen each 8000 8000                                                                                                                                                                                                                                                      \nq)\\t (neg h)@\\:(`.gw.asyncexec;(`f;2);`hdb`rdb); (neg h)@\\:(::); r:h@\\:(::)\n4012\nq)r                                                                                                                                                                                                                                                                           \n+(,`a)!,5014 5015 5016 5017 5018 5012 5013 5014 5015 5016\n+(,`a)!,5013 5014 5015 5016 5017 9999 10000 10001 10002 10003  Alternatively async queries can specify a postback so the client does\nnot have to block and wait for the result. The postback function must\ntake two parameters- the first is the function that was sent up, the\nsecond is the results. The postback can either be a lambda, or the name\nof a function.  q)h:hopen 8000                                                                                                                                                                                                                                                                \nq)handleresults:{-1(string .z.z),\" got results\"; -3!x; show y}                                                                                                                                                                                                                \nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;handleresults;0Wn)                                                                                                                                                                                                           \nq)\nq)  /- These q prompts are from pressing enter\nq)  /- The q client is not blocked, unlike the previous example\nq)\nq)2014.01.07T16:53:42.481 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\n\n/- Can also use a named function rather than a lambda\nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;`handleresults;0Wn)\nq)\nq)              \nq)2014.01.07T16:55:12.235 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016  Asynchronous queries can also be timed out. This query will run for 22\nseconds, but should be timed out after 5 seconds. There is a tolerance\nof +5 seconds on the timeout value, as that is how often the query list\nis checked. This can be reduced as required.  q)(neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)\n\nq)q)q)r                                                                                                                                                                                                                                                                       \n\"error: query has exceeded specified timeout value\"\nq)\\t (neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)                                                                                                                                                                                                  \n6550", 
            "title": "Client Calls"
        }, 
        {
            "location": "/Processes/#non-kdb-clients", 
            "text": "All the examples in the previous section are from clients written in q.\nHowever it should be possible to do most of the above from non kdb+\nclients. The officially supported APIs for Java, C# and C allow the\nasynchronous methods above. For example, we can modify the try block in\nthe main function of the  Java Grid\nViewer :  import java.awt.BorderLayout;\nimport java.awt.Color;\nimport java.io.IOException;\nimport java.lang.reflect.Array;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTable;\nimport javax.swing.table.AbstractTableModel;\nimport kx.c;\n\npublic class Main {\n    public static class KxTableModel extends AbstractTableModel {\n        private c.Flip flip;\n        public void setFlip(c.Flip data) {\n            this.flip = data;\n        }\n\n        public int getRowCount() {\n            return Array.getLength(flip.y[0]);\n        }\n\n        public int getColumnCount() {\n            return flip.y.length;\n        }\n\n        public Object getValueAt(int rowIndex, int columnIndex) {\n            return c.at(flip.y[columnIndex], rowIndex);\n        }\n\n        public String getColumnName(int columnIndex) {\n            return flip.x[columnIndex];\n        }\n    };\n\n    public static void main(String[] args) {\n        KxTableModel model = new KxTableModel();\n        c c = null;\n        try {\n            c = new c(\"localhost\", 8000,\"username:password\");\n            // Create the query to send\n        String query=\".gw.asyncexec[(`f;2);`hdb`rdb]\";\n            // Send the query \n        c.ks(query);\n            // Block on the socket and wait for the result\n        model.setFlip((c.Flip) c.k());\n        } catch (Exception ex) {\n            Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex);\n        } finally {\n            if (c != null) {try{c.close();} catch (IOException ex) {}\n          }\n        }\n        JTable table = new JTable(model);\n        table.setGridColor(Color.BLACK);\n        String title = \"kdb+ Example - \"+model.getRowCount()+\" Rows\";\n        JFrame frame = new JFrame(title);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        frame.getContentPane().add(new JScrollPane(table), BorderLayout.CENTER);\n        frame.setSize(300, 300);\n        frame.setVisible(true);\n    }\n}  Some of the unofficially supported APIs may only allow synchronous calls\nto be made.", 
            "title": "Non kdb+ Clients"
        }, 
        {
            "location": "/Processes/#real-time-database-rdb", 
            "text": "The Real Time Database is a modified version of r.q found in kdb+tick.\nThe modifications from the standard r.q include:    Tickerplant (data source) and HDB location derived from processes\n    defined by the discovery service or from config file;    Automatic re-connection and resubscription to tickerplant;    List of tables to subscribe to supplied as configuration setting;    More pre-built flexibility in end-of-day;    More verbose end-of-day logging;    Reload multiple authenticated HDBs after end-of-day;    End-of-day save down manipulation code is shared between RDB, WDB\n      and tickerplant log replay    See the top of the file for more information.", 
            "title": "Real Time Database (RDB)"
        }, 
        {
            "location": "/Processes/#write-database-wdb", 
            "text": "The Write Database or WDB is based on w.q. This process features a\nnumber of modifications and enhancements over w.q:    Provides the option to write down to a custom partition scheme,\n    defined by parted columns in sort.csv, which removes the need for\n    end of day sorting;    Greater configuration options; max rows on a per table basis, list\n     subscription tables, upd function etc. See the top of the process\n     file for the options;    Use of common code with the RDB and Tickerplant Log Replay process\n     to manipulate tables before save, sort and apply attributes;    Checks whether to persist data to disk on a timer rather than on\n     each tick;    Informs other RDB, HDB and GW processes that end of day save and\n     sort has completed;    More log information supplied.    The WDB process can broken down into two main functions:    Periodically saving data to disk and    Sorting data at end of day    The WDB process provides flexibility so it can be set-up as a\nstand-alone process that will both save and sort data or two separate\nprocesses (one that saves the data and another that will sort the data\non disk). This allows greater flexibility around the end of day event as\nsorting data can be time consuming. It is also helps when implementing\nseemless rollovers (i.e. no outage window at end-of-day).  The behaviour of the WDB process is controlled by the  .wdb.mode \nparameter. This should be set to one of following three values:    saveandsort - the process will subscribe for data, periodically\n    write data to disk and at EOD it will flush remaining data to disk\n    before sorting it and informing GWs, RDBs and HDBs etc.    save - the process will subscribe for data, periodically write data\n      to disk and at EOD it will flush remaining data to disk. It will\n      then inform its respective sort mode process to sort the data    sort - the process will wait to get a trigger from its respective\n      save mode process. When this is triggered it will sort the data on\n      disk, apply attributes and the trigger a reload on the RDB, HDB and\n      GW processes    When running a system with separate save and sort process, the sort\nprocess should be configured in the processes.csv file with a proctype\nof sort. The save process will check for processes with a proctype of\nsort when it attempts to trigger the end of day sort of the data.  The wdb process provides two methods for persisting data to disk and\nsorting at the end of the day.    default - Data is persisted into a partition defined by the\n    [partitiontype] variable, similar to the hdb partition scheme. The\n    general scheme is of the form\n    [wdbdir]/[partitiontype]/[table]/. And a typical partition\n    directory would be similar to wdb/database/2015.11.26/trades/. At\n    the end of the day, before being moved to the hdb, the data is\n    sorted according to parameters defined in sort.csv. For each table,\n    sort.csv will specify the columns to sort (using xasc) and apply\n    attributes to.    partbyattr - Data is persisted to a custom partition scheme, derived\n      from parameters in the sort.csv file. The write down scheme is taken\n      from sort.csv, to reflect the effect of using xasc at the end of\n      day. For each table, the columns defined in sort.csv, with the\n      parted attribute, are used to create custom partitions in the wdb.\n      Multiple columns can be defined with the parted attribute and\n      distinct combinations of each are generated for custom partitions.\n      The general partition scheme is of the form\n      [wdbdir]/[partitiontype]/[table]/[parted column(s)]/. And a\n      typical partition directory would be similar to\n      wdb/database/2015.11.26/trade/MSFT_N. In the above example, the\n      data is parted by sym and source, and so a unique partition\n      directory MSFT_N is created in the wdb directory.  At the end of the day, data is upserted into the hdb without the\n  need for sorting. The number of rows that are joined at once is\n  limited by the mergenumrows and mergenumtab parameters.    The optional partbyattr method may provide a significant saving in time\nat the end of day, allowing the hdb to be accessed sooner. For large\ndata sets with a low cardinality (ie. small number of distinct elements)\nthe optional method may provide a significant time saving, upwards of\n50%. The optional method should also reduce the memory usage at the end\nof day event, as joining data is generally less memory intensive than\nsorting.", 
            "title": "Write Database (WDB)"
        }, 
        {
            "location": "/Processes/#tickerplant-log-replay", 
            "text": "The Tickerplant Log Replay script is for replaying tickerplant logs.\nThis is useful for:    handling end of day save down failures;    handling large volumes of data (larger than can fit into RAM).    The process takes as the main input either an individual log file to\nreplay, or a directory containing a set of log files. Amongst other\nfunctionality, the process can:    replay specific message ranges;    replay in manageable message chunks;    recover as many messages as possible from a log file rather than\n      just stopping at the first bad message;    ignore specific tables;    modify the tables before or after they are saved;    apply sorting and parting after all the data is written out.    The process must have some variables set (the tickerplant log file or\ndirectory, the schema file, and the on-disk database directory to write\nto) or it will fail on startup. These can either be set in the config\nfile, or overridden from the command line in the usual way. An example\nstart line would be:  q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.tplogfile ../test/tplogs/marketdata2013.12.17 -.replay.schemafile ../test/marketdata.q -.replay.hdbdir ../test/hdb1  The tickerplant log replay script has extended usage information which\ncan be accessed with -.replay.usage.  q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.usage", 
            "title": "Tickerplant Log Replay"
        }, 
        {
            "location": "/Processes/#housekeeping", 
            "text": "The housekeeping process is used to undertake periodic system\nhousekeeping and maintenance, such as compressing or removing files\nwhich are no longer required. The process will run the housekeeping jobs\nperiodically on a timer. Amongst other functionality the process:    Allows for removing and zipping of directory files;    Provides an inbuilt search utility and selectively searches using a\n      \u2018find\u2019 and \u2018exclude\u2019 string, and an \u2018older than\u2019 parameter;    Reads all tasks from a single CSV;    Runs on a user defined timer;    Can be run immediately from command line or within the process;    Can be easily extended to include new user defined housekeeping\n      tasks.    The process has two main parameters that should be set prior to use;\nruntimes and inputcsv.\u2018Runtimes\u2019 sets the timer to run housekeeping at\nthe set time(s), and \u2018Inputcsv\u2019 provides the location of the\nhousekeeping csv file. These can either be set in the config file, or\noverridden via the command line. If these are not set, then default\nparameters are used; 12.00 and \u2018KDBCONFIG/housekeeping.csv\u2019\nrespectively. The process is designed to run from a single csv file with\nfive headings:    Function details the action that you wish to be carried out on the\n    files, initially, this can be rm (remove) and zip (zipping);    Path specifies the directory that the files are in;    Match provides the search string to the find function, files\n      returned will have names that match this string;    Exclude provides a second string to the find function, and these\n      files are excluded from the match list;    Age is the \u2018older than\u2019 parameter, and the function will only be\n      carried out on files older than the age given (in days).    An example csv file would be:  function,path,match,exclude,age\nzip,./logs/,*.log,*tick*,2\nrm,./logs/,*.log*,*tick*,4\nzip,./logs/,*tick*,,1\nrm,./logs/,*tick*,,3\n\nfunction path      match    exclude  age\n----------------------------------------\nzip      \"./logs/\" \"*.log\"  \"*tick*\" 2\nrm       \"./logs/\" \"*.log*\" \"*tick*\" 4\nzip      \"./logs/\" \"*tick*\" \"\"       1\nrm       \"./logs/\" \"*tick*\" \"\"       3  The process reads in the csv file, and passes it line by line to a\n\u2018find\u2019 function; providing a dictionary of values that can be used to\nlocate the files required. The find function takes advantage of system\ncommands to search for the files according to the specifications in the\ndictionary. A search is performed for both the match string and the\nexclude string, and cross referenced to produce a list of files that\nmatch the parameters given. The files are then each passed to a further\nset of system commands to perform the task of either zipping or\nremoving. Note that an incomplete csv or non-existant path will throw an\nerror.  The remove and zipping functions form only basic implimentations of the\nhousekeeping process; it is designed to be exended to include more\nactions than those provided. Any user function defined in the\nhousekeeping code can be employed in the same fashion by providing the\nname of the function,search string and age of files to the csv.  As well as being scheduled on a timer, the process can also be run\nimmediately. Adding \u2018-hk.runnow 1\u2019 to the command line when starting the\nprocess will force immediate running of the actions in the housekeeping\ncsv. Likewise, setting runnow to 1b in the config file will immediately\nrun the cleaning process. Both methods will cause the process to exit\nupon completion. Calling hkrun[] from within the q process will also\nrun the csv instructions immediately. This will not affect any timer\nscheduling and the process will remain open upon completion.  Housekeeping works both on windows and unix based systems. Since the\nprocess utilizes inbuilt system commands to perform maintenances, a\nunix/windows switch detects the operating system of the host and applies\neither unix or widows functions appropriately. Extensions need only be\nmade in the namespace of the hosting operating system (i.e. if you are\nusing a unix system, and wish to add a new function, you do not need to\nadd the function to the windows namespace to). Usage information can be\naccessed using the \u2018-hkusage\u2019 flag:  q torq.q -load code/processes/housekeeping.q -p 9999 -proctype housekeeping -procname hk1 -debug -hkusage", 
            "title": "Housekeeping"
        }, 
        {
            "location": "/Processes/#file-alerter", 
            "text": "The file alerter process is a long-running process which periodically\nscans a set of directories for user-specified files. If a matching file\nis found it will then carry out a user-defined function on it. The files\nto search for and the functions to run are read in from a csv file.\nAdditionally, the file alerter process can:    run more than one function on the specified file.    optionally move the file to a new directory after running the\n      function.    store a table of files that have already been processed.    run the function only on new files or run it every time the file is\n      modified.    ignore any matching files already on the system when the process\n      starts and only run a function if a new file is added or a file is\n      modified.    The file alerter process has four parameters which should be set prior\nto use. These parameters can either be set in the config file or\noverridden on the command-line. If they are not set, the default\nparameters will be used. The parameters are as follows.  inputcsv  - The name and location of the csv file which defines the\nbehaviour of the process. The default is KDBCONFIG/filealerter.csv.  polltime  - How often the process will scan for matching files. The\ndefault is 0D:00:01, i.e., every minute.  alreadyprocessed  - The name and location of the already-processed\ntable. The default is KDBCONFIG/filealerterprocessed. This table will\nbe created automatically the first time the process is ran.  skipallonstart  - If this is set to 1, it will ignore all files\nalready on the system; if it is set to 0, it will not. The default\nvalue is 0.  The files to find and the functions to run are read in from a csv file\ncreated by the user. This file has five columns, which are detailed\nbelow.  path  - This is the path to the directory that will be scanned for\nthe file.  match  - This is a search string matching the name of the file to be\nfound. Wildcards can be used in this search, for example, \u201cfile*\u201d will\nfind all files starting with \u201cfil\u201d.  function  - This is the name of the function to be run on the file.\nThis function must be defined in the script\nKDBCODE/processes/filealerter.q. If the function is not defined or fails\nto run, the process will throw an error and ignore that file from then\non.  newonly  - This is a boolean value. If it is set to 1, it will\nonly run the function on the file if it has been newly created. If it is\nset to 0, then it will run the function every time the file is\nmodified.  movetodirectory  - This is the path of the directory you would like\nto move the file to after it has been processed. If this value is left\nblank, the file will not be moved.  It is possible to run two separate functions on the same file by adding\nthem as separate lines in the csv file. If the file is to be moved after\nit is processed, the file alerter will run both functions on the file\nand then attempt to move it. A typical csv file to configure the file\nalerter would look like:  path,match,function,newonly,movetodirectory\n/path/to/dirA,fileA.*,copy,0,/path/to/newDir\n/path/to/dirB,fileB.txt,email,1,\n/path/to/dirA,fileA.*,delete,0,/path/to/newDir\n\npath        match      function  newonly    movetodirectory\n---------------------------------------------------\n\"/path/to/dirA\" \"fileA.*\"   copy    0   \"/path/to/newDir\"\n\"/path/to/dirB\" \"fileB.txt\" email   1   \"\"\n\"/path/to/dirA\" \"fileA.*\"   delete  0   \"/path/to/newDir\"  The file alerter process reads in each line of the csv file and searches\nfiles matching the search string specified in that line. Note that there\nmay be more than one file found if a wildcard is used in the search\nstring. If it finds any files, it will check that they are not in the\nalready processed table. If newonly is set to 1, it only checks if\nthe filename is already in the table. If newonly is set to 0, it\nchecks against the filename, filesize and a md5 hash of the file. The\nmd5 hash and the filesize are used to determine if the file has been\nmodified since it was processed last. If the found files have not been\nprocessed already, it then attempts to run the specified function to\nthese files.  After the process has run through each line of the csv, it generates a\ntable of all files that were processed on that run. These files are\nappended to the already processed table which is then saved to disk. The\nfile alerter will attempt to move the files to the \u2018movetodirectory\u2019, if\nspecified. If the file has already been moved during the process (for\nexample, if the function to run on it was \u2018delete\u2019), the file alerter\nwill not attempt to move it.  The file alerter is designed to be extended by the user. Customised\nfunctions should be defined within the filealerter.q script. They should\nbe diadic functions, i.e., they take two parameters: the path and the\nfilename. As an example, a simple function to make a copy of a file in\nanother directory could be:  copy:{[path;file] system \"cp \", path,\"/\", file, \" /path/to/newDir\"}  Although the process is designed to run at regular intervals throughout\nthe day, it can be called manually by invoking the FArun[] command\nfrom within the q session. Similarly, if new lines are added to the csv\nfile, then it can be re-loaded by calling the loadcsv[] command\nfrom the q session.  Each stage of the process, along with any errors which may occur, are\nappropriately logged in the usual manner.  The file alerter process is designed to work on both Windows and Unix\nbased systems. Since many of the functions defined will use inbuilt\nsystem command they will be need to written to suit the operating system\nin use. It should also be noted that Windows does not have an inbuilt\nmd5 hashing function so the file alerter will only detect different\nversions of files if the filename or filesize changes.", 
            "title": "File Alerter"
        }, 
        {
            "location": "/Processes/#reporter", 
            "text": "", 
            "title": "Reporter"
        }, 
        {
            "location": "/Processes/#overview_1", 
            "text": "The reporter process is used to run periodic reports on specific\nprocesses. A report is the result of a query that is run on a process at\na specific time. The result of the query is then handled by one of the\ninbuilt result handlers, with the ability to add custom result handlers.   Features:    Easily create a report for information that you want;    Fully customizable scheduling such as start time, end time and days\n      of the week;    Run reports repeatedly with a custom period between them;    Asynchronous querying with custom timeout intervals;    Inbuilt result handlers allow reports to be written to file or\n      published;    Custom result handlers can be defined;    Logs each step of the report process;    Fully integrated with the TorQ gateway to allow reports to be run\n      across backend processes.    The reporter process has three parameters that are read in on\ninitialisation from the reporter.q file found in the\n$KDBCONFIG/settings directory. These settings are the string filepath\nof the input csv file, a boolean to output log messages and timestamp\nfor flushing the query log table.  To run the reporter process:  q torq.q -load code/processes/reporter.q -p 20004  Once the reporter process has been initiated, the reports will be\nscheduled and no further input is required from the user.", 
            "title": "Overview"
        }, 
        {
            "location": "/Processes/#report-configuration", 
            "text": "By default, the process takes its inputs from a file called reporter.csv\nwhich is found in the $KDBCONFIG directory. This allows the user\ncomplete control over the configuration of the reports. As the queries\nare evaluated on the target process, local variables can be referenced\nor foreign functions can be run. Table [table:reportertable] shows the\nmeaning of the csv schema.     Column Header  Description and Example      name  Report name e.g. Usage    query  Query to be evaluated on that process. It can be a string query or function    resulthandler  Result handlers are run on the returned result. Custom result handlers can be added. The result handler must be a monadic function with the result data being passed in e.g. writetofile[\u201c./output\u201d;\u201cusage\u201d]    gateway  If non null the reporter will query processes route the query to the proctype specified in this field. The values in the proctype field will be the process types on which the gateway runs the backend query. e.g. `gateway    joinfunction  Used to join the results when a gateway query is being used. The choice of joinfunction must take into account the result that will be received. The function must be monadic and the parameter will be the list of results returned from the backend processes e.g. raze    proctype  The type of process that the report will be run on. If the gateway field is not empty this may be a list of process types, otherwise the reporter will throw an error on startup. e.g. `rdb    procname  The name of a specific process to run the report on. If left null, the reporter process will select a random process with the specified proctype. If the gateway field is not null, this field specifies the specific gateway process name to run the query against e.g. `hdb1    start  Time on that day to start at e.g. 12:00    end  Time on that day that the report will stop at e.g. 23:00    period  The period between each report query e.g. 00:00:10    timeoutinterval  The amount of time the reporter waits before timing out a report e.g. 00:00:30    daysofweek  Numeric value required for the day of the week. Where 0 is Saturday and 2 is Monday     When running a report on a gateway, the gateway field must be set to the\nproctype of the gateway that will be queried. It will then run the\nreport on the processes which are listed in the proctype field and join\nthe results by using the function specified in the joinfunction field.\nIf there is no join function then the reporter process will not start.\nMultiple entries in the proctype field must be separated by a space and\nare only allowed when the gateway field is not empty. If gateway field\nis empty and there are multiple entries in the proctype field then the\nreporter process will not load.  Listing [code:csvschema] shows an example of the schema needed in the\ninput csv file.  name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nusage|10#.usage.usage|writetofiletype[\"./output/\";\"usage\";\"csv\"]|||rdb||00:01|23:50|00:01|00:00:01|0 1 2 3 4 5 6\nmemory|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|||rdb|rdb1|00:05|18:00|00:01|00:00:08|0 1 2 3 4 5 6\nusage_gateway|10#.usage.usage||gateway|raze|rdb hdb||00:02|22:00|00:01|00:00:10|0 1 2 3 4 5 6", 
            "title": "Report Configuration"
        }, 
        {
            "location": "/Processes/#result-handlers", 
            "text": "There are several default result handlers which are listed below. Custom\nresult handlers can be defined as required. The result handler will be\ninvoked with a single parameter (the result of the query).  writetofiletype  - Accepts 3 parameters: path, filename, filetype and\ndata. When writing to file it uses a date time suffix so the resultant\nfilename will be  usage_rdb_2014_01_02_15_00_12.txt  e.g.  writetofiletype[\"./output/\";\"usage\";\"csv\"]  splaytable  - This accepts 3 parameters: path, file and data. This\nsplays the result to a directory. The result must be a table in order to\nuse this function e.g.  splaytable[\"./output/\";\"usage\"]  emailalert  - This accepts 3 parameters: period, recipient list and\ndata. The period dictates the throttle i.e. emails will be sent at most\nevery period. The result of the report must be a table with a single\ncolumn called messages which contains the character list of the email\nmessage. This is used with the monitoring checks to raise alerts, but\ncan be used with other functions.  emailalert[0D00:30;(\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\")]  emailreport  - This accepts 3 parameters: temporary path, recipient\nlist, file name, file type and data. The data is written out as the file\ntype (e.g. csv, xml, txt, xls, json) with the given file name to the\ntemporary path. It is then emailed to the recipient list, and the\ntemporary file removed.  emailreport[\"./tempdir/\"; (\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\"); \"EndOfDayReport\"; \"csv\"]  publishresult  - Accepts 1 parameter and that is the data. This is\ndiscussed later in the subsection\u00a0subresults.\nCustom result handlers can be added to $KDBCODE/processes/reporter.q .\nIt is important to note that the result handler is referencing local\nfunctions as it is executed in the reporter process and not the target\nprocess. When the query has been successful the result handler will be\npassed a dictionary with the following keys: queryid, time, name,\nprocname, proctype and result.", 
            "title": "Result Handlers"
        }, 
        {
            "location": "/Processes/#report-process-tracking", 
            "text": "Each step of the query is logged by the reporter process. Each query is\ngiven a unique id and regular system messages are given the id 0. The\nstage column specifies what stage the query is in and these are shown in\ntable [table:stagetable]. An appropriate log message is also shown so\nany problems can easily be diagnosed. The in memory table is flushed\nevery interval depending on the value of the flushqueryloginterval\nvariable in the reporter.q file found in the $KDBCONFIG/settings\ndirectory.      Stage symbol  Explanation      R  The query is currently running    E  An error has occurred during the query    C  The query has been completed with no errors    T  The query has exceeded the timeout interval    S  System message e.g. \u201cReporter Process Initialised\u201d     time                         | queryid stage message\n-----------------------------| ------------------------------------------------------------------------\n2014.10.20D22:20:06.597035000| 37 R \"Received result\"\n2014.10.20D22:20:06.600692000| 37 R \"Running resulthandler\"\n2014.10.20D22:20:06.604455000| 37 C \"Finished report\"\n2014.10.20D22:30:00.984572000| 38 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:00.991862000| 38 R \"Received result\"\n2014.10.20D22:30:00.995527000| 38 R \"Running resulthandler\"\n2014.10.20D22:30:00.999236000| 38 C \"Finished report\"\n2014.10.20D22:30:06.784419000| 39 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:06.796431000| 39 R \"Received result\"", 
            "title": "Report Process Tracking"
        }, 
        {
            "location": "/Processes/#subscribing-for-results", 
            "text": "To publish the results of the report, the reporter process uses the pub\nsub functionality of TorQ. This is done by using the using the inbuilt\nresult handler called publishresult. In order to subscribe to this feed,\nconnect to the reporter process and send the function shown below over\nthe handle. To subscribe to all reports use a backtick as the second\nparameter and to subscribe to a specific reports results include the\nreporter name as a symbol.  /- define a upd function\nupd:insert\n\n/- handle to reporter process\nh: hopen 20004\n\n/- Subscribe to all results that use the publishresult handler\nh(`.ps.subscribe;`reporterprocessresults;`)\n\n/- Subscribe to a specific report called testreport\nh(`.ps.subscribe;`reporterprocessresults;`testreport)", 
            "title": "Subscribing for Results"
        }, 
        {
            "location": "/Processes/#example-reports", 
            "text": "The following are examples of reports that could be used in the reporter\nprocess. The rdbtablecount report will run hourly and return the count\nof all the tables in a rdb process. The memoryusage report will run\nevery 10 minutes against the gateway for multiple processes and will\nreturn the  .Q.w[]  information. Both of these reports run between\n9:30am to 4:00pm during the weekdays. The report onetimequery is an\nexample of a query that is run one time, in order to run a query once,\nthe period must be the same as the difference between the start and end\ntime.  name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nrdbtablecount|ts!count each value each ts:tables[]|{show x`result}|||rdb|rdb1|09:30|16:00|01:00|00:00:10|2 3 4 5 6\nmemoryusage|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|gateway1|{enlist raze x}|rdb hdb||09:30|16:00|00:10|00:00:10|2 3 4 5 6\nonetimequery|10#.usage.usage|writetofile[\"./output/\";\"onetime.csv\"]|||rdb||10:00|10:01|00:01|00:00:10|2 3 4 5 6", 
            "title": "Example reports"
        }, 
        {
            "location": "/Processes/#monitor", 
            "text": "The Monitor process is a simple process to monitor the health of the\nother processes in the system. It connects to each process that it finds\n(by default using the discovery service, though can use the static file\nas well) and subscribes to both heartbeats and log messages. It\nmaintains a keyed table of heartbeats, and a table of all log messages\nreceived.  Run it with:  aquaq $ q torq.q -load code/processes/monitor.q -p 20001  It is probably advisable to run the monitor process with the -trap flag,\nas there may be some start up errors if the processes it is connecting\nto do not have the necessary heartbeating or publish/subscribe code\nloaded.  aquaq $ q torq.q -load code/processes/monitor.q -p 20001 -trap  The current heartbeat statuses are tracked in .hb.hb, and the log\nmessages in logmsg  q)show .hb.hb                                                                                                                                                                                                                                                                 \nsym       procname    | time                          counter warning error\n----------------------| ---------------------------------------------------\ndiscovery discovery2  | 2014.01.07D13:24:31.848257000 893     0       0    \nhdb       hdb1        | 2014.01.07D13:24:31.866459000 955     0       0    \nrdb       rdb_europe_1| 2014.01.07D13:23:31.507203000 901     1       0    \nrdb       rdb1        | 2014.01.07D13:24:31.848259000 34      0       0\n\nq)show select from logmsg where loglevel=`ERR                                                                                                              \ntime                          sym  host  loglevel id      message                               \n-------------------------------------------------------------------------------------\n2014.01.07D12:25:17.457535000 hdb1 aquaq ERR      reload  \"failed to reload database\"           \n2014.01.07D13:29:28.784333000 rdb1 aquaq ERR      eodsave \"failed to save tables : trade, quote\"", 
            "title": "Monitor"
        }, 
        {
            "location": "/Processes/#html5-front-end", 
            "text": "A HTML5 front end has been built to display important process\ninformation that is sent from the monitor process. It uses HTML5,\nWebSockets and JavaScript on the front end and interacts with the\nmonitor process in the kdb+ side. The features of the front end include:    Heartbeat table with processes that have warnings highlighted in\n    orange and errors in red    Log message table displaying the last 30 errors    Log message error chart that is by default displayed in 5 minute\n      bins    Chart\u2019s bin value can be changed on the fly    Responsive design so works on all main devices i.e. phones, tablets\n      and desktop    It is accessible by going to the url  http://HOST:PORT/.non?monitorui", 
            "title": "HTML5 front end"
        }, 
        {
            "location": "/Processes/#compression", 
            "text": "The compression process is a thin wrapper around the compression utility\nlibrary. It allows periodic compression of whole or parts of databases\n(e.g. data is written out uncompressed and then compressed after a\ncertain period of time). It uses four variables defined in\nKDBCONFIG/settings/compression.q which specify    the compression configuration file to use    the database directory to compress    the maximum age of data to attempt to compress    whether the process should exit upon completion    The process is run like other TorQ processes:  q torq.q -load code/processes/compression.q -p 20005  Modify the settings file or override variables from the command line as\nappropriate.", 
            "title": "Compression"
        }, 
        {
            "location": "/Processes/#kill", 
            "text": "The kill process is used to connect to and terminate currently running\nprocesses. It kills the process by sending the exit command therefore\nthe kill process must have appropriate permissions to send the command,\nand it must be able to create a connection (i.e. it will not be able to\nkill a blocked process in the same way that the unix command kill -9\nwould). By default, the kill process will connect to the discovery\nservice(s), and kill the processes of the specified types. The kill\nprocess can be modified to not use the discovery service and instead use\nthe process.csv file via the configuration in the standard way.  If run without any command line parameters, kill.q will try to kill each\nprocess it finds with type defined by its .servers.CONNECTIONS variable.  q torq.q -load code/processes/kill.q -p 20000  .servers.CONNECTIONS can optionally be overridden from the command line\n(as can any other process variable):  q torq.q -load code/processes/kill.q -p 20000 -.servers.CONNECTIONS rdb tickerplant  The kill process can also be used to kill only specific named processes\nwithin the process types:  q torq.q -load code/processes/kill.q -p 20000 -killnames hdb1 hdb2", 
            "title": "Kill"
        }, 
        {
            "location": "/Processes/#chained-tickerplant", 
            "text": "In tick+ architecture the main tickerplant is the most important\ncomponent, as it is relied upon by all the real time subscribers. When\nthe tickerplant goes down data will be lost, compare this to an rdb\nwhich can be recovered after it fails. The chained tickerplant process\nis an additional tickerplant that is a real time subscriber to the main\ntickerplant but replicates its behaviour. It will have its own real time\nsubscribers and can be recovered when it fails. This is the recommended\napproach when users want to perform their own custom real time analysis.  The chained tickerplant can:    subscribe to specific tables and syms    batch publish at an interval or publish tick by tick    create a tickerplant log that its real time subscribers can replay    replay the source tickerplant log    To launch the chained tickerplant  q torq.q -load code/processes/chainedtp.q -p 12009  Chained tickerplant settings are found in  config/settings/chainedtp.q \nand are under the  .ctp  namespace.     Setting  Explanation  Default      tickerplantname  list of tickerplant names to try and make a connection to  `tickerplant1    pubinterval  publish batch updates at this interval. If the value is 0D00:00:00 then it will publish tick by tick  0D00:00:00    tpconnsleep  number of seconds between attempts to connect to the source tickerplant  10    createlogfile  create a log file  0b    logdir  directory containing chained tickerplant logs  `:hdb    subscribeto  subscribe to these tables only (null for all)  `    subscribesyms  subscribe to these syms only (null for all)  `    replay  replay the tickerplant log file  0b    schema  retrieve schema from tickerplant  1b    clearlogonsubscription  clear log on subscription, only called if createlogfile is also enabled  0b", 
            "title": "Chained Tickerplant"
        }, 
        {
            "location": "/Processes/#integration-with-kdbtick", 
            "text": "AquaQ TorQ can be fully integrated with kdb+tick. For further details,\nuse one of the AquaQ TorQ Starter packs to set up a production kdb+ data\ncapture system.", 
            "title": "Integration with kdb+tick"
        }, 
        {
            "location": "/visualisation/", 
            "text": "Visualisation\n\n\nkdb+ supports websockets and so HTML5 GUIs can be built. We have\nincorporated a set of server side and client side utilities to ease HTML\nGUI development.\n\n\n\n\nkdb+ Utilities\n\n\nThe server side utilities are contained in html.q. These utilise some\ncommunity code, specifically json.k and a modified version of u.q, both\nfrom Kx Systems. The supplied functionality includes:\n\n\n\n\n\n\njson.k provides two way conversion between kdb+ data structures and\n    JSON;\n\n\n\n\n\n\nu.q is the standard pub/sub functionality provided with kdb+tick,\n    and a modified version is incorporated to publish data structures\n    which can be easily interpreted in JavaScript;\n\n\n\n\n\n\nfunctions for reformatting temporal types to be JSON compliant;\n\n\n\n\n\n\npage serving to utilise the inbuilt kdb+ webserver to serve custom\n    web pages. An example would be instead of having to serve a page\n    with a hardcoded websocket connection host and port, the kdb+\n    process can serve a page connecting back to itself no matter which\n    host or port it is running on.\n\n\n\n\n\n\n\n\nJavaScript Utilities\n\n\nThe JavaScript utilities are contained in kdbconnect.js. The library\nallows you to:\n\n\n\n\n\n\ncreate a connection to the kdb+ process;\n\n\n\n\n\n\ndisplay the socket status;\n\n\n\n\n\n\nsending queries;\n\n\n\n\n\n\nbinding results returned from kdb+ to updates in the webpage.\n\n\n\n\n\n\n\n\nOutline\n\n\nAll communication between websockets and kdb+ is asynchronous. The\napproach we have adopted is to ensure that all data sent to the web\nbrowser is encoded as a JSON object containing a tag to enable the web\npage to decipher what the data relates to. The format we have chosen is\nfor kdb+ to send dictionaries of the form:\n\n\n`name`data!(\"dataID\";dataObject)\n\n\n\nAll the packing can be done by .html.dataformat. Please note that the\ntemporal types are converted to longs which can easily be converted to\nJavaScript Date types. This formatting can be modified in the formating\ndictionary .html.typemap.\n\n\nq)a:flip `minute`time`date`month`timestamp`timespan`datetime`float`sym!enlist each (09:00; 09:00:00.0;.z.d; `month$.z.d; .z.p; .z.n;.z.z;20f;`a)\nq).html.dataformat[\"start\";(enlist `tradegraph)!enlist a]\nname| \"start\"\ndata| (,`tradegraph)!,+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a)\nq)first (.html.dataformat[\"start\";(enlist `tradegraph)!enlist a])[`data;`tradegraph]                                                                                     \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a\n\n\n\nWe have also extended this structure to allow web pages to receive data\nin a way similar to the standard kdb+tick pub/sub format. In this case,\nthe data object looks like:\n\n\n`name`data!(\"upd\";`tablename`tabledata!(`trade;([]time:09:00 09:05 09:10; price:12 13 14)))\n\n\n\nThis can be packed with .html.updformat:\n\n\nq).html.updformat[\"upd\";`tablename`tabledata!(`trade;a)]                                                                                                                 \nname| \"upd\"\ndata| `tablename`tabledata!(`trade;+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a))\nq)first(.html.updformat[\"upd\";`tablename`tabledata!(`trade;a)])[`data;`tabledata]                                                                                        \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a\n\n\n\nTo utilise the pub/sub functionality, the web page must connect to the\nkdb+ process and subscribe for updates. Subscriptions are done using\n\n\n.html.wssub[`tablename]\n\n\n\nPublications from the kdb+ side are done with\n\n\n.html.pub[`tablename;tabledata]\n\n\n\nOn the JavaScript side the incoming messages (data events) must be bound\nto page updates. For example, there might be an initialisation event\ncalled \u201cstart\u201d which allows the web page to retrieve all the initial\ndata from the process. The code below redraws the areas of the page with\nthe received data.\n\n\n/* Bind data - Data type \"start\" will execute the callback function */\nKDBCONNECT.bind(\"data\",\"start\",function(data){\n  // Check that data is not empty\n  if(data.hbtable.length !== 0)\n   // Write HTML table to div element with id heartbeat-table\n   { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.hbtable));}\n  if(data.lmtable.length !== 0)\n   // Write HTML table to div element with id logmsg-table\n   { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.lmtable));}  \n  if(data.lmchart.length !== 0)\n   // Log message error chart\n   { MONITOR.barChart(data.lmchart,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n  });\n\n\n\nSimilarly the upd messages must be bound to page updates. In this case,\nthe structure is slightly different:\n\n\nKDBCONNECT.bind(\"data\",\"upd\",function(data){\n  if(data.tabledata.length===0) return;\n  if(data.tablename === \"heartbeat\")\n    { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"logmsg\")\n    { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"lmchart\")\n    { MONITOR.barChart(data.tabledata,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n });\n\n\n\nTo display the WebSocket connection status the event \u201cws_event\u201d must be\nbound and it will output one of these default messages: \u201cConnecting...\u201d,\n\u201cConnected\u201d and \u201cDisconnected\u201d depending on the connection state of the\nWebSocket. Alternatively the value of the readyState attribute will\ndetermine the WebSocket status.\n\n\n// Select html element using jQuery\nvar $statusMsg = $(\"#status-msg\");  \nKDBCONNECT.bind(\"ws_event\",function(data){\n  // Data is the default message string\n  $statusMsg.html(data);\n});\nKDBCONNECT.core.websocket.readyState // Returns 1 if connected.\n\n\n\nErrors can be displayed by binding the event called \u201cerror\u201d.\n\n\nKDBCONNECT.bind(\"error\",function(data){\n  $statusMsg.html(\"Error - \" + data);\n});\n\n\n\n\n\nExample\n\n\nA basic example is provided with the Monitor process. To get this to\nwork, u.q from kdb+tick should be placed in the code/common directory to\nallow all processes to publish updates. It should be noted that this is\nnot intended as a production monitoring visualisation screen, moreso a\ndemonstration of functionality. See section\u00a0monitorgui for more\ndetails.\n\n\n\n\nFurther Work\n\n\nFurther work planned includes:\n\n\n\n\n\n\nallow subscriptions on a key basis- currently all subscribers\n    receive all updates;\n\n\n\n\n\n\nadd JavaScript controls to allow in-place updates based on key\n    pairs, and scrolling window updates e.g. add N new rows to\n    top/bottom of the specified table;\n\n\n\n\n\n\nallow multiple websocket connections to be maintained at the same\n    time.", 
            "title": "Visualisation"
        }, 
        {
            "location": "/visualisation/#visualisation", 
            "text": "kdb+ supports websockets and so HTML5 GUIs can be built. We have\nincorporated a set of server side and client side utilities to ease HTML\nGUI development.", 
            "title": "Visualisation"
        }, 
        {
            "location": "/visualisation/#kdb-utilities", 
            "text": "The server side utilities are contained in html.q. These utilise some\ncommunity code, specifically json.k and a modified version of u.q, both\nfrom Kx Systems. The supplied functionality includes:    json.k provides two way conversion between kdb+ data structures and\n    JSON;    u.q is the standard pub/sub functionality provided with kdb+tick,\n    and a modified version is incorporated to publish data structures\n    which can be easily interpreted in JavaScript;    functions for reformatting temporal types to be JSON compliant;    page serving to utilise the inbuilt kdb+ webserver to serve custom\n    web pages. An example would be instead of having to serve a page\n    with a hardcoded websocket connection host and port, the kdb+\n    process can serve a page connecting back to itself no matter which\n    host or port it is running on.", 
            "title": "kdb+ Utilities"
        }, 
        {
            "location": "/visualisation/#javascript-utilities", 
            "text": "The JavaScript utilities are contained in kdbconnect.js. The library\nallows you to:    create a connection to the kdb+ process;    display the socket status;    sending queries;    binding results returned from kdb+ to updates in the webpage.", 
            "title": "JavaScript Utilities"
        }, 
        {
            "location": "/visualisation/#outline", 
            "text": "All communication between websockets and kdb+ is asynchronous. The\napproach we have adopted is to ensure that all data sent to the web\nbrowser is encoded as a JSON object containing a tag to enable the web\npage to decipher what the data relates to. The format we have chosen is\nfor kdb+ to send dictionaries of the form:  `name`data!(\"dataID\";dataObject)  All the packing can be done by .html.dataformat. Please note that the\ntemporal types are converted to longs which can easily be converted to\nJavaScript Date types. This formatting can be modified in the formating\ndictionary .html.typemap.  q)a:flip `minute`time`date`month`timestamp`timespan`datetime`float`sym!enlist each (09:00; 09:00:00.0;.z.d; `month$.z.d; .z.p; .z.n;.z.z;20f;`a)\nq).html.dataformat[\"start\";(enlist `tradegraph)!enlist a]\nname| \"start\"\ndata| (,`tradegraph)!,+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a)\nq)first (.html.dataformat[\"start\";(enlist `tradegraph)!enlist a])[`data;`tradegraph]                                                                                     \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a  We have also extended this structure to allow web pages to receive data\nin a way similar to the standard kdb+tick pub/sub format. In this case,\nthe data object looks like:  `name`data!(\"upd\";`tablename`tabledata!(`trade;([]time:09:00 09:05 09:10; price:12 13 14)))  This can be packed with .html.updformat:  q).html.updformat[\"upd\";`tablename`tabledata!(`trade;a)]                                                                                                                 \nname| \"upd\"\ndata| `tablename`tabledata!(`trade;+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a))\nq)first(.html.updformat[\"upd\";`tablename`tabledata!(`trade;a)])[`data;`tabledata]                                                                                        \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a  To utilise the pub/sub functionality, the web page must connect to the\nkdb+ process and subscribe for updates. Subscriptions are done using  .html.wssub[`tablename]  Publications from the kdb+ side are done with  .html.pub[`tablename;tabledata]  On the JavaScript side the incoming messages (data events) must be bound\nto page updates. For example, there might be an initialisation event\ncalled \u201cstart\u201d which allows the web page to retrieve all the initial\ndata from the process. The code below redraws the areas of the page with\nthe received data.  /* Bind data - Data type \"start\" will execute the callback function */\nKDBCONNECT.bind(\"data\",\"start\",function(data){\n  // Check that data is not empty\n  if(data.hbtable.length !== 0)\n   // Write HTML table to div element with id heartbeat-table\n   { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.hbtable));}\n  if(data.lmtable.length !== 0)\n   // Write HTML table to div element with id logmsg-table\n   { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.lmtable));}  \n  if(data.lmchart.length !== 0)\n   // Log message error chart\n   { MONITOR.barChart(data.lmchart,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n  });  Similarly the upd messages must be bound to page updates. In this case,\nthe structure is slightly different:  KDBCONNECT.bind(\"data\",\"upd\",function(data){\n  if(data.tabledata.length===0) return;\n  if(data.tablename === \"heartbeat\")\n    { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"logmsg\")\n    { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"lmchart\")\n    { MONITOR.barChart(data.tabledata,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n });  To display the WebSocket connection status the event \u201cws_event\u201d must be\nbound and it will output one of these default messages: \u201cConnecting...\u201d,\n\u201cConnected\u201d and \u201cDisconnected\u201d depending on the connection state of the\nWebSocket. Alternatively the value of the readyState attribute will\ndetermine the WebSocket status.  // Select html element using jQuery\nvar $statusMsg = $(\"#status-msg\");  \nKDBCONNECT.bind(\"ws_event\",function(data){\n  // Data is the default message string\n  $statusMsg.html(data);\n});\nKDBCONNECT.core.websocket.readyState // Returns 1 if connected.  Errors can be displayed by binding the event called \u201cerror\u201d.  KDBCONNECT.bind(\"error\",function(data){\n  $statusMsg.html(\"Error - \" + data);\n});", 
            "title": "Outline"
        }, 
        {
            "location": "/visualisation/#example", 
            "text": "A basic example is provided with the Monitor process. To get this to\nwork, u.q from kdb+tick should be placed in the code/common directory to\nallow all processes to publish updates. It should be noted that this is\nnot intended as a production monitoring visualisation screen, moreso a\ndemonstration of functionality. See section\u00a0monitorgui for more\ndetails.", 
            "title": "Example"
        }, 
        {
            "location": "/visualisation/#further-work", 
            "text": "Further work planned includes:    allow subscriptions on a key basis- currently all subscribers\n    receive all updates;    add JavaScript controls to allow in-place updates based on key\n    pairs, and scrolling window updates e.g. add N new rows to\n    top/bottom of the specified table;    allow multiple websocket connections to be maintained at the same\n    time.", 
            "title": "Further Work"
        }
    ]
}