{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The TorQ framework created by AquaQ Analytics forms the basis of a production kdb+ system by implementing some core functionality and utilities on top of kdb+, allowing developers to concentrate on the application business logic. It incorporates as many best practices as possible, with particular focus on performance, process management, diagnostic information, maintainability and extensibility. Wherever possible, we have tried to avoid re-inventing the wheel and instead have used contributed code from code.kx.com (either directly or modified). This framework will be suitable for those looking to create a new kdb+ system from scratch or those looking to add additional functionality to their existing kdb+ systems. The easiest way to get a production capture started is to download and install one of the Starter Packs . We also have a Google Group for questions/discussions . For recent updates to TorQ please check out our blog . We've highlighted some key TorQ features on our blog posts page. For email support contact support@aquaq.co.uk","title":"Home"},{"location":"InstallGuide/","text":"Install Guide Usage instruction for a fresh TorQ Install In your Linux terminal run the following lines copying them in one by one: wget https://raw.githubusercontent.com/AquaQAnalytics/TorQ/master/installtorqapp.sh wget --content-disposition https://github.com/AquaQAnalytics/TorQ/archive/3.7.0.tar.gz wget --content-disposition https://github.com/AquaQAnalytics/TorQ-Finance-Starter-Pack/archive/v1.9.0.tar.gz Then to launch the script. bash installtorqapp.sh --torq TorQ-3.7.0.tar.gz --releasedir deploy --data datatemp --installfile TorQ-Finance-Starter-Pack-1.9.0.tar.gz --env Where data parameter and env parameter are optional parameters. Full usage of the parameters available in the table below. The folder structure after installation will look like this: Then to run the TorQ stack: ./deploy/bin/torq.sh start all Check if the stack is up ./deploy/bin/torq.sh summary Parameters used .tg {border-collapse:collapse;border-color:#ccc;border-spacing:0;} .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} .tg .tg-btxf{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:top} Command line parameter Explanation and Usage torq Is a mandatory parameter that is the full path or relative path to the TorQ installation. It can either be a TorQ Directory where the version is already unzipped, that can be used when multiple TorQ Applications are used on the server for example and all point to a single TorQ main code. This will create a softlink to the relevant TorQ code. Or it can be a .tar.gz file of the TorQ installation for a fresh install. Example usage in the script: --torq /home/user/TorQ/TorQ-3.7.0 Where TorQ-3.7.0 is unzipped directory from the latest release .tar.gz file. Or --torq /home/user/TorQ/TorQ-3.7.0.tar.gz Which is the .tar.gz file from GitHub using: wget --content-disposition https://github.com/AquaQAnalytics/TorQ/archive/3.7.0.tar.gz releasedir Is a mandatory parameter that is the full path or relative path to the deployment directory that will populate the TorQ and TorQApp. If the directory doesn't exist then script creates one. It can be anything, if following the previously released instructions the folder name would be deploy. The releasedir parameter can be used as follows: --releasedir /home/user/deploy installfile Is a mandatory parameter with the full path or relative path to the TorQApp installation file (ACCEPTS ONLY .tar.gz FILE). Can be used as follows: --installfile /home/user/TorQ-FSP/TorQ-Finance-Starter-Pack-master.tar.gz data An optional parameter. That is if you want to have your data directory as defined by TORQDATAHOME live in a different part of the system rather than the place where the code lives. Can be used as follows: --data /home/data/torq_data If the directory doesn't exist the script will make one. Also accepts a relative path if necessary. env Env is the environment-specific optional installation parameters. That is a separate .sh script that can be configured for different environments like DEV/UAT/PROD. In the script, there are SED replacements for necessary variables. If this parameter is left empty or isn't included nothing happens. If you want to include it you have to insert the parameters as follows (also accepts relative path): --env /home/user/env_spec_dev.sh --env /env_spec_dev.sh Below is a user guide on how to set up the .sh script to have necessary replacements by the env parameter. For env parameter the env_spec script should look like this: echo $1 find $1 -type f -name \"*.sh\" find $1 -type f -name \"*.sh\" -exec sed -i \"s/export KDBBASEPORT=.*/export KDBBASEPORT=7373/g\" {} \\; Create an sh script env_spec_dev.sh and then add the parameter to the install script start line. --env /home/user/env_spec_dev.sh This will change the KDBBASEPORT to a new value. Similar actions can be done with other variables, and required user basic knowledge of sed commands. The script will scan through the code in the TorQApp directory and the bin directory from the deploy folder. If DEV and UAT run on different data sources then using env variable the install script can replace them with the correct server address. This is essentially the environment-specific config file. Version control The installtion script currently works with: TorQ v3.7.0 or higher TorQ-FSP v1.9.0 and higher TorQ-Crypto v1.0.0 and higher TorQ-TAQ v1.0.0 and higher","title":"Installation"},{"location":"InstallGuide/#install-guide","text":"","title":"Install Guide"},{"location":"InstallGuide/#usage-instruction-for-a-fresh-torq-install","text":"In your Linux terminal run the following lines copying them in one by one: wget https://raw.githubusercontent.com/AquaQAnalytics/TorQ/master/installtorqapp.sh wget --content-disposition https://github.com/AquaQAnalytics/TorQ/archive/3.7.0.tar.gz wget --content-disposition https://github.com/AquaQAnalytics/TorQ-Finance-Starter-Pack/archive/v1.9.0.tar.gz Then to launch the script. bash installtorqapp.sh --torq TorQ-3.7.0.tar.gz --releasedir deploy --data datatemp --installfile TorQ-Finance-Starter-Pack-1.9.0.tar.gz --env Where data parameter and env parameter are optional parameters. Full usage of the parameters available in the table below. The folder structure after installation will look like this: Then to run the TorQ stack: ./deploy/bin/torq.sh start all Check if the stack is up ./deploy/bin/torq.sh summary","title":"Usage instruction for a fresh TorQ Install"},{"location":"InstallGuide/#parameters-used","text":".tg {border-collapse:collapse;border-color:#ccc;border-spacing:0;} .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} .tg .tg-btxf{background-color:#f9f9f9;border-color:inherit;text-align:left;vertical-align:top} Command line parameter Explanation and Usage torq Is a mandatory parameter that is the full path or relative path to the TorQ installation. It can either be a TorQ Directory where the version is already unzipped, that can be used when multiple TorQ Applications are used on the server for example and all point to a single TorQ main code. This will create a softlink to the relevant TorQ code. Or it can be a .tar.gz file of the TorQ installation for a fresh install. Example usage in the script: --torq /home/user/TorQ/TorQ-3.7.0 Where TorQ-3.7.0 is unzipped directory from the latest release .tar.gz file. Or --torq /home/user/TorQ/TorQ-3.7.0.tar.gz Which is the .tar.gz file from GitHub using: wget --content-disposition https://github.com/AquaQAnalytics/TorQ/archive/3.7.0.tar.gz releasedir Is a mandatory parameter that is the full path or relative path to the deployment directory that will populate the TorQ and TorQApp. If the directory doesn't exist then script creates one. It can be anything, if following the previously released instructions the folder name would be deploy. The releasedir parameter can be used as follows: --releasedir /home/user/deploy installfile Is a mandatory parameter with the full path or relative path to the TorQApp installation file (ACCEPTS ONLY .tar.gz FILE). Can be used as follows: --installfile /home/user/TorQ-FSP/TorQ-Finance-Starter-Pack-master.tar.gz data An optional parameter. That is if you want to have your data directory as defined by TORQDATAHOME live in a different part of the system rather than the place where the code lives. Can be used as follows: --data /home/data/torq_data If the directory doesn't exist the script will make one. Also accepts a relative path if necessary. env Env is the environment-specific optional installation parameters. That is a separate .sh script that can be configured for different environments like DEV/UAT/PROD. In the script, there are SED replacements for necessary variables. If this parameter is left empty or isn't included nothing happens. If you want to include it you have to insert the parameters as follows (also accepts relative path): --env /home/user/env_spec_dev.sh --env /env_spec_dev.sh Below is a user guide on how to set up the .sh script to have necessary replacements by the env parameter. For env parameter the env_spec script should look like this: echo $1 find $1 -type f -name \"*.sh\" find $1 -type f -name \"*.sh\" -exec sed -i \"s/export KDBBASEPORT=.*/export KDBBASEPORT=7373/g\" {} \\; Create an sh script env_spec_dev.sh and then add the parameter to the install script start line. --env /home/user/env_spec_dev.sh This will change the KDBBASEPORT to a new value. Similar actions can be done with other variables, and required user basic knowledge of sed commands. The script will scan through the code in the TorQApp directory and the bin directory from the deploy folder. If DEV and UAT run on different data sources then using env variable the install script can replace them with the correct server address. This is essentially the environment-specific config file.","title":"Parameters used"},{"location":"InstallGuide/#version-control","text":"The installtion script currently works with: TorQ v3.7.0 or higher TorQ-FSP v1.9.0 and higher TorQ-Crypto v1.0.0 and higher TorQ-TAQ v1.0.0 and higher","title":"Version control"},{"location":"Overview/","text":"Overview What is kdb+? kdb+ is the market leading timeseries database from Kx Systems. kdb+ is used predominently in the Financial Services sector to capture, process and analyse billions of records on a daily basis, with Kx counting almost all of the top tier investment banks as customers. kdb+ incorporates a programming language, q, which is known for its performance and expressive power. Given the unsurpassed data management and analytical capabilities of kdb+, the applicability of kdb+ technology extends beyond the financial domain into any sector where rapid pre-built or adhoc analysis of large datasets is required. Other sectors which have good use cases for kdb+ include utilities, pharmaceuticals, telecoms, manufacturing, retail and any sector utilising telemetry or sensor data. What is AquaQ TorQ? AquaQ TorQ is a framework which forms the basis of a production kdb+ system by implementing some core functionality and utilities on top of kdb+, allowing developers to concentrate on the application business logic. We have incorporated as many best practices as possible, with particular focus on performance, process management, diagnostic information, maintainability and extensibility. We have kept the code as readable as possible using descriptive comments, error messages, function names and variable names. Wherever possible, we have tried to avoid re-inventing the wheel and instead have used contributed code from code.kx.com (either directly or modified). All code sections taken from code.kx.com are referenced in this document. AquaQ TorQ can be extended or modified as required. We have chosen some default behaviour, but it can all be overridden. The features of AquaQ TorQ are: Process Management: Each process is given a type and name, and can optionally be given a parent type. By default these are used to determine the code base it loads, the configuration loaded, log file naming and how it reports itself to discovery services. Whenever possible we have tried to ensure that all default behaviour can be overridden at the process type level, and further at the process name level. Code Management: Processes can optionally load common or process type/name specific code bases. All code loading is error trapped. Configuration Management: Configuration scripts can be loaded as standard and with specific process type/name configuration overriding default values. Configuration scripts are loaded in a specific order; default, then parent process type specific (optional), process type specific, then process name specific. Values loaded last will override values loaded previously. Usage Logging: All process usage is logged to a single text log file and periodically rolled. Logging includes opening/closing of connections, synchronous and asynchronous queries and functions executed on the timer. Logged values include the request, the details of where it came from, the time it was received, the time it took, memory usage before and after, the size of the result set, the status and any error message. Incoming and Outgoing Connection Management: Incoming (client) and outgoing (server) connections are stored and their usage monitored through query counts and total data size counts. Connections are stored and retrieved and can be set to automatically be re-opened as required. The password used for outgoing connections can be overridden at default, parent process type (optional), process type and process name level. Access Controls: Basic access controls are provided, and could be extended. These apply restrictions on the IP addresses of remote connections, the users who can access the process, and the functions that each user can execute. A similar hierarchical approach is used for access control management as for configuration management. Timer Extensions: Mechanism to allow multiple functions to be added to the timer either on a repeating or one-off basis. Multiple re-scheduling algorithms supplied for repeating timers. Standard Out/Error Logging: Functions to print formatted messages to standard out and error. Hooks are provided to extend these as required, e.g. publication to centralised logging database. Standard out and error are redirected to appropriately named, timestamped and aliased log files, which are periodically rolled. Error Handling: Different failure options are supplied in case code fails to load; either exit upon failure, stop at the point of failure or trap and continue. Visualisation: Utilities to ease GUI development using websockets and HTML5. Documentation and Development Tools: Functionality to document the system is built into AquaQ TorQ, and can be accessed directly from every q session. Developers can extend the documentation as they add new functions. Functionality for searching for functions and variables by name and definition is provided, and for ordering variables by memory usage. The standard help.q from code.kx is also included. Utilities: We intend to build out and add utilities as we find them to be suitably useful and generic. So far we have: Caching: allows a result set cache to be declared and result sets to be stored and retrieved from the cache. Suitable for functions which may be run multiple times with the same parameters, with the underlying data not changing in a short time frame; Timezone Handling: derived from code.kx, allows conversion between timestamps in different timezones; Email: an library to send emails; Async Messaging: allows easy use of advanced async messaging methods such as deferred synchronous communication and async communication using postback functions; Heartbeating: each process can be set to publish heartbeats, and subscribe to and manage heartbeats from other processes in the environment; Data Loading: utility wrapper around .Q.fsn to read a data file from disk, manipulate it and write it out in chunks; Subscriptions: allow processes to dynamically detect and subscribe to datasources; Tickerplant Log File Recovery: recover as many messages as possible from corrupt log files; Database Writing: utility functions for writing to, sorting and parting on disk databases; Compression: allows compression of a database. This can be performed using a set of parameters for the entire database, but also gives the flexibilty of compressing user-specified tables and/or columns of those tables with different parameters if required, and also offers decompression. Data Access API: A streamlined method to retrieve data across a variety of vanilla and exotic processes. AquaQ TorQ will wrap easily around kdb+tick and therefore around any tickerplant, RDB, HDB or real time processing application. We currently have several customised processes of our own: Discovery Service: Every process has a type, name and set of available attributes, which are used by other processes to connect to it. The Discovery Service is a central point that can be used to find other available processes. Client processes can subscribe to updates from the discovery service as new processes become available- the discovery service will notify its subscribers, which can then use the supplied hook to implement required behavior e.g. connect to the newly available process; Gateway: A fully synchronous and asynchronous gateway is provided. The gateway will connect to a defined list of process types (can be homogenous or heterogeneous processes) and will route queries across them according to the priority of received requests. The routing algorithms can be easily modified e.g. give priority to user X, or only route queries to processes which exist in the same data centre or geographical region to avoid the WAN (this would entail using the process attributes). The gateway can either return the result to the client back down the same handle, or it can wrap it in a callback function to be invoked on the client; Real Time Database (RDB): A customized version of the kdb+tick RDB, to allow dynamic tickerplant subscriptions, reloading of multiple HDBs using authenticated connections, and customized end-of-day save downs. The RDB, WDB and tickerplant log replay share a common code base to ensure that a save-down modification to a table is applied across each of these processes. Write Database (WDB): The job of a WDB is to write data to disk rather than to serve client queries. WDBs usually write data out periodically throughout the day, and are useful when there is too much data to fit into memory and/or the end of day save operation needs to be speeded up. The concept is based on w.q Tickerplant Log Replay: A process for replaying tickerplant log files to create on-disk data sets. Extended features are provided for only replaying subsets of log files (by message number and/or table name), replaying in chunks, invoking bespoke final behaviour etc.; Reporter: The Reporter Process runs defined reports (q queries or parameterized functions) against specific database or gateways on a schedule. The results are retrieved and processed. Processing can be user defined, or can be a standard operation such as writing the data to disk, or emailing the results to a list of recipients. This can be useful for running system checks or generating management reports. Housekeeping: A process to undertake housekeeping tasks periodically, such as compressing and removing files that are no longer used. Housekeeping looks up a file of instructions and performs maintenance tasks on directories accordingly. Features allow selective file deletion and zipping according to file age, including a search string parameter and the ability to exclude items from the search. The process can be scheduled, or run immediately from the command line and can be extended as required to incorporate more tasks. File Alerter: A process to periodically scan a set of directories and execute a function based on the availability of a file. This is useful where files may arrive to the system during the day and must be acted upon (e.g. files are uploaded to a shared directory by users/clients). The functions to execute are defined by the user and the whole process is driven by a csv file detailing the file to search for, the function to execute and, optionally, a directory to move the file to after it has been processed. Monitor: A basic monitoring process which uses the Discovery Service to locate the other processes within the system, listens for heartbeats, and subscribes for log messages. This should be extended as required but provides a basic central point for system health checks; Kill: A process used to kill other processes, optionally using the Discovery Service to locate them. A Large Scale Data Processing Platform One of the key drivers behind TorQ development has been to ensure all the tools necessary to build a large scale data processing platform are available. kdb+tick provides the basic building blocks, and a standard set-up usually looks something like this: However, in reality it is usually more complicated. A larger scale architecture serving large numbers of client queries and receiving data from multiple sources may look like this: A common practice is to use a gateway (section gateway) to manage client queries across back-end processes. The gateway can load balance across processes and make failures transparent to the client. If the clients access the gateway with asynchronous calls, then the gateway can serve many requests at once and additionally implement client queuing algorithms. Other common production features include: A modified version of the RDB (section sec:rdb) which does different operations at end-of-day, reloads multiple HDB processes etc. A Write Database (section [sec:wdb]) which receives data from the tickerplant and periodically writes it to disk. WDBs are used when there is too much data in a day to fit into memory and/or to speed up the end-of-day rollover job Processes that load data from other sources either into the HDB directly or to the RDB potentially via the tickerplant (section [sec:dataloader]). The data may be dropped in specific locations which have to be monitored (section [sec:filealerter]) A Reporting Engine (section [sec:reporter]) to run periodic reports and do something with the result (e.g. generate an xls file from the database and email it to senior management). Reporting engines can also be used to run periodic checks of the system A Discovery Service (section [sec:discovery]) to allow processes to locate each other, and to allow processes to dynamically register availability and push notifications around the system. Basic Monitoring (section [sec:monitor]) of process availability Housekeeping (section [sec:housekeeping]) to ensure log files are tidied up, tickerplant log files are compressed/moved in a timely fashion etc. Do I Really Have to Read This Whole Document? Hopefully not. The core of AquaQ TorQ is a script called torq.q and we have tried to make it as descriptive as possible, so perhaps that will suffice. The first place to look will be in the config files, the main one being \\$KDBCONFIG/settings/default.q. This should contain a lot of information on what can be modified. There is also a cheatsheet contained within the documentation. In addition: We have added a load of usage information: aquaq$ q torq.q -usage KDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems General: This script should form the basis of a production kdb+ environment. It can be sourced from other files if required, or used as a launch script before loading other files/directories using either -load or -loaddir flags ... etc ... If sourcing from another script there are hooks to modify and extend the usage information as required. We have some pretty extensive logging: aquaq$ q torq.q -p 9999 -debug KDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems 2013.11.05D12:22:42.597500000|aquaq|torq.q_3139_9999|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0 2013.11.05D12:22:42.597545000|aquaq|torq.q_3139_9999|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0 2013.11.05D12:22:42.597810000|aquaq|torq.q_3139_9999|INF|init|attempting to read required process parameters proctype,procname from file /torqhome/config/process.csv 2013.11.05D12:22:42.598081000|aquaq|torq.q_3139_9999|INF|init|read in process parameters of proctype=hdb; procname=hdb1 2013.11.05D12:22:42.598950000|aquaq|hdb1|INF|fileload|config file /torqhome/config/default.q found ... etc ... We have added functionality to find functions or variables defined in the session, and also to search function definitions. q).api.f`max name | vartype namespace public descrip .. --------------------| ------------------------------------------------.. maxs | function .q 1 \"\" .. mmax | function .q 1 \"\" .. .clients.MAXIDLE | variable .clients 0 \"\" .. .access.MAXSIZE | variable .access 0 \"\" .. .cache.maxsize | variable .cache 1 \"The maximum size in .. .cache.maxindividual| variable .cache 1 \"The maximum size in .. max | primitive 1 \"\" .. q)first 0!.api.p`.api name | `.api.f vartype | `function namespace| `.api public | 1b descrip | \"Find a function/variable/table/view in the current process\" params | \"[string:search string]\" return | \"table of matching elements\" q).api.p`.api name | vartype namespace public descrip .. ------------| --------------------------------------------------------.. .api.f | function .api 1 \"Find a function/variable/tabl.. .api.p | function .api 1 \"Find a public function/variab.. .api.u | function .api 1 \"Find a non-standard q public .. .api.s | function .api 1 \"Search all function definitio.. .api.find | function .api 1 \"Generic method for finding fu.. .api.search | function .api 1 \"Generic method for searching .. .api.add | function .api 1 \"Add a function to the api des.. .api.fullapi| function .api 1 \"Return the full function api .. We have incorporated help.q. q)help` adverb | adverbs/operators attributes| data attributes cmdline | command line parameters data | data types define | assign, define, control and debug dotz | .z locale contents errors | error messages save | save/load tables syscmd | system commands temporal | temporal - date & time casts verbs | verbs/functions We have separated and commented all of our config: aquaq$ head config/default.q /- Default configuration - loaded by all processes /- Process initialisation \\d .proc loadcommoncode:1b /- whether to load the common code defined at /- ${KDBCODE}/common loadprocesscode:0b /- whether to load the process specific code defined at /- ${KDBCODE}/{process type} loadnamecode:0b /- whether to load the name specific code defined at /- ${KDBCODE}/{name of process} loadhandlers:1b /- whether to load the message handler code defined at /- ${KDBCODE}/handlers logroll:1b /- whether to roll the std out/err logs daily ... etc ... Operating System and kdb+ Version AquaQ TorQ has been built and tested on the linux and OSX operating systems though as far as we are aware there is nothing that would make this incompatible with Solaris or Windows. It has also been tested with kdb+ 3.1 and 2.8. Please report any incompatibilities with other kdb+ versions or operating systems. License This code is released under the MIT license.","title":"About"},{"location":"Overview/#overview","text":"","title":"Overview"},{"location":"Overview/#what-is-kdb","text":"kdb+ is the market leading timeseries database from Kx Systems. kdb+ is used predominently in the Financial Services sector to capture, process and analyse billions of records on a daily basis, with Kx counting almost all of the top tier investment banks as customers. kdb+ incorporates a programming language, q, which is known for its performance and expressive power. Given the unsurpassed data management and analytical capabilities of kdb+, the applicability of kdb+ technology extends beyond the financial domain into any sector where rapid pre-built or adhoc analysis of large datasets is required. Other sectors which have good use cases for kdb+ include utilities, pharmaceuticals, telecoms, manufacturing, retail and any sector utilising telemetry or sensor data.","title":"What is kdb+?"},{"location":"Overview/#what-is-aquaq-torq","text":"AquaQ TorQ is a framework which forms the basis of a production kdb+ system by implementing some core functionality and utilities on top of kdb+, allowing developers to concentrate on the application business logic. We have incorporated as many best practices as possible, with particular focus on performance, process management, diagnostic information, maintainability and extensibility. We have kept the code as readable as possible using descriptive comments, error messages, function names and variable names. Wherever possible, we have tried to avoid re-inventing the wheel and instead have used contributed code from code.kx.com (either directly or modified). All code sections taken from code.kx.com are referenced in this document. AquaQ TorQ can be extended or modified as required. We have chosen some default behaviour, but it can all be overridden. The features of AquaQ TorQ are: Process Management: Each process is given a type and name, and can optionally be given a parent type. By default these are used to determine the code base it loads, the configuration loaded, log file naming and how it reports itself to discovery services. Whenever possible we have tried to ensure that all default behaviour can be overridden at the process type level, and further at the process name level. Code Management: Processes can optionally load common or process type/name specific code bases. All code loading is error trapped. Configuration Management: Configuration scripts can be loaded as standard and with specific process type/name configuration overriding default values. Configuration scripts are loaded in a specific order; default, then parent process type specific (optional), process type specific, then process name specific. Values loaded last will override values loaded previously. Usage Logging: All process usage is logged to a single text log file and periodically rolled. Logging includes opening/closing of connections, synchronous and asynchronous queries and functions executed on the timer. Logged values include the request, the details of where it came from, the time it was received, the time it took, memory usage before and after, the size of the result set, the status and any error message. Incoming and Outgoing Connection Management: Incoming (client) and outgoing (server) connections are stored and their usage monitored through query counts and total data size counts. Connections are stored and retrieved and can be set to automatically be re-opened as required. The password used for outgoing connections can be overridden at default, parent process type (optional), process type and process name level. Access Controls: Basic access controls are provided, and could be extended. These apply restrictions on the IP addresses of remote connections, the users who can access the process, and the functions that each user can execute. A similar hierarchical approach is used for access control management as for configuration management. Timer Extensions: Mechanism to allow multiple functions to be added to the timer either on a repeating or one-off basis. Multiple re-scheduling algorithms supplied for repeating timers. Standard Out/Error Logging: Functions to print formatted messages to standard out and error. Hooks are provided to extend these as required, e.g. publication to centralised logging database. Standard out and error are redirected to appropriately named, timestamped and aliased log files, which are periodically rolled. Error Handling: Different failure options are supplied in case code fails to load; either exit upon failure, stop at the point of failure or trap and continue. Visualisation: Utilities to ease GUI development using websockets and HTML5. Documentation and Development Tools: Functionality to document the system is built into AquaQ TorQ, and can be accessed directly from every q session. Developers can extend the documentation as they add new functions. Functionality for searching for functions and variables by name and definition is provided, and for ordering variables by memory usage. The standard help.q from code.kx is also included. Utilities: We intend to build out and add utilities as we find them to be suitably useful and generic. So far we have: Caching: allows a result set cache to be declared and result sets to be stored and retrieved from the cache. Suitable for functions which may be run multiple times with the same parameters, with the underlying data not changing in a short time frame; Timezone Handling: derived from code.kx, allows conversion between timestamps in different timezones; Email: an library to send emails; Async Messaging: allows easy use of advanced async messaging methods such as deferred synchronous communication and async communication using postback functions; Heartbeating: each process can be set to publish heartbeats, and subscribe to and manage heartbeats from other processes in the environment; Data Loading: utility wrapper around .Q.fsn to read a data file from disk, manipulate it and write it out in chunks; Subscriptions: allow processes to dynamically detect and subscribe to datasources; Tickerplant Log File Recovery: recover as many messages as possible from corrupt log files; Database Writing: utility functions for writing to, sorting and parting on disk databases; Compression: allows compression of a database. This can be performed using a set of parameters for the entire database, but also gives the flexibilty of compressing user-specified tables and/or columns of those tables with different parameters if required, and also offers decompression. Data Access API: A streamlined method to retrieve data across a variety of vanilla and exotic processes. AquaQ TorQ will wrap easily around kdb+tick and therefore around any tickerplant, RDB, HDB or real time processing application. We currently have several customised processes of our own: Discovery Service: Every process has a type, name and set of available attributes, which are used by other processes to connect to it. The Discovery Service is a central point that can be used to find other available processes. Client processes can subscribe to updates from the discovery service as new processes become available- the discovery service will notify its subscribers, which can then use the supplied hook to implement required behavior e.g. connect to the newly available process; Gateway: A fully synchronous and asynchronous gateway is provided. The gateway will connect to a defined list of process types (can be homogenous or heterogeneous processes) and will route queries across them according to the priority of received requests. The routing algorithms can be easily modified e.g. give priority to user X, or only route queries to processes which exist in the same data centre or geographical region to avoid the WAN (this would entail using the process attributes). The gateway can either return the result to the client back down the same handle, or it can wrap it in a callback function to be invoked on the client; Real Time Database (RDB): A customized version of the kdb+tick RDB, to allow dynamic tickerplant subscriptions, reloading of multiple HDBs using authenticated connections, and customized end-of-day save downs. The RDB, WDB and tickerplant log replay share a common code base to ensure that a save-down modification to a table is applied across each of these processes. Write Database (WDB): The job of a WDB is to write data to disk rather than to serve client queries. WDBs usually write data out periodically throughout the day, and are useful when there is too much data to fit into memory and/or the end of day save operation needs to be speeded up. The concept is based on w.q Tickerplant Log Replay: A process for replaying tickerplant log files to create on-disk data sets. Extended features are provided for only replaying subsets of log files (by message number and/or table name), replaying in chunks, invoking bespoke final behaviour etc.; Reporter: The Reporter Process runs defined reports (q queries or parameterized functions) against specific database or gateways on a schedule. The results are retrieved and processed. Processing can be user defined, or can be a standard operation such as writing the data to disk, or emailing the results to a list of recipients. This can be useful for running system checks or generating management reports. Housekeeping: A process to undertake housekeeping tasks periodically, such as compressing and removing files that are no longer used. Housekeeping looks up a file of instructions and performs maintenance tasks on directories accordingly. Features allow selective file deletion and zipping according to file age, including a search string parameter and the ability to exclude items from the search. The process can be scheduled, or run immediately from the command line and can be extended as required to incorporate more tasks. File Alerter: A process to periodically scan a set of directories and execute a function based on the availability of a file. This is useful where files may arrive to the system during the day and must be acted upon (e.g. files are uploaded to a shared directory by users/clients). The functions to execute are defined by the user and the whole process is driven by a csv file detailing the file to search for, the function to execute and, optionally, a directory to move the file to after it has been processed. Monitor: A basic monitoring process which uses the Discovery Service to locate the other processes within the system, listens for heartbeats, and subscribes for log messages. This should be extended as required but provides a basic central point for system health checks; Kill: A process used to kill other processes, optionally using the Discovery Service to locate them.","title":"What is AquaQ TorQ?"},{"location":"Overview/#a-large-scale-data-processing-platform","text":"One of the key drivers behind TorQ development has been to ensure all the tools necessary to build a large scale data processing platform are available. kdb+tick provides the basic building blocks, and a standard set-up usually looks something like this: However, in reality it is usually more complicated. A larger scale architecture serving large numbers of client queries and receiving data from multiple sources may look like this: A common practice is to use a gateway (section gateway) to manage client queries across back-end processes. The gateway can load balance across processes and make failures transparent to the client. If the clients access the gateway with asynchronous calls, then the gateway can serve many requests at once and additionally implement client queuing algorithms. Other common production features include: A modified version of the RDB (section sec:rdb) which does different operations at end-of-day, reloads multiple HDB processes etc. A Write Database (section [sec:wdb]) which receives data from the tickerplant and periodically writes it to disk. WDBs are used when there is too much data in a day to fit into memory and/or to speed up the end-of-day rollover job Processes that load data from other sources either into the HDB directly or to the RDB potentially via the tickerplant (section [sec:dataloader]). The data may be dropped in specific locations which have to be monitored (section [sec:filealerter]) A Reporting Engine (section [sec:reporter]) to run periodic reports and do something with the result (e.g. generate an xls file from the database and email it to senior management). Reporting engines can also be used to run periodic checks of the system A Discovery Service (section [sec:discovery]) to allow processes to locate each other, and to allow processes to dynamically register availability and push notifications around the system. Basic Monitoring (section [sec:monitor]) of process availability Housekeeping (section [sec:housekeeping]) to ensure log files are tidied up, tickerplant log files are compressed/moved in a timely fashion etc.","title":"A Large Scale Data Processing Platform"},{"location":"Overview/#do-i-really-have-to-read-this-whole-document","text":"Hopefully not. The core of AquaQ TorQ is a script called torq.q and we have tried to make it as descriptive as possible, so perhaps that will suffice. The first place to look will be in the config files, the main one being \\$KDBCONFIG/settings/default.q. This should contain a lot of information on what can be modified. There is also a cheatsheet contained within the documentation. In addition: We have added a load of usage information: aquaq$ q torq.q -usage KDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems General: This script should form the basis of a production kdb+ environment. It can be sourced from other files if required, or used as a launch script before loading other files/directories using either -load or -loaddir flags ... etc ... If sourcing from another script there are hooks to modify and extend the usage information as required. We have some pretty extensive logging: aquaq$ q torq.q -p 9999 -debug KDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems 2013.11.05D12:22:42.597500000|aquaq|torq.q_3139_9999|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0 2013.11.05D12:22:42.597545000|aquaq|torq.q_3139_9999|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0 2013.11.05D12:22:42.597810000|aquaq|torq.q_3139_9999|INF|init|attempting to read required process parameters proctype,procname from file /torqhome/config/process.csv 2013.11.05D12:22:42.598081000|aquaq|torq.q_3139_9999|INF|init|read in process parameters of proctype=hdb; procname=hdb1 2013.11.05D12:22:42.598950000|aquaq|hdb1|INF|fileload|config file /torqhome/config/default.q found ... etc ... We have added functionality to find functions or variables defined in the session, and also to search function definitions. q).api.f`max name | vartype namespace public descrip .. --------------------| ------------------------------------------------.. maxs | function .q 1 \"\" .. mmax | function .q 1 \"\" .. .clients.MAXIDLE | variable .clients 0 \"\" .. .access.MAXSIZE | variable .access 0 \"\" .. .cache.maxsize | variable .cache 1 \"The maximum size in .. .cache.maxindividual| variable .cache 1 \"The maximum size in .. max | primitive 1 \"\" .. q)first 0!.api.p`.api name | `.api.f vartype | `function namespace| `.api public | 1b descrip | \"Find a function/variable/table/view in the current process\" params | \"[string:search string]\" return | \"table of matching elements\" q).api.p`.api name | vartype namespace public descrip .. ------------| --------------------------------------------------------.. .api.f | function .api 1 \"Find a function/variable/tabl.. .api.p | function .api 1 \"Find a public function/variab.. .api.u | function .api 1 \"Find a non-standard q public .. .api.s | function .api 1 \"Search all function definitio.. .api.find | function .api 1 \"Generic method for finding fu.. .api.search | function .api 1 \"Generic method for searching .. .api.add | function .api 1 \"Add a function to the api des.. .api.fullapi| function .api 1 \"Return the full function api .. We have incorporated help.q. q)help` adverb | adverbs/operators attributes| data attributes cmdline | command line parameters data | data types define | assign, define, control and debug dotz | .z locale contents errors | error messages save | save/load tables syscmd | system commands temporal | temporal - date & time casts verbs | verbs/functions We have separated and commented all of our config: aquaq$ head config/default.q /- Default configuration - loaded by all processes /- Process initialisation \\d .proc loadcommoncode:1b /- whether to load the common code defined at /- ${KDBCODE}/common loadprocesscode:0b /- whether to load the process specific code defined at /- ${KDBCODE}/{process type} loadnamecode:0b /- whether to load the name specific code defined at /- ${KDBCODE}/{name of process} loadhandlers:1b /- whether to load the message handler code defined at /- ${KDBCODE}/handlers logroll:1b /- whether to roll the std out/err logs daily ... etc ...","title":"Do I Really Have to Read This Whole Document?"},{"location":"Overview/#operating-system-and-kdb-version","text":"AquaQ TorQ has been built and tested on the linux and OSX operating systems though as far as we are aware there is nothing that would make this incompatible with Solaris or Windows. It has also been tested with kdb+ 3.1 and 2.8. Please report any incompatibilities with other kdb+ versions or operating systems.","title":"Operating System and kdb+ Version"},{"location":"Overview/#license","text":"This code is released under the MIT license.","title":"License"},{"location":"Processes/","text":"Processes A set of processes is included. These processes build upon AquaQ TorQ, providing specific functionality. All the process scripts are contained in $KDBCODE/processes. All processes should have an entry in $KDBCONFIG/process.csv. All processes can have any type and name, except for discovery services which must have a process type of \u201cdiscovery\u201d. An example process.csv is: aquaq$ cat config/process.csv host,port,proctype,procname aquaq,9998,rdb,rdb_europe_1 aquaq,9997,hdb,rdb_europe_1aquaq,9999,hdb,hdb1 aquaq,9996,discovery,discovery1 aquaq,9995,discovery,discovery2 aquaq,8000,gateway,gateway1 aquaq,5010,tickerplant,tickerplant1 aquaq,5011,rdb,rdb1 aquaq,5012,hdb,hdb1 aquaq,5013,hdb,hdb2 aquaq,9990,tickerlogreplay,tpreplay1 aquaq,20000,kill,killhdbs aquaq,20001,monitor,monitor1 aquaq,20002,housekeeping,hk1 Discovery Service Overview Processes use the discovery service to register their own availability, find other processes (by process type) and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections- it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. The discovery service uses the process.csv file to make connections to processes on start up. After start up it is up to each individual process to attempt connections and register with the discovery service. This is done automatically, depending on the configuration parameters. Multiple discovery services can be run in which case each process will try to register and retrieve process details from each discovery process it finds in its process.csv file. Discovery services do not replicate between themselves. A discovery process must have its process type listed as discovery. To run the discovery service, use a start line such as: aquaq $ q torq.q -load code/processes/discovery.q -p 9995 -proctype discovery -procname discovery1 Modify the configuration as required. Operation Processes register with the discovery service. Processes use the discovery service to locate other processes. When new services register, any processes which have registered an interest in that process type are notified. Available Processes The list of available processes can be found in the .servers.SERVERS table. q).servers.SERVERS procname proctype hpup w hits startp lastp endp attributes ------------------------------------------------------------------------------------- discovery1 discovery :aquaq:9995 0 2014.01.22D17:00:40.947470000 ()!() discovery2 discovery :aquaq:9996 0 2014.01.22D17:00:40.947517000 ()!() hdb2 hdb :aquaq:5013 0 2014.01.22D17:00:40.947602000 ()!() killtick kill :aquaq:20000 0 2014.01.22D17:00:40.947602000 ()!() tpreplay1 tickerlogreplay :aquaq:20002 0 2014.01.22D17:00:40.947602000 ()!() tickerplant1 tickerplant :aquaq:5010 6 0 2014.01.22D17:00:40.967699000 2014.01.22D17:00:40.967698000 ()!() monitor1 monitor :aquaq:20001 9 0 2014.01.22D17:00:40.971344000 2014.01.22D17:00:40.971344000 ()!() rdb1 rdb :aquaq:5011 7 0 2014.01.22D17:06:13.032883000 2014.01.22D17:06:13.032883000 `date`tables!(,2014.01.22;`fxquotes`heartbeat`logmsg`quotes`trades) hdb3 hdb :aquaq:5012 8 0 2014.01.22D17:06:18.647349000 2014.01.22D17:06:18.647349000 `date`tables!(2014.01.13 2014.01.14;`fxquotes`heartbeat`logmsg`quotes`trades) gateway1 gateway :aquaq:5020 10 0 2014.01.22D17:06:32.152836000 2014.01.22D17:06:32.152836000 ()!() Gateway A synchronous and asynchronous gateway is provided. The gateway can be used for load balancing and/or to join the results of queries across heterogeneous servers (e.g. an RDB and HDB). Ideally the gateway should only be used with asynchronous calls. Prior to KDB v3.6, synchronous calls caused the gateway to block which limits the gateway to serving one query at a time (although if querying across multiple backend servers the backend queries will be run in parallel). For v3.6+, deferred synchronous requests to the gateway are supported. This allows the gateway to process multiple synchronous requests at once, therefore removing the requirement for the gateway to allow only one type of request. When using asynchronous calls, the client can either block and wait for the result (deferred synchronous) or post a call back function which the gateway will call back to the client with. The backend servers to be queried against with asynchronous and synchronous queries are selected using process type. The gateway API can be seen by querying .api.p\u201c.gw.*\u201d within a gateway process. Asynchronous Behaviour Asynchronous queries allow much greater flexibility. They allow multiple queries to be serviced at once, prioritisation, and queries to be timed out. When an asynchronous query is received the following happens: the query is placed in a queue; the list of available servers is retrieved; the queue is prioritised, so those queries with higher priority are serviced first; queries are sent to back end servers as they become available. Once the backend server returns its result, it is given another query; when all the partial results from the query are returned the results are aggregated and returned to the client. They are either returned directly, or wrapped in a callback and posted back asynchronously to the client. The two main customisable features of the gateway are the selection of available servers (.gw.availableservers) and the queue prioritisation (.gw.getnextqueryid). With default configuration, the available servers are those servers which are not currently servicing a query from the gateway, and the queue priority is a simple FIFO queue. The available servers could be extended to handle process attributes, such as the available datasets or the location of the process, and the queue prioritisation could be modified to anything required e.g. based on the query itself, the username, host of the client etc. An asynchronous query can be timed out using a timeout defined by the client. The gateway will periodically check if any client queries have not completed in the alotted time, and return a timeout error to the client. If the query is already running on any backend servers then they cannot be timed out other than by using the standard -T flag. Synchronous Behaviour Prior to KDB v3.6, when using synchronous queries the gateway could only handle one query at a time and cannot timeout queries other than with the standard -T flag. The variable .gw.synccallsallowed is by default set to 0b prior to KDB v3.6. To send synchronous calls, edit the gateway.q file so that .gw.synccallsallowed is set to true. (The exception being with TorQ-FSP, in which case it is set to 1b by default.) For v3.6+, deferred synchronous calls are supported, allowing the gateway to process multiple requests at a time. All synchronous queries will be immediately dispatched to the back end processes. They will be dispatched using an asynchronous call, allowing them to run in parallel rather than serially. When the results are received they are aggregated and returned to the client. Process Discovery The gateway uses the discovery service to locate processes to query across. The discovery service will notify the gateway when new processes become available and the gateway will automatically connect and start using them. The gateway can also use the static information in process.csv, but this limits the gateway to a predefined list of processes rather than allowing new services to come online as demand requires. Error Handling All errors and results can now be formatted with the formatresult function. Each response to the client is passed through this function with inputs status (1b=result,0b=error), sync (1b=sync,0b=async) and result (result/error) to allow different errors/results to be handled appropriately. As default, when synchronous calls are used, q errors are returned to clients as they are encountered. When using asynchronous calls, appropriately prefixed strings are used. It is up to the client to check the type of the received result and if it is a string then whether it contains the error prefix. The error prefix can be changed, but the default is \u201cerror: \u201d. Alternatively, the formatresult function can be altered as necessary. Errors will be returned when: the client requests a query against a server type which the gateway does not currently have any active instances of (this error is returned immediately); the client requests a query with the wrong servertype types; the client requests a query with null servers; the query is timed out; a back end server returns an error; a back end server fails; the join function fails. If postback functions are used, the error string will be posted back within the postback function (i.e. it will be packed the same way as a valid result). Client Calls There are four main client calls. The .gw.sync* methods should only be invoked synchronously, and the .gw.async* methods should only be invoked asynchronously. Each of these are documented more extensively in the gateway api. Use .api.p\u201c.gw.*\u201d for more details. Function Description .gw.syncexec[query; servertypes] Execute the specified query synchronously against the required list of servers. If more than one server, the results will be razed. .gw.syncexecj[query; servertypes; joinfunction] Execute the specified query against the required list of servers. Use the specified join function to aggregate the results. .gw.asyncexec[query; servertypes] Execute the specified query against the required list of servers. If more than one server, the results will be razed. The client must block and wait for the results. .gw.asyncexecjpt[query; servertypes; joinfunction; postback; timeout] Execute the specified query against the required list of servers. Use the specified join function to aggregate the results. If the postback function is not set, the client must block and wait for the results. If it is set, the result will be wrapped in the specified postback function and returned asynchronously to the client. The query will be timed out if the timeout value is exceeded. Client Call Examples Here are some examples for using client calls via a handle to the gateway process. To reiterate, v3.6+ users can use synchronous calls, whilst asynchronous calls are only relevant for users on < v3.6. Calls to the RDB only For synchronous calls // To return the avg price per sym for the day so far q) h(`.gw.syncexec;\"select avp:avg price by sym from trade where time.date=.z.d\";`rdb) // hloc function in RDB process q) h(`.gw.syncexec;`hloc;`rdb) {[startdate;enddate;bucket] $[.z.d within (startdate;enddate); select high:max price, low:min price, open:first price,close:last price,totalsize:sum `long$size, vwap:size wavg price by sym, bucket xbar time from trade; ([sym:`symbol$();time:`timestamp$()] high:`float$();low:`float$();open:`float$();close:`float$();totalsize:`long$();vwap:`float$())]} // Using the hloc function - change query for appropriate date q) h(`.gw.syncexec;(`hloc;2020.01.08;2020.01.08;10);`rdb) // Returns following table sym time | high low open close totalsize vwap ----------------------------------| ---------------------------------------------- AAPL 2020.01.08D00:00:00.836801000| 103.62 103.62 103.62 103.62 88 103.62 AAPL 2020.01.08D00:00:01.804684000| 103.64 103.64 103.64 103.64 86 103.64 AAPL 2020.01.08D00:00:02.405682000| 103.86 103.86 103.86 103.86 90 103.86 AAPL 2020.01.08D00:00:03.005465000| 104.06 104.06 104.06 104.06 78 104.06 AAPL 2020.01.08D00:00:03.404383000| 103.9 103.9 103.9 103.9 49 103.9 .. For asynchronous calls // To return the sum size per sym for the day so far q) neg[h](`.gw.asyncexec;\"select sum size by sym from trade\";`rdb);h[] Calls to the HDB only For synchronous calls // For the high, low, open and close prices of the day before q) h(`.gw.syncexec;\"select h:max price, l:min price, o:first price, c:last price by sym from trade where date=.z.d-1\";`hdb) For asynchronous calls q) neg[h](`.gw.asyncexec;\"`$last .z.x\";`hdb);h[] Calls to the HDB and RDB For synchronous calls q) h(`.gw.syncexec;\"$[.proc.proctype=`hdb; select from trade where date within (.z.d-2;.z.d-1); select from trade]\";`rdb`hdb) For asynchronous calls q) neg[h](`.gw.asyncexec;\"$[.proc.proctype=`hdb; select from trade where date within (.z.d-2;.z.d-1); select from trade]\";`rdb`hdb);h[] Demonstrating Aggregation of data For the purposes of demonstration, assume that the following queries must be run across a single RDB and a single HDB process, and the gateway has one RDB and two HDB processes available to it. q).gw.servers handle| servertype inuse active querycount lastquery usage attributes ------| -------------------------------------------------------------------- 7 | rdb 0 1 17 2014.01.07D17:05:03.113927000 0D00:00:52.149069000 `datacentre`country!`essex`uk 8 | hdb 0 1 17 2014.01.07D17:05:03.113927000 0D00:01:26.143564000 `datacentre`country!`essex`uk 9 | hdb 0 1 2 2014.01.07D16:47:33.615538000 0D00:00:08.019862000 `datacentre`country!`essex`uk 12 | rdb 0 1 2 2014.01.07D16:47:33.615538000 0D00:00:04.018349000 `datacentre`country!`essex`uk Both the RDB and HDB processes have a function f and table t defined. f will run for 2 seconds longer on the HDB processes then it will the RDB. q)f {system\"sleep \",string x+$[`hdb=.proc.proctype;2;0]; t} //if process type is HDB, sleep for x+2 seconds and then return table t. If not, sleep for x seconds and return table t q)t:([]a:(5013;5014;5015;5016;5017)) q)t a ---- 5013 5014 5015 5016 5017 Run the gateway. The main parameter which should be set is the .servers.CONNECTIONS parameter, which dictates the process types the gateway queries across. Also, we need to explicitly allow sync calls. We can do this from the config or from the command line using the following line: q torq.q -load code/processes/gateway.q -p 8000 -.gw.synccallsallowed 1 -.servers.CONNECTIONS hdb rdb -proctype gateway -procname gateway1 Start a client and connect to the gateway. Start with a sync query. The HDB query should take 4 seconds and the RDB query should take 2 seconds. If the queries run in parallel, the total query time should be 4 seconds. q)h:hopen 8000 q)h(`.gw.syncexec;(`f;2);`hdb`rdb) a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 q)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb) 4009 If a query is done for a server type which is not registered, an error is returned: q)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb`other) `not all of the requested server types are available; missing other Custom join functions can be specified: q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by a from x} each x}) //[query;servertype;joinfunction(lambda)] a | x ----| - 5014| 2 5015| 2 5016| 2 5017| 1 5018| 1 5012| 1 5013| 1 Custom joins can fail with appropriate errors: q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by b from x} each x}) `failed to apply supplied join function to results: b Asynchronous queries must be sent in async and blocked: q)(neg h)(`.gw.asyncexec;(`f;2);`hdb`rdb); r:h(::) /- This white space is from pressing return /- the client is blocked and unresponsive q)q)q) q) q)r a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 q) We can send multiple async queries at once. Given the gateway has two RDBs and two HDBs avaialble to it, it should be possible to service two of these queries at the same time. q)h:hopen each 8000 8000 q)\\t (neg h)@\\:(`.gw.asyncexec;(`f;2);`hdb`rdb); (neg h)@\\:(::); r:h@\\:(::) 4012 q)r +(,`a)!,5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 +(,`a)!,5013 5014 5015 5016 5017 9999 10000 10001 10002 10003 Alternatively async queries can specify a postback so the client does not have to block and wait for the result. The postback function must take two parameters- the first is the function that was sent up, the second is the results. The postback can either be a lambda, or the name of a function eg. handleresults. q)h:hopen 8000 q)handleresults:{-1(string .z.z),\" got results\"; -3!x; show y} //postback with timestamp, got results and an output of the results q)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;{-1(string .z.z),\" got results\"; -3!x; show y};0Wn) //[.gw.asyncexecjpt[query;servertypes(list of symbols);joinfunction(lambda);postbackfunction(lambda or symbol);timeout(timespan)] q) q) /- These q prompts are from pressing enter q) /- The q client is not blocked, unlike the previous example q) q)2014.01.07T16:53:42.481 got results a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 /- Can also use a named function rather than a lambda q)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;`handleresults;0Wn) q) q) q)2014.01.07T16:55:12.235 got results a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 Asynchronous queries can also be timed out. This query will run for 22 seconds, but should be timed out after 5 seconds. There is a tolerance of +5 seconds on the timeout value, as that is how often the query list is checked. This can be reduced as required. q)(neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::) q)q)q)r \"error: query has exceeded specified timeout value\" q)\\t (neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::) 6550 Non kdb+ Clients All the examples in the previous section are from clients written in q. However it should be possible to do most of the above from non kdb+ clients. The officially supported APIs for Java, C# and C allow the asynchronous methods above. For example, we can modify the try block in the main function of the Java Grid Viewer : import java.awt.BorderLayout; import java.awt.Color; import java.io.IOException; import java.lang.reflect.Array; import java.util.logging.Level; import java.util.logging.Logger; import javax.swing.JFrame; import javax.swing.JScrollPane; import javax.swing.JTable; import javax.swing.table.AbstractTableModel; import kx.c; public class Main { public static class KxTableModel extends AbstractTableModel { private c.Flip flip; public void setFlip(c.Flip data) { this.flip = data; } public int getRowCount() { return Array.getLength(flip.y[0]); } public int getColumnCount() { return flip.y.length; } public Object getValueAt(int rowIndex, int columnIndex) { return c.at(flip.y[columnIndex], rowIndex); } public String getColumnName(int columnIndex) { return flip.x[columnIndex]; } }; public static void main(String[] args) { KxTableModel model = new KxTableModel(); c c = null; try { c = new c(\"localhost\", 8000,\"username:password\"); // Create the query to send String query=\".gw.asyncexec[(`f;2);`hdb`rdb]\"; // Send the query c.ks(query); // Block on the socket and wait for the result model.setFlip((c.Flip) c.k()); } catch (Exception ex) { Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex); } finally { if (c != null) {try{c.close();} catch (IOException ex) {} } } JTable table = new JTable(model); table.setGridColor(Color.BLACK); String title = \"kdb+ Example - \"+model.getRowCount()+\" Rows\"; JFrame frame = new JFrame(title); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.getContentPane().add(new JScrollPane(table), BorderLayout.CENTER); frame.setSize(300, 300); frame.setVisible(true); } } Some of the unofficially supported APIs may only allow synchronous calls to be made. Tickerplant The tickerplant is a modified version of the standard kdb+tick tickerplant. The modifications from the standard tick.q include: Applies timestamps as timestamp rather than timespan; Tracks per table record counts in .u.icounts dictionary for faster recovery of real time subscribers; Allows configuration of timezones for timestamping data and performing end of day rollover (see eodtime.q ); The tickerplant log file will be written to hdb/database. Segmented Tickerplant The idea behind the STP was to create a process which retained all the functionality of the Tickerplant while adding flexibility in terms of logging, publishing and subscriptions. The functionality of the STP is almost fully backwardly compatible with a few minor (and we believe seldom utilised) exceptions. We have introduced: ability to create more granular log files a new batch publication mode ability to easily introduce custom table modifications upon message receipt more flexible subscriptions error handling for easier debugging when developing data feeds performance improvements for several use cases faster restart All the TorQ based subscriber processes (e.g. RDB and WDB), and any subscribers that use the TorQ subscription library, can switch between the TP and STP. For the minor modifications that must be made to data consumers, please see the Subscriptions section. Logging Modes The default TP logging behaviour is to write all updates to disk in a single log file which is rolled on a daily basis. The log file may become large. A consumer of the data must replay all the data from the log file, even if only a subset is required. Additionally, when the TP restarts, it must count through the log file in full to ensure that it has the correct message number. To add more flexibility, the following logging modes have been added which are set with the .stplg.multilog variable. Additionally a table of meta data is stored to describe the log files, including the full schema of all the tables contained within it. The .stpm.metatable table is saved as a q object to the STPLOG folder along with any error logs generated by the error mode. ``` singular: This mode is essentially the default TP behaviour, where all ticks across all tables for a given day are stored in a single file, eg. database20201026154808 . This is the simplest form of logging as everything is in one place. Note that in this mode, and all other logging modes, a new log file will be created on STP restart, and the stpmeta table updated accordingly. Note also that in all modes the timestamp on the end of the log file (YYYYMMDDHHMMSS) will come from the system clock when the log file is created, and will not be rounded to any particular value. stplogs \u251c\u2500\u2500stp1_2020.11.05/ \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 stpmeta \u2502 \u2514\u2500\u2500 stp1_20201105000000 \u2514\u2500\u2500stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 stpmeta \u2514\u2500\u2500 stp1_20201106000000 periodic: In this mode all the updates are stored in a the same file but the logs are rolled according to a custom period, set with .stplg.multilogperiod . For example, if the period is set to an hour a new log file will be created every hour and stored in a daily partitioned directory. As a result, this mode potentially allows for easier management by breaking the log files into smaller sizes, all while maintaining message arrival order during replays. Note that if the memory batch publish mode is used then the order of updates across different tables is no longer guaranteed (the order for a single table is guaranteed). stplogs \u251c\u2500\u2500stp1_2020.11.05/ \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 periodic20201105000000 \u2502 \u251c\u2500\u2500 periodic20201105010000 \u2502 \u251c\u2500\u2500 periodic20201105020000 \u2502 \u2514\u2500\u2500 stpmeta \u2514\u2500\u2500stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 periodic20201106000000 \u251c\u2500\u2500 periodic20201106010000 \u251c\u2500\u2500 periodic20201106020000 \u2514\u2500\u2500 stpmeta tabular: This mode is similar to the default behaviour except that each table has its own log file which is rolled daily in the form tradeYYYYMMDDHHMMSS . This allows for more granular log file management, and prioritisation of data recovery. stplogs/ \u251c\u2500\u2500 stp1_2020.11.05 \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 logmsg_20201105000000 \u2502 \u251c\u2500\u2500 packets_20201105000000 \u2502 \u251c\u2500\u2500 quote_20201105000000 \u2502 \u251c\u2500\u2500 quote_iex_20201105000000 \u2502 \u251c\u2500\u2500 stpmeta \u2502 \u251c\u2500\u2500 trade_20201105000000 \u2502 \u2514\u2500\u2500 trade_iex_20201105000000 \u2514\u2500\u2500 stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 logmsg_20201106000000 \u251c\u2500\u2500 packets_20201106000000 \u251c\u2500\u2500 quote_20201106000000 \u251c\u2500\u2500 quote_iex_20201106000000 \u251c\u2500\u2500 stpmeta \u251c\u2500\u2500 trade_20201106000000 \u2514\u2500\u2500 trade_iex_20201106000000 tabperiod: As the name suggests this mode combines the behaviour of the tabular and periodic logging modes, whereby each table has its own log file, each of which are rolled periodically as defined in the process. This adds the flexibility of both those modes when it comes to replays. stplogs/ \u251c\u2500\u2500 stp1_2020.11.05 \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 err20201105010000 \u2502 \u251c\u2500\u2500 logmsg_20201105000000 \u2502 \u251c\u2500\u2500 logmsg_20201105010000 \u2502 \u251c\u2500\u2500 packets_20201105000000 \u2502 \u251c\u2500\u2500 packets_20201105010000 \u2502 \u251c\u2500\u2500 quote_20201105000000 \u2502 \u251c\u2500\u2500 quote_20201105010000 \u2502 \u251c\u2500\u2500 quote_iex_20201105000000 \u2502 \u251c\u2500\u2500 quote_iex_20201105010000 \u2502 \u251c\u2500\u2500 stpmeta \u2502 \u251c\u2500\u2500 trade_20201105000000 \u2502 \u251c\u2500\u2500 trade_20201105010000 \u2502 \u251c\u2500\u2500 trade_iex_20201105000000 \u2502 \u2514\u2500\u2500 trade_iex_20201105010000 \u2514\u2500\u2500 stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 err20201106010000 \u251c\u2500\u2500 logmsg_20201106000000 \u251c\u2500\u2500 logmsg_20201106010000 \u251c\u2500\u2500 packets_20201106000000 \u251c\u2500\u2500 packets_20201106010000 \u251c\u2500\u2500 quote_20201106000000 \u251c\u2500\u2500 quote_20201106010000 \u251c\u2500\u2500 quote_iex_20201106000000 \u251c\u2500\u2500 quote_iex_20201106010000 \u251c\u2500\u2500 stpmeta \u251c\u2500\u2500 trade_20201106000000 \u251c\u2500\u2500 trade_20201106010000 \u251c\u2500\u2500 trade_iex_20201106000000 \u2514\u2500\u2500 trade_iex_20201106010000 custom This mode allows the user to have more granular control over how each table is logged. The variable .stplg.customcsv points to a CSV file containing two columns, table and mode, and this allows the user to decide which logging mode to use for each table. An example CSV is below: table,mode trade,periodic trade_iex,periodic quote,tabular quote_iex,tabluar heartbeat,tabperiod Here we have the trade and trade_iex tables both being saved to the same periodic log file, the quote and quote_iex tables both having their own daily log file and the heartbeat table having a periodic log file all to itself. This mode may be advantageous in the case where some tables receive far more updates than others, so they can have more rigorously partitioned logs, and the sparser tables can be pooled together. There is some complexity associated with this mode, as there can be different log files rolling at different times. As part of these new changes it is important to note that if the STP process is restarted, the stp will open new log files in the existing log directory and will not immediately replay the previous log files. Note that each of the new logging modes includes a q object saved in the directory called stpmeta, this is a table that contains information on the stp logs present in the directory. The table stpmeta contains multiple columns including: seq, the order of the logfiles, for periodic data all files for the same time period have the same seq number logname, pathway to the logfile start, the time and date that the logfile was created end, the time and date that the logfile was closed tbls, the tables present in each logfile msgcount, the count of the messages in each logfile schema, schemas for each of the tables in the tbls column additional, any additional information about the logfile Note that both end and msgcount are informative and not guaranteed, and the STP doesn't have any dependency on them. If the STP is killed or dies in an unclean manner, then they will not be populated. Batching Modes There are named modes which are set with the .stplg.batchmode variable and these allow the user to be flexible with process latency and throughput by altering the .u.upd and .z.ts functions: defaultbatch: This is effectively the standard TP batching mode where, upon receiving a tick, the STP immediately logs it to disk and batches the update which is published to subscribers whenever the timer function is next called. This mode represents a good balance of latency and overall throughput. immediate: In this mode no batching occurs, and the update is logged and published immediately upon entering the STP. This is less efficient in terms of overall throughput but allows for lower latency. memorybatch: In this mode, neither logging nor publishing happens immediately but everything is held in memory until the timer function is called, at which point the update is logged and published. High overall message throughput is possible with this mode, but there is a risk that some messages aren't logged in the case of STP failure. Also note that the the ordering of messages from different tables in the log file will not align with arrival order. Starting a Segmented Tickerplant process Starting an STP process is similar to starting a tickerplant, we need to have an updated process.csv that contains a line for the STP process like the one below. Optional flags such as -.stplg.batchmode and -.stplg.errmode can be added to change settings for the process. localhost,${KDBBASEPORT}+103,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -.stplg.batchmode immediate -.stplg.errmode 0 -t 1,q The process can either be started using: bash torq.sh start stp1 or: q ${TORQHOME}/torq.q -proctype segmentedtickerplant -procname stp1 -load ${KDBCODE}/processes/segmentedtickerplant.q Subscriptions It is easy for a subscriber to subscribe to a STP process. It follows the same process as subscribing to a TP through .u.sub however some changes have been made. Each subscriber connecting to the STP needs to be updated to search for the STP instead of the original tickerplant. This is done using .servers.CONNECTIONS in the settings config file for that process, for example: .servers.CONNECTIONS:enlist `segmentedtickerplant The STP requires these functions to be defined in subscriber processes (the definitions will be unique to the requirements of each subscriber): - upd[t;x] Called for updates from the STP. Arguments are t (the table the data is for), x (the data to be inserted) - endofperiod[currentpd;nextpd;data] Called at the end of a period for periodic STP modes. Takes 3 arguments currentpd (the current period), nextpd (the next period) and data (a dictionary containing some basic information on the stp process and the current time on the stp) - endofday[date;data] Called at the end of day for all modes. Takes 2 arguments date (current date) and data (a dictionary containing some basic information on the stp process and the current time on the stp) The data dictionary contains the STP name and type, list of subscribable tables in STP and the time at which the message is sent from the STP. In order to add further information to data, simply add additional elements in the endofdaydata function defined in code/segmentedtickerplant/stplg.q script. For more information on subscriptions, see the documentation on the pubsub.q utility script. Error Trapping If the .stplg.errmode Boolean variable is set to true, an error log is opened on start up and the .u.upd function is wrapped in an error trap. If an error is thrown by the STP when it receives and update from the feed, then the update is written to the error log. This should allow easier debugging during onboarding of new data feeds. Unless the feed is very unstable, this should not be necessary in production usage as it incurs a small overhead on each update. This mode is really designed for development/testing purposes, it shouldn't be necessary for a stable production feed and will add a small overhead to each update. Time Zone Behaviour A key tickerplant function is to timestamp the incoming data before it gets published to the rest of the system. Similar to the existing TP, the STP allows definition of the timezone that data is timestamped at upon arrival the timezone that the STP executes it's end-of-day roll in the offset from midnight that the STP executes its end-of-day roll in This allows for more complex configurations, such as an Foreign Exchange data capture system which timestamps data in UTC but rolls at 5 PM EST. The key variable used is .eodtime.dailyadj but more information on setting up a TorQ process in a different time zone can be found here . Per Table Customisation Each table has its own upd function, meaning that some additional processing, such as adding a sequence number or a time-zone offset, can be done in the STP itself rather than needing to be done in a separate process. This is done by altering the .stplg.updtab dictionary in the segmentedtickerplant settings config file. The default behaviour is for every update to automatically have the current timestamp applied to it. // In file $TORQHOME/appconfig/settings/segmentedtickerplant.q // Apply a sequence number to 'tabname' .stplg.updtab[`tabname]:{((count first x)#'(y;.stplg.seqnum),x} // In the STP process q) .stplg.updtab quote | {(enlist(count first x)#y),x} trade | {(enlist(count first x)#y),x} tabname | {((count first x)#'(y;.stplg.seqnum),x} ... Custom Batching Modes The batching behaviour depends on two functions: .u.upd and .z.ts . The former is called every time an update arrives in the STP and the latter whenever the timer function is called (e.g. if the process is started with -t 1000 , this will be called every second). In the default batching mode, .u.upd inserts the update into a local table and writes it to the log file, and .z.ts publishes the contents of the local table before clearing it. To customise these functions, the .stplg.upd and .stplg.zts dictionaries will need to be customised. For example, the default batching code looks like the following: \\d .stplg // Standard batch mode - write to disk immediately, publish in batches upd[`defaultbatch]:{[t;x;now] t insert x:.stplg.updtab[t] . (x;now); `..loghandles[t] enlist(`upd;t;x); // track tmp counts, and add these after publish @[`.stplg.tmpmsgcount;t;+;1]; @[`.stplg.tmprowcount;t;+;count first x]; }; zts[`defaultbatch]:{ // publish and clear all tables, increment counts .stpps.pubclear[.stpps.t]; // after data has been published, updated the counts .stplg.msgcount+:.stplg.tmpmsgcount; .stplg.rowcount+:.stplg.tmprowcount; // reset temp counts .stplg.tmpmsgcount:.stplg.tmprowcount:()!(); }; \\d . Multiple Updates From Publishers The STP handles single messages which contain multiple updates. The standard interface for publishers is the .u.upd interface supported by the original tickerplant (two arguments, the table to be updated followed by the data). The arguments for this can now but lists, i.e. a list of table names followed by a list of associated data. These updates can all be stamped with the same sequence number, and is useful where upstream publishers receive a single update which is split across downstream tables. Once this is done, simply update .stplg.batchmode with the name of the new mode and start the process. Performance Comparison A custom performance stack was set up comprising a feed, a consumer, an STP, a vanilla TP (normal TorQ tickerplant) and a kdb+ tick process along with an observer process which was responsible for coordinating the tests and processing the results. When the tests begin, the feed pushes single row updates to the selected TP process in a loop for one minute before pushing updates in batches of 100 rows for one minute. The observer then collects the results from the consumer which is subscribed to the TP and clears the table before resetting things so that the feed is pointing at either the same process in a different batching mode or a new process. In this way all the process modes are tested, including the immediate and batched modes for the TP and tick processes. These tests were run on a shared host with dual Intel Xeon Gold 6128 CPUs with a total of 12 cores and 24 threads with 128GB of memory. Each tickerplant will only have been using a single core. The results below show the average number of messages per second (mps) received by the subscriber. The results for single updates can be seen below. It should be noted that the message rates achieved will be dependent on hardware configuration. The purpose of the testing below is to demonstrate the relative performance between the different implementations and batching modes. Process Batch Mode Average mps STP Default batch 103k STP Immediate 89k STP Memory batch 174k TorQ TP Immediate 75k TorQ TP Batch 98k Tick Immediate 87k Tick Batch 103k And the following are for batched updates (note that each message contains 100 ticks): Process Batching Mode Average mps STP Default batch 19k STP Immediate 18k STP Memory batch 21k TorQ TP Immediate 18k TorQ TP Batch 18k Tick Immediate 17k Tick Batch 19k The first obvious thing to be noticed is that batching the updates results in greater performance as there are fewer IPC operations and disk writes, and while some insight can be gleaned from these figures the single update results provide a better comparison of the actual process code performance. The memory batching mode is the clear leader in terms of raw performance as it does not write to disk on every update. The three 'default' batching modes are roughly equivalent in terms of performance and all have similar functionality. The three Immediate modes bring up the rear in terms of raw throughput, though the STP version is the performance leader here as it stores table column names in a dictionary which can be easily accessed rather than having to read the columns of a table in the root namespace. In this set up each message contained either one or 100 ticks, each of which had 11 fields, and there was one feed pushing to one TP which had one simple subscriber and varying these figures will impact performance. Here the size of a single tick is 141 bytes and the 100-tick update is 6137 bytes. Increasing the fields in each tick or the number of ticks in a message will results in more costly IPC and IO operations which will decrease performance. Having more subscribers and increasing the complexity of subscriptions, ie. having complex where clause conditions on the subscription, will also reduce performance. These are all things worth bearing in mind as one builds an application. When writing the code for these STP modes some compromise was taken between performance and maintenance. All the UPD functions are written in a standard way and have certain common elements abstracted away in namespaces, which does technically reduce performance. We will see later on how custom modes can be defined which can be more tailored to a given application. Chained STP A chained tickerplant (CTP) is a TP that is subscribed to another TP. This is useful for the following: protecting the main TP by offloading publication load. Examples would be distributing data to multiple consumers, or executing complex subscription filtering logic. distributing a system across multiple hosts where data is pushed once across a host boundary and then further distributed within that host. The CTP can also be used to create a local version of the TP log files within that host. The CTP inherits functionality from the STP- essentially, the code can be run in \"chained\" mode. In this case it subscribes to an STP, and forwards data. Timings (e.g. end-of-day, end-of-period) are driven from the STP. A chained STP can be in a different batching mode from the STP. The main choice is around what type of logging, if any, the CTP is required to do: none: Chained STP does not create or access any log files. create: Chained STP creates its own log files independent of the STP logs. Subscribers then access the chained STP log files during replays parent: STP logs are passed to subscribers during replays. Chained STP does not create any logs itself The 'parent' logging mode is useful when all of the Torq processes have access to the same disk. In this case, the subscriber can access the logs of the STP and the data is replayed through the Chained STP. This prevents the SCTP from needing to create duplicate logs and so saves on storage. This replay would look like the following: The 'create' logging mode should be used when the chained STP is running on a separate host to the STP, as illustrated in the diagram below. Here RDB1 may access the STP logs as they are running on the same host. However, RDB2 does not have access to the STP logs and so they cannot be replayed through the SCTP. Therefore, the chained STP needs to create its own logs for RDB2 to access. Backward Compatibility Not everything about the STP is exactly the same as the TP, a couple of things have been changed: All updates must be lists of lists, meaning that single updates must be enlisted. Consumers must have endofday[currentdate;DATA], endofperiod[currentperiod;nextperiod;DATA] and upd[newdata;table] defined in order to successfully subscribe to the STP (.u.end is no longer used). Here DATA is a dictionary of metadata about the STP containing the STP name and type, subscribable tables and the time at which the message was sent from the STP. First two columns in tables do not have to be time and sym By default, every update entering the STP will be timestamped as the first column. However, this behaviour can easily be modified by altering the dictionary of updtab functions. To read more on this, visit the Per Table Customisation subsection of the STP documentation in this file. Real Time Database (RDB) The Real Time Database is a modified version of r.q found in kdb+tick. The modifications from the standard r.q include: Tickerplant (data source) and HDB location derived from processes defined by the discovery service or from config file; Automatic re-connection and resubscription to tickerplant; List of tables to subscribe to supplied as configuration setting; More pre-built flexibility in end-of-day; More verbose end-of-day logging; Reload multiple authenticated HDBs after end-of-day; End-of-day save down manipulation code is shared between RDB, WDB and tickerplant log replay See the top of the file for more information. Write Database (WDB) The Write Database or WDB is based on w.q. This process features a number of modifications and enhancements over w.q: Provides the option to write down to a custom partition scheme, defined by parted columns in sort.csv, which removes the need for end of day sorting; Greater configuration options; max rows on a per table basis, list subscription tables, upd function etc. See the top of the process file for the options; Use of common code with the RDB and Tickerplant Log Replay process to manipulate tables before save, sort and apply attributes; Checks whether to persist data to disk on a timer rather than on each tick; Informs other RDB, HDB and GW processes that end of day save and sort has completed; More log information supplied; End of day timezone can be configured (see eodtime.q ). The WDB process can broken down into two main functions: Periodically saving data to disk and Sorting data at end of day The WDB process provides flexibility so it can be set-up as a stand-alone process that will both save and sort data or two separate processes (one that saves the data and another that will sort the data on disk). This allows greater flexibility around the end of day event as sorting data can be time consuming. It is also helps when implementing seemless rollovers (i.e. no outage window at end-of-day). The behaviour of the WDB process is controlled by the .wdb.mode parameter. This should be set to one of following three values: saveandsort - the process will subscribe for data, periodically write data to disk and at EOD it will flush remaining data to disk before sorting it and informing GWs, RDBs and HDBs etc. save - the process will subscribe for data, periodically write data to disk and at EOD it will flush remaining data to disk. It will then inform its respective sort mode process to sort the data sort - the process will wait to get a trigger from its respective save mode process. When this is triggered it will sort the data on disk, apply attributes and the trigger a reload on the RDB, HDB and GW processes When running a system with separate save and sort process, the sort process should be configured in the processes.csv file with a proctype of sort. The save process will check for processes with a proctype of sort when it attempts to trigger the end of day sort of the data. The wdb process provides two methods for persisting data to disk and sorting at the end of the day. default - Data is persisted into a partition defined by the [partitiontype] variable, similar to the hdb partition scheme. The general scheme is of the form [wdbdir]/[partitiontype]/[table]/. And a typical partition directory would be similar to wdb/database/2015.11.26/trades/. At the end of the day, before being moved to the hdb, the data is sorted according to parameters defined in sort.csv. For each table, sort.csv will specify the columns to sort (using xasc) and apply attributes to. partbyattr - Data is persisted to a custom partition scheme, derived from parameters in the sort.csv file. The write down scheme is taken from sort.csv, to reflect the effect of using xasc at the end of day. For each table, the columns defined in sort.csv, with the parted attribute, are used to create custom partitions in the wdb. Multiple columns can be defined with the parted attribute and distinct combinations of each are generated for custom partitions. The general partition scheme is of the form [wdbdir]/[partitiontype]/[table]/[parted column(s)]/. And a typical partition directory would be similar to wdb/database/2015.11.26/trade/MSFT_N. In the above example, the data is parted by sym and source, and so a unique partition directory MSFT_N is created in the wdb directory. At the end of the day, data is upserted into the hdb without the need for sorting. The number of rows that are joined at once is limited by the mergenumrows and mergenumtab parameters. The optional partbyattr method may provide a significant saving in time at the end of day, allowing the hdb to be accessed sooner. For large data sets with a low cardinality (ie. small number of distinct elements) the optional method may provide a significant time saving, upwards of 50%. The optional method should also reduce the memory usage at the end of day event, as joining data is generally less memory intensive than sorting. Tickerplant Log Replay The Tickerplant Log Replay script is for replaying tickerplant logs. This is useful for: handling end of day save down failures; handling large volumes of data (larger than can fit into RAM). The process takes as the main input either an individual log file to replay, or a directory containing a set of log files. Amongst other functionality, the process can: replay specific message ranges; replay in manageable message chunks; recover as many messages as possible from a log file rather than just stopping at the first bad message; ignore specific tables; modify the tables before or after they are saved; apply sorting and parting after all the data is written out. The process must have some variables set (the tickerplant log file or directory, the schema file, and the on-disk database directory to write to) or it will fail on startup. These can either be set in the config file, or overridden from the command line in the usual way. An example start line would be: q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.tplogfile ../test/tplogs/marketdata2013.12.17 -.replay.schemafile ../test/marketdata.q -.replay.hdbdir ../test/hdb1 -proctype tickerlogreplay -procname tplogreplay1 In order to replay log files from a segmented tickerplant, the directory containing those log files can be passed in and the variable .replay.segmentedmode must be true. It should be noted that a directory must be passed in and the directory must contain the STP meta table for that day. Only one log directory can be replayed at a time. The tickerplant log replay script has extended usage information which can be accessed with -.replay.usage. q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.usage -proctype tickerlogreplay -procname tplogreplay1 Housekeeping The housekeeping process is used to undertake periodic system housekeeping and maintenance, such as compressing or removing files which are no longer required. The process will run the housekeeping jobs periodically on a timer. Amongst other functionality the process: Allows for removing and zipping of directory files; Provides an inbuilt search utility and selectively searches using a \u2018find\u2019 and \u2018exclude\u2019 string, and an \u2018older than\u2019 parameter; Reads all tasks from a single CSV; Runs on a user defined timer; Can be run immediately from command line or within the process; Can be easily extended to include new user defined housekeeping tasks. The process has two main parameters that should be set prior to use; runtimes and inputcsv.\u2018Runtimes\u2019 sets the timer to run housekeeping at the set time(s), and \u2018Inputcsv\u2019 provides the location of the housekeeping csv file. These can either be set in the config file, or overridden via the command line. If these are not set, then default parameters are used; 12.00 and \u2018KDBCONFIG/housekeeping.csv\u2019 respectively. The process is designed to run from a single csv file with seven headings: Function details the action that you wish to be carried out on the files or directories. Initially, this can be rm (remove) and zip (zipping) for files, and tardir (zipping) for directories; Path specifies the directory that the files/directories are in; Match provides the search string to the find function, files/directories returned will have names that match this string; Exclude provides a second string to the find function, and these files/directories are excluded from the match list; Age is the \u2018older than\u2019 parameter, and the function will only be carried out on files/directories older than the age given (in days); Agemin switches the age measurement from days to minutes; Checkfordirectory specifies whether to search for directories, instead of files. An example csv file would be: function,path,match,exclude,age,agemin,checkfordirectory zip,./logs/,*.log,*tick*,2,, rm,./logs/,*.log*,*tick*,4,, zip,./logs/,*tick*,,1,, rm,./logs/,*tick*,,3,, tardir,./stplogs/,database*,,1,,1 function path match exclude age agemin checkfordirectory ----------------------------------------------------------------------- zip \"./logs/\" \"*.log\" \"*tick*\" 2 0 0 rm \"./logs/\" \"*.log*\" \"*tick*\" 4 0 0 zip \"./logs/\" \"*tick*\" \"\" 1 0 0 rm \"./logs/\" \"*tick*\" \"\" 3 0 0 tardir \"./stplogs/\" \"database*\" \"\" 1 0 1 The process reads in the csv file, and passes it line by line to a \u2018find\u2019 function; providing a dictionary of values that can be used to locate the files/directories required. The find function takes advantage of system commands to search for matches according to the specifications in the dictionary. A search is performed for both the match string and the exclude string, and cross referenced to produce a list of files/directories that match the parameters given. The matches are then each passed to a further set of system commands to perform the task of either zipping or removing. Note that an incomplete csv or non-existant path will throw an error. The remove and zipping functions form only basic implementations of the housekeeping process; it is designed to be exended to include more actions than those provided. Any user function defined in the housekeeping code can be employed in the same fashion by providing the name of the function,search string and age of files to the csv. As well as being scheduled on a timer, the process can also be run immediately. Adding \u2018-hk.runnow 1\u2019 to the command line when starting the process will force immediate running of the actions in the housekeeping csv. Likewise, setting runnow to 1b in the config file will immediately run the cleaning process. Both methods will cause the process to exit upon completion. Calling hkrun[] from within the q process will also run the csv instructions immediately. This will not affect any timer scheduling and the process will remain open upon completion. Housekeeping works both on windows and unix based systems. Since the process utilizes inbuilt system commands to perform maintenances, a unix/windows switch detects the operating system of the host and applies either unix or widows functions appropriately. Extensions need only be made in the namespace of the hosting operating system (i.e. if you are using a unix system, and wish to add a new function, you do not need to add the function to the windows namespace to). Usage information can be accessed using the \u2018-hkusage\u2019 flag: q torq.q -load code/processes/housekeeping.q -p 9999 -proctype housekeeping -procname hk1 -debug -hkusage File Alerter The file alerter process is a long-running process which periodically scans a set of directories for user-specified files. If a matching file is found it will then carry out a user-defined function on it. The files to search for and the functions to run are read in from a csv file. Additionally, the file alerter process can: run more than one function on the specified file. optionally move the file to a new directory after running the function. store a table of files that have already been processed. run the function only on new files or run it every time the file is modified. ignore any matching files already on the system when the process starts and only run a function if a new file is added or a file is modified. The file alerter process has four parameters which should be set prior to use. These parameters can either be set in the config file or overridden on the command-line. If they are not set, the default parameters will be used. The parameters are as follows. inputcsv - The name and location of the csv file which defines the behaviour of the process. The default is KDBCONFIG/filealerter.csv. polltime - How often the process will scan for matching files. The default is 0D:00:01, i.e., every minute. alreadyprocessed - The name and location of the already-processed table. The default is KDBCONFIG/filealerterprocessed. This table will be created automatically the first time the process is ran. skipallonstart - If this is set to 1, it will ignore all files already on the system; if it is set to 0, it will not. The default value is 0. The files to find and the functions to run are read in from a csv file created by the user. This file has five columns, which are detailed below. path - This is the path to the directory that will be scanned for the file. match - This is a search string matching the name of the file to be found. Wildcards can be used in this search, for example, \u201cfile*\u201d will find all files starting with \u201cfil\u201d. function - This is the name of the function to be run on the file. This function must be defined in the script KDBCODE/processes/filealerter.q. If the function is not defined or fails to run, the process will throw an error and ignore that file from then on. newonly - This is a boolean value. If it is set to 1, it will only run the function on the file if it has been newly created. If it is set to 0, then it will run the function every time the file is modified. movetodirectory - This is the path of the directory you would like to move the file to after it has been processed. If this value is left blank, the file will not be moved. It is possible to run two separate functions on the same file by adding them as separate lines in the csv file. If the file is to be moved after it is processed, the file alerter will run both functions on the file and then attempt to move it. A typical csv file to configure the file alerter would look like: path,match,function,newonly,movetodirectory /path/to/dirA,fileA.*,copy,0,/path/to/newDir /path/to/dirB,fileB.txt,email,1, /path/to/dirA,fileA.*,delete,0,/path/to/newDir path match function newonly movetodirectory --------------------------------------------------- \"/path/to/dirA\" \"fileA.*\" copy 0 \"/path/to/newDir\" \"/path/to/dirB\" \"fileB.txt\" email 1 \"\" \"/path/to/dirA\" \"fileA.*\" delete 0 \"/path/to/newDir\" The file alerter process reads in each line of the csv file and searches files matching the search string specified in that line. Note that there may be more than one file found if a wildcard is used in the search string. If it finds any files, it will check that they are not in the already processed table. If newonly is set to 1, it only checks if the filename is already in the table. If newonly is set to 0, it checks against the filename, filesize and a md5 hash of the file. The md5 hash and the filesize are used to determine if the file has been modified since it was processed last. If the found files have not been processed already, it then attempts to run the specified function to these files. After the process has run through each line of the csv, it generates a table of all files that were processed on that run. These files are appended to the already processed table which is then saved to disk. The file alerter will attempt to move the files to the \u2018movetodirectory\u2019, if specified. If the file has already been moved during the process (for example, if the function to run on it was \u2018delete\u2019), the file alerter will not attempt to move it. The file alerter is designed to be extended by the user. Customised functions should be defined within the filealerter.q script. They should be diadic functions, i.e., they take two parameters: the path and the filename. As an example, a simple function to make a copy of a file in another directory could be: copy:{[path;file] system \"cp \", path,\"/\", file, \" /path/to/newDir\"} Although the process is designed to run at regular intervals throughout the day, it can be called manually by invoking the FArun[] command from within the q session. Similarly, if new lines are added to the csv file, then it can be re-loaded by calling the loadcsv[] command from the q session. Each stage of the process, along with any errors which may occur, are appropriately logged in the usual manner. The file alerter process is designed to work on both Windows and Unix based systems. Since many of the functions defined will use inbuilt system command they will be need to written to suit the operating system in use. It should also be noted that Windows does not have an inbuilt md5 hashing function so the file alerter will only detect different versions of files if the filename or filesize changes. Reporter Overview The reporter process is used to run periodic reports on specific processes. A report is the result of a query that is run on a process at a specific time. The result of the query is then handled by one of the inbuilt result handlers, with the ability to add custom result handlers. Features: Easily create a report for information that you want; Fully customizable scheduling such as start time, end time and days of the week; Run reports repeatedly with a custom period between them; Asynchronous querying with custom timeout intervals; Inbuilt result handlers allow reports to be written to file or published; Custom result handlers can be defined; Logs each step of the report process; Fully integrated with the TorQ gateway to allow reports to be run across backend processes. The reporter process has three parameters that are read in on initialisation from the reporter.q file found in the $KDBCONFIG/settings directory. These settings are the string filepath of the input csv file, a boolean to output log messages and timestamp for flushing the query log table. To run the reporter process: q torq.q -load code/processes/reporter.q -p 20004 -proctype reporter -procname reporter1 Once the reporter process has been initiated, the reports will be scheduled and no further input is required from the user. Report Configuration By default, the process takes its inputs from a file called reporter.csv which is found in the $KDBCONFIG directory. This allows the user complete control over the configuration of the reports. As the queries are evaluated on the target process, local variables can be referenced or foreign functions can be run. Table [table:reportertable] shows the meaning of the csv schema. Column Header Description and Example name Report name e.g. Usage query Query to be evaluated on that process. It can be a string query or function resulthandler Result handlers are run on the returned result. Custom result handlers can be added. The result handler must be a monadic function with the result data being passed in e.g. writetofile[\u201c./output\u201d;\u201cusage\u201d] gateway If non null the reporter will query processes route the query to the proctype specified in this field. The values in the proctype field will be the process types on which the gateway runs the backend query. e.g. `gateway joinfunction Used to join the results when a gateway query is being used. The choice of joinfunction must take into account the result that will be received. The function must be monadic and the parameter will be the list of results returned from the backend processes e.g. raze proctype The type of process that the report will be run on. If the gateway field is not empty this may be a list of process types, otherwise the reporter will throw an error on startup. e.g. `rdb procname The name of a specific process to run the report on. If left null, the reporter process will select a random process with the specified proctype. If the gateway field is not null, this field specifies the specific gateway process name to run the query against e.g. `hdb1 start Time on that day to start at e.g. 12:00 end Time on that day that the report will stop at e.g. 23:00 period The period between each report query e.g. 00:00:10 timeoutinterval The amount of time the reporter waits before timing out a report e.g. 00:00:30 daysofweek Numeric value required for the day of the week. Where 0 is Saturday and 2 is Monday When running a report on a gateway, the gateway field must be set to the proctype of the gateway that will be queried. It will then run the report on the processes which are listed in the proctype field and join the results by using the function specified in the joinfunction field. If there is no join function then the reporter process will not start. Multiple entries in the proctype field must be separated by a space and are only allowed when the gateway field is not empty. If gateway field is empty and there are multiple entries in the proctype field then the reporter process will not load. Listing [code:csvschema] shows an example of the schema needed in the input csv file. name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek usage|10#.usage.usage|writetofiletype[\"./output/\";\"usage\";\"csv\"]|||rdb||00:01|23:50|00:01|00:00:01|0 1 2 3 4 5 6 memory|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|||rdb|rdb1|00:05|18:00|00:01|00:00:08|0 1 2 3 4 5 6 usage_gateway|10#.usage.usage||gateway|raze|rdb hdb||00:02|22:00|00:01|00:00:10|0 1 2 3 4 5 6 Result Handlers There are several default result handlers which are listed below. Custom result handlers can be defined as required. The result handler will be invoked with a single parameter (the result of the query). writetofiletype - Accepts 3 parameters: path, filename, filetype and data. When writing to file it uses a date time suffix so the resultant filename will be usage_rdb_2014_01_02_15_00_12.txt e.g. writetofiletype[\"./output/\";\"usage\";\"csv\"] splaytable - This accepts 3 parameters: path, file and data. This splays the result to a directory. The result must be a table in order to use this function e.g. splaytable[\"./output/\";\"usage\"] emailalert - This accepts 3 parameters: period, recipient list and data. The period dictates the throttle i.e. emails will be sent at most every period. The result of the report must be a table with a single column called messages which contains the character list of the email message. This is used with the monitoring checks to raise alerts, but can be used with other functions. emailalert[0D00:30;(\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\")] emailreport - This accepts 3 parameters: temporary path, recipient list, file name, file type and data. The data is written out as the file type (e.g. csv, xml, txt, xls, json) with the given file name to the temporary path. It is then emailed to the recipient list, and the temporary file removed. emailreport[\"./tempdir/\"; (\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\"); \"EndOfDayReport\"; \"csv\"] publishresult - Accepts 1 parameter and that is the data. This is discussed later in the subsection subresults. Custom result handlers can be added to $KDBCODE/processes/reporter.q . It is important to note that the result handler is referencing local functions as it is executed in the reporter process and not the target process. When the query has been successful the result handler will be passed a dictionary with the following keys: queryid, time, name, procname, proctype and result. Report Process Tracking Each step of the query is logged by the reporter process. Each query is given a unique id and regular system messages are given the id 0. The stage column specifies what stage the query is in and these are shown in table [table:stagetable]. An appropriate log message is also shown so any problems can easily be diagnosed. The in memory table is flushed every interval depending on the value of the flushqueryloginterval variable in the reporter.q file found in the $KDBCONFIG/settings directory. Stage symbol Explanation R The query is currently running E An error has occurred during the query C The query has been completed with no errors T The query has exceeded the timeout interval S System message e.g. \u201cReporter Process Initialised\u201d time | queryid stage message -----------------------------| ------------------------------------------------------------------------ 2014.10.20D22:20:06.597035000| 37 R \"Received result\" 2014.10.20D22:20:06.600692000| 37 R \"Running resulthandler\" 2014.10.20D22:20:06.604455000| 37 C \"Finished report\" 2014.10.20D22:30:00.984572000| 38 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\" 2014.10.20D22:30:00.991862000| 38 R \"Received result\" 2014.10.20D22:30:00.995527000| 38 R \"Running resulthandler\" 2014.10.20D22:30:00.999236000| 38 C \"Finished report\" 2014.10.20D22:30:06.784419000| 39 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\" 2014.10.20D22:30:06.796431000| 39 R \"Received result\" Subscribing for Results To publish the results of the report, the reporter process uses the pub sub functionality of TorQ. This is done by using the using the inbuilt result handler called publishresult. In order to subscribe to this feed, connect to the reporter process and send the function shown below over the handle. To subscribe to all reports use a backtick as the second parameter and to subscribe to a specific reports results include the reporter name as a symbol. /- define a upd function upd:insert /- handle to reporter process h: hopen 20004 /- Subscribe to all results that use the publishresult handler h(`.ps.subscribe;`reporterprocessresults;`) /- Subscribe to a specific report called testreport h(`.ps.subscribe;`reporterprocessresults;`testreport) Example reports The following are examples of reports that could be used in the reporter process. The rdbtablecount report will run hourly and return the count of all the tables in a rdb process. The memoryusage report will run every 10 minutes against the gateway for multiple processes and will return the .Q.w[] information. Both of these reports run between 9:30am to 4:00pm during the weekdays. The report onetimequery is an example of a query that is run one time, in order to run a query once, the period must be the same as the difference between the start and end time. name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek rdbtablecount|ts!count each value each ts:tables[]|{show x`result}|||rdb|rdb1|09:30|16:00|01:00|00:00:10|2 3 4 5 6 memoryusage|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|gateway1|{enlist raze x}|rdb hdb||09:30|16:00|00:10|00:00:10|2 3 4 5 6 onetimequery|10#.usage.usage|writetofile[\"./output/\";\"onetime.csv\"]|||rdb||10:00|10:01|00:01|00:00:10|2 3 4 5 6 Monitor The Monitor process is a simple process to monitor the health of the other processes in the system. It connects to each process that it finds (by default using the discovery service, though can use the static file as well) and subscribes to both heartbeats and log messages. It maintains a keyed table of heartbeats, and a table of all log messages received. Run it with: aquaq $ q torq.q -load code/processes/monitor.q -p 20001 -proctype monitor -procname monitor1 It is probably advisable to run the monitor process with the -trap flag, as there may be some start up errors if the processes it is connecting to do not have the necessary heartbeating or publish/subscribe code loaded. aquaq $ q torq.q -load code/processes/monitor.q -p 20001 -trap -proctype monitor -procname monitor1 The current heartbeat statuses are tracked in .hb.hb, and the log messages in logmsg q)show .hb.hb sym procname | time counter warning error ----------------------| --------------------------------------------------- discovery discovery2 | 2014.01.07D13:24:31.848257000 893 0 0 hdb hdb1 | 2014.01.07D13:24:31.866459000 955 0 0 rdb rdb_europe_1| 2014.01.07D13:23:31.507203000 901 1 0 rdb rdb1 | 2014.01.07D13:24:31.848259000 34 0 0 q)show select from logmsg where loglevel=`ERR time sym host loglevel id message ------------------------------------------------------------------------------------- 2014.01.07D12:25:17.457535000 hdb1 aquaq ERR reload \"failed to reload database\" 2014.01.07D13:29:28.784333000 rdb1 aquaq ERR eodsave \"failed to save tables : trade, quote\" Checkmonitor The checkmonitor.q script extends the functionality of the monitor process. The script takes a set of user defined configuration settings for a set of process specific checks. These can initially be provided in the form of a CSV, a sample of which is shown here: family|metric|process|query|resultchecker|params|period|runtime datacount|tradecount|rdb1|{count trade}|checkcount|`varname`count`cond!(`trade;10;`morethan)|0D00:01|0D00:00:01 Upon start up, the CSV file is loaded and inserted into the in-memory table, checkconfig . During this insertion, each check will also be assigned a unique checkid number. q)checkconfig checkid| family metric process query resultchecker params period runtime active -------| ----------------------------------------------------------------------------------------------------------------------------------------------------- 1 | datacount tradecount rdb1 \"{count trade}\" \"checkcount\" `varname`count`cond!(`trade;10;`morethan) 0D00:01:00.000000000 0D00:00:01.000000000 1 For each check, the query will be sent via asynchronous requests to the specified processes and waits for postback of the results. Once the monitoring process receives the result of the query, it will then be checked by the resultchecker function to identify whether it will pass or fail. Result checker functions must only take two parameters: p- a parameter dictionary, and r- the result row. The status in r will be modified based on whether the r result value passes the conditions specified by the resultchecker function. q)checkcount {[p;r] if[`morethan=p`cond; if[p[`count]<r`result; :`status`result!(1h;\"\")]; :`status`result!(0h;\"variable \",(string p`varname),\" has count of \",(string r`result),\" but requires \",string p`count)]; if[`lessthan=p`cond; if[p[`count]>r`result; :`status`result!(1h;\"\")]; :`status`result!(0h;\"variable \",(string p`varname),\" has count of \",(string r`result),\" but should be less than \",string p`count)]; } q)p varname| `trade count | 10 cond | `morethan q)r status| 1h result| \"\" This example checks whether the trade table within the rdb is larger than 10. As this is true, the status has been set to 1h and no error message has been returned. This information is inserted into the checkstatus table, which is the primary table where all results are stored. q)checkstatus checkid| family metric process lastrun nextrun status executiontime totaltime timerstatus running result -------| -------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | datacount tradecount rdb1 2019.02.18D10:58:45.908919000 2019.02.18D10:59:45.911635000 1 0D00:00:00.000017000 0D00:00:00.002677000 1 0 \"\" In addition to tracking the status of the specified queries, a number of metrics are also returned in the checkstatus table. Column Header Value Type Description lastrun Timestamp Last time check was run nextrun Timestamp Next time check is scheduled to run status Boolean Indicates whether query result has passed resultchecker function executiontime Timespan Time taken for last query to be executed totaltime Timespan Total time taken for check to be run, including sending times and execution timerstatus Boolean Indicates whether last totaltime was executed under threshold value running Boolean Indicates whether check is currently running results String Will display any errors or additional information regarding running checks The function checkruntime uses the running column to identify functions that are running extremely slow, and set their status and timerstatus to 0h. When the process is exited, the .z.exit has been modified to save the checkconfig table as a flat binary file. This will then be preferentially loaded next time the process is started up again. The process of saving down the in-memory functions makes altering configuration parameters easier. Four functions are available to do so: addcheck , updateconfig , updateconfigfammet and forceconfig . Function Name Description addcheck[dictionary] addcheck allows a new check to be added, and accepts a dictionary as its argument. The keys must be a match to the current checkconfig table, and the values must be of the correct type. updateconfig[checkid;paramkey;newval] updateconfig changes the parameter key of an existing check, using the checkid to specify which check to alter. The type of the new parameter value must match the current value type. forceconfig[checkid;newconfig] forceconfig changes the parameter keys of an existing check and will not check for types. updateconfigfammet[family;metric;paramkey;newval] updateconfig changes the parameter key of an existing check, using the family and metric combination to specify which check to alter. If this combination does not exist, the function will return an error. The type of the new parameter value must match the current value type. There are other additional functions that are useful for using the check monitor. Function Name Value Type currentstatus Will return only status, timerstatus, result and running from the checktracker table. It accepts a list of checkids, or will return all checks if passed a null. timecheck Will check the median time for current checks to be run against a user-defined timespan. It returns a table displaying the median time and a boolean value. statusbyfam Function will return a table of all checks from specified families, ordered firstly by status, and then by timestatus. If a null is provided, ordered checks from all families will be returned. All checks can be tracked using the table checktracker . Here, each run is assigned a unique runid- thus individual runs for each check can be tracked. For each run, it tracks the time tkane for target process to recieve the query, as well as the execution time. The result value will also be displayed. HTML5 front end A HTML5 front end has been built to display important process information that is sent from the monitor process. It uses HTML5, WebSockets and JavaScript on the front end and interacts with the monitor process in the kdb+ side. The features of the front end include: Heartbeat table with processes that have warnings highlighted in orange and errors in red Log message table displaying the last 30 errors Log message error chart that is by default displayed in 5 minute bins Chart\u2019s bin value can be changed on the fly Responsive design so works on all main devices i.e. phones, tablets and desktop It is accessible by going to the url http://HOST:PORT/.non?monitorui Compression The compression process is a thin wrapper around the compression utility library. It allows periodic compression of whole or parts of databases (e.g. data is written out uncompressed and then compressed after a certain period of time). It uses four variables defined in KDBCONFIG/settings/compression.q which specify the compression configuration file to use the database directory to compress the maximum age of data to attempt to compress whether the process should exit upon completion The process is run like other TorQ processes: q torq.q -load code/processes/compression.q -p 20005 -proctype compression -procname compression1 Modify the settings file or override variables from the command line as appropriate. Kill The kill process is used to connect to and terminate currently running processes. It kills the process by sending the exit command therefore the kill process must have appropriate permissions to send the command, and it must be able to create a connection (i.e. it will not be able to kill a blocked process in the same way that the unix command kill -9 would). By default, the kill process will connect to the discovery service(s), and kill the processes of the specified types. The kill process can be modified to not use the discovery service and instead use the process.csv file via the configuration in the standard way. If run without any command line parameters, kill.q will try to kill each process it finds with type defined by its .servers.CONNECTIONS variable. q torq.q -load code/processes/kill.q -p 20000 -proctype kill -procname killtick .servers.CONNECTIONS can optionally be overridden from the command line (as can any other process variable): q torq.q -load code/processes/kill.q -p 20000 -.servers.CONNECTIONS rdb tickerplant -proctype kill -procname killtick The kill process can also be used to kill only specific named processes within the process types: q torq.q -load code/processes/kill.q -p 20000 -killnames hdb1 hdb2 -proctype kill -procname killtick Chained Tickerplant In tick+ architecture the main tickerplant is the most important component, as it is relied upon by all the real time subscribers. When the tickerplant goes down data will be lost, compare this to an rdb which can be recovered after it fails. The chained tickerplant process is an additional tickerplant that is a real time subscriber to the main tickerplant but replicates its behaviour. It will have its own real time subscribers and can be recovered when it fails. This is the recommended approach when users want to perform their own custom real time analysis. The chained tickerplant can: subscribe to specific tables and syms batch publish at an interval or publish tick by tick create a tickerplant log that its real time subscribers can replay replay the source tickerplant log To launch the chained tickerplant q torq.q -load code/processes/chainedtp.q -p 12009 -proctype chainedtp -procname chainedtp1 Chained tickerplant settings are found in config/settings/chainedtp.q and are under the .ctp namespace. Setting Explanation Default tickerplantname list of tickerplant names to try and make a connection to `tickerplant1 pubinterval publish batch updates at this interval. If the value is 0D00:00:00 then it will publish tick by tick 0D00:00:00 tpconnsleep number of seconds between attempts to connect to the source tickerplant 10 createlogfile create a log file 0b logdir directory containing chained tickerplant logs `:hdb subscribeto subscribe to these tables only (null for all) ` subscribesyms subscribe to these syms only (null for all) ` replay replay the tickerplant log file 0b schema retrieve schema from tickerplant 1b clearlogonsubscription clear log on subscription, only called if createlogfile is also enabled 0b TorQ Data Quality System Architecture Whilst the Monitor process checks the health of other processes in the system, it does not check the quality of the data captured. An RDB process could be running, but capturing and populating its tables with null values, or not running at all. These errors would not be caught by the Monitor process, so the Data Quality System has been developed with the purpose of capturing such errors. The system periodically runs a set of user-specified checks on selected processes to ensure data quality. For example, you can place checks on a table to periodically check that the percentage of nulls it contains in a certain column is below a given threshold, or check that the values of a column stay within a specified range that changes throughout the day. The system behaves based on Data Quality Config files. The metrics from the config files and the results of the checks performed on databases in the data capturing system are saved to Data Quality Databases. The results can then be used for monitoring tools to alert users. The Data Quality System consists of four processes: the Data Quality Checker(DQC) and the Data Quality Engine(DQE), as well as a Database process for each (DQCDB and DQEDB). These processes are explained in detail below. Data Quality Checker (DQC) The Data Quality Checker process runs checks on other TorQ processes to check the quality of data in the system. The specific parameters of the checks that are being run can be configured in config/dqcconfig.csv . Configuration from dqcconfig.csv is then loaded to configtable in the dqe namespace, and the checks are then run based on the parameters. The results of the checks are saved to the results table in the dqe namespace. The results table that contains all check results will periodically be saved to the DQCDB intraday. The configuration of the checks will also be periodically saved to the DQCDB throughout the day. Example of configtable is shown below: action params proc mode starttime endtime period checkid -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- tableticking \"`comp`vars!(0b;(`quote;5;`minute))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:05:00.000000000 0 chkslowsub \"`comp`vars!(0b;1000000)\" \"`tickerplant\" repeat 2020.03.20D09:00:00.000000000 2020.03.20D19:00:00.000000000 0D00:10:00.000000000 1 tableticking \"`comp`vars!(0b;(`quote;5;`minute))\" \"`rdb1\" single 2020.03.20D12:00:00.000000000 2 constructcheck \"`comp`vars!(0b;(`quote;`table))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:02:00.000000000 3 constructcheck \"`comp`compallow`compproc`vars!(1b;0;`hdb1;(`date;`variable))\" \"`hdb2\" repeat 2020.03.20D09:00:00.000000000 0D00:02:00.000000000 4 attrcheck \"`comp`vars!(0b;(`quote;`s`g;`time`sym))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:02:00.000000000 5 schemacheck \"`comp`vars!(0b;(`quote;`time`sym`bid`ask`bsize`asize`mode`ex`src;\\\"psffjjccs\\\";`````````;`s`g```````))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:15:00.000000000 6 freeform \"`comp`vars!(0b;\\\"select from trade where date=2020.01.02\\\")\" \"`hdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:03:00.000000000 7 symfilecheck \"(`comp`vars!(0b;(.dqe.hdbdir;`sym)))\" \"`hdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:10:00.000000000 8 xmarketalert \"`comp`vars!(0b;(`quote))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:05:00.000000000 9 action - The check function that you would like to perform. Specific checks and usages can be found in the directory code/dqc . params - The parameters that are used by the function. The Checker accepts params as a dictionary, with comp and vars being the two keys when function is only used on one process, and comp , compallow , compproc , and vars being the four keys when function is used to compare two processes. comp should be assigned a boolean value - when it is 1b , comparison is ON and the function is then used to compare two processes. compallow would then be the percentage allowed for for the difference between the results that are returned from the two processes. compproc would be the process that you are comparing proc with. vars would be the variable used for the function provided. When it is 0b , comparison is OFF and the function is only used in one process. The var key should have a value of all the variables that are used by the function. Details of the variables used by checker functions can be found in config/dqedetail.csv . proc - The process(es) that you would want the check function to be used for - in symbol datatype. mode - Mode could be set to either repeat or single . When it is set to repeat the check will be periodically run based on the period parameter, which sets the time it will take before running the same check again. When it is set to single , the check will only be ran once. starttime - The start time of the check function - in timespan datatype. endtime - When mode is repeat , end time specifies when the check will run for the last time for the day. When mode is single , endtime should be set to 0N . Should be in timespan datatype. period - When mode is repeat , period represents the time it takes for the next run to start. When mode is single , period should be set to 0N . Should be in timespan datatype. An example of .dqe.results is shown below: id funct params procs procschk starttime endtime result descp chkstatus chkruntype --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2 .dqc.tableticking quote,5,minute rdb rdb1 2020.03.19D20:04:59.777402000 2020.03.19D20:04:59.784107000 1 \"there are 44 records\" complete scheduled 0 .dqc.tableticking quote,5,minute rdb rdb1 2020.03.19D20:05:00.178288000 2020.03.19D20:05:00.186770000 1 \"there are 82 records\" complete scheduled 9 .dqc.xmarketalert quote rdb rdb1 2020.03.19D20:05:00.179664000 2020.03.19D20:05:00.193734000 1 \"bid has not exceeded the ask in market data\" complete scheduled 3 .dqc.constructcheck quote,table rdb rdb1 2020.03.19D20:06:00.178885000 2020.03.19D20:06:00.196380000 1 \"quote table exists\" complete scheduled 4 .dqc.constructcheck date,variable,comp(hdb1,0) hdb hdb2 2020.03.19D20:06:00.180247000 2020.03.19D20:06:00.203920000 1 \"hdb1 | match hdb2\" complete scheduled 5 .dqc.attrcheck quote,s,g,time,sym rdb rdb1 2020.03.19D20:06:00.182379000 2020.03.19D20:06:00.201300000 1 \"attribute of time,sym matched expectation\" complete scheduled 7 .dqc.freeform select from trade where date=2020.01.02 hdb hdb1 2020.03.19D20:06:00.184577000 2020.03.19D20:06:00.205209000 1 \"select from trade where date=2020.01.02 passed\" complete scheduled 3 .dqc.constructcheck quote,table rdb rdb1 2020.03.19D20:08:00.378141000 2020.03.19D20:08:00.398781000 1 \"quote table exists\" complete scheduled 4 .dqc.constructcheck date,variable,comp(hdb1,0) hdb hdb2 2020.03.19D20:08:00.379574000 2020.03.19D20:08:00.404501000 1 \"hdb1 | match hdb2\" complete scheduled The columns of results table provide information about the checks performed: id - The id of the check after being loaded into the process. In the sample table above, .dqc.construckcheck with the id 3 that checks whether quote table exists and .dqc.construckcheck with the id 4 that checks whether data in hdb1 matches hdb2 had two results showing as they have been ran twice within that time period, but the parameters of the checks have not been changed. The id column is also useful for manually running checks. funct - The check function that was performed. params - The variables that were input while the check function was performed. procs - The process(es) type that the function was performed on. procschk - The specific process(es) that the function was performed on. starttime - When the function was started. endtime - When the function stopped running. result - Returns a boolean value. If the result is 1 , then the function was ran successfully, and no data anomaly was found. If the result is 0 , then the function may not have been run successfully, or the data quality may be corrupted. descp - A string description describing the result of the check function. chkstatus - Could display either complete or failed . When the check function runs successfully, whether the result column is 0 or 1, chkstatus would be complete . However, if there was an error that caused the check to not run successfully(Ex: variables being a wrong type), failed would be displayed instead. chkruntype - Could display either scheduled or manual , meaning either the check was ran as scheduled from the configtable, or it was forced to run manually from the console. Below, we list all the built-in checks that we offer as part of the Data Quality Checker. .dqc.constructcheck - Checks if a construct exists. .dqc.tableticking - Checks if a table has obtained records within a specified time period. .dqc.chkslowsub - Checks queues on handles to make sure there are no slow subscribers. .dqc.tablecount - Checks a table count against a number. This can be a > , < or = relationship. .dqc.tablehasrecords - A projection of .dqc.tablecount that is used to check if a table is non-empty. .dqc.attrcheck - Checks that a tables actual schema matches the expectation. .dqc.infinitychk - Checks the percentage of infinities in certain columns of a table are below a given threshold. .dqc.freeform - Checks if a query has passed correctly. .dqc.schemacheck - Checks if the meta of a table matches expectation. .dqc.datechk - Checks the date vector contains the latest date in a HDB. .dqc.nullchk - Checks percentages of nulls in columns of a table are below a given threshold. .dqc.symfilecheck - Checks that the sym file exists. .dqc.xmarketalert - Tests whether the bid price has exceeded in the ask price in market data. .dqc.dfilechk - Checks the .d file in the latest date partition matches the previous date values. .dqc.rangechk - Checks whether the values of columns of a table are within a given range. .dqc.tablecomp - Counts the number of rows in a table, used for comparison between two processes .dqc.pctAvgDailyChange - Checks if a function applied to a table is within the threshold limits of an n-day average. .dqc.symfilegrowth - Checks if today's sym file count has grown more than a certain percentage .dqc.timdiff - Checks if idfferences between time columns are over a certain limit .dqc.memoryusage - Checks percentage of memory usage compared to max memory .dqc.refdatacheck - Checks whether the referenced column of a table is in another column of another table New Custom Check To add custom checks, create a new q file in /code/dqc. The new q script should be under the namespace dqc, and should contain the function. The function should return a mixed list with two atoms: first atom being boolean, and second being string. The first atom would contain whether the check was successful (1b) or was there an error/failed check(0b). This will be shown in the result column of the results table. The second atom would contain the description of the result of the check. Below is a sample of a qscript named customcheck.q, with the function customcheck, written in pseudo-code for reference: \\d .dqc customcheck:{[variable1;variable2] $[variable1~variable2; (1b;\"Description if the function ran successfully\"); (0b;\"Description if the function failed to run\")] } To use the function to run check, proceed to /appconfig/dqcconfig.csv, and modify how you would want the check to be ran. As an example, to run the customcheck above with the following settings - params - Not comparing two processes, and running with the two variables being abc and def . proc - running on the processes rdb1 and rdb2 . mode - the function being run repeatedly. starttime - 9AM. endtime - not specified. period - the check to be run every 10 minutes. The line in dqcconfig.csv should be: action,params,proc,mode,starttime,endtime,period customcheck,`comp`vars!(0b;(`abc;`def)),`rdb1;`rdb2,repeat,09:00:00.000000000,0Wn,0D00:10:00 To add a check that compares two processes, the check function would have to return a mixed listed with three atoms. The first two being the same as above, and the third being a numeric value. The numeric value would be what is compared between the two processes. Below is a sample of a qscript named customcheck.q, with the function customcheck, written in pseudo-code for reference to compare two processes: \\d .dqc customcheck:{[variable1;variable2] $[variable1~variable2; (1b;\"Description if the function ran successfully\";count variable1); (0b;\"Description if the function failed to run\";avg variable2)] } Secondly, the line in dqcconfig.csv should be modified. As mentioned above, the params parameter would now have to be changed, and two additional dictionary keys would have to be added. As an example, if we were to run the function customcheck comparing hdb1 and hdb2 , with there being no difference allowed between the two results returned from the two processes, the line would be the following: action,params,proc,mode,starttime,endtime,period customcheck,`comp`compallow`compproc`vars!(1b;0;`hdb1;(`abc;`def)),`hdb2,repeat,09:00:00.000000000,0Wn,0D00:10:00 If we were to allow for 50% difference between the results returned from the two processes using the function customcheck, the line would be the following: action,params,proc,mode,starttime,endtime,period customcheck,`comp`compallow`compproc`vars!(1b;50;`hdb1;(`abc;`def)),`hdb2,repeat,09:00:00.000000000,0Wn,0D00:10:00 Data Quality Engine (DQE) The DQE process stores daily statistics of other TorQ processes. It is a separate process from the DQC because the DQE does not run checks. The DQE and the DQC could be used to track the percentage change of records in a table from day to day. The Checker can then use the data saved the DQEDB to perform advanced checks.The behaviour of the Engine is based on the config file stored in config/dqengineconfig.csv . The config csv file is shown below: query,params,proc,querytype,starttime ------------------------------------- tablecount,.z.d-1,`hdb1,table,09:00:00.000000000 symfilecheck,`sym,`hdb1,other,09:00:00.000000000 symcount,(`quote;`sym),`hdb1,table,09:00:00.000000000 query - The query function that is used to provide daily statistics of a process. params - the variables that is used for the query function. proc - The process that the query function is running on. querytype - Whether the query is ran for a table or other . starttime - What time should the query start. The daily statistics of other TorQ processes are saved to the resultstab and the advancedres tables in dqe namespace, which would be saved to Data Quality Engine Database (DQEDB). The resultstab table contains the results of simple qeuries that return intger values that is reflected by the column resvalue . The advancedres table contains the results of advanced queries that return whole tables, which is reflected by the column \"resultdata\". Example of resultstab is shown below: date procs funct table column resvalue ------------------------------------------------------- 2020.03.16 hdb1 tablecount quote 0 2020.03.16 hdb1 tablecount quote_iex 0 2020.03.16 hdb1 tablecount trade 0 2020.03.16 hdb1 tablecount trade_iex 0 2020.03.16 hdb1 symfilecheck 10 2020.03.16 hdb1 symcount quote sym 10 date - Date that the data was saved. procs - The process that the query function was running on. funct - The query function that was used. table - Table that the function ran on. If the query was not performed on a table, the section is left blank. column - Column of the table that the function ran on. If the query did not specify the column, the section is left blank. resvalue - The value returned from the function that was ran. Example of advancedres is shown below: date procs funct table resultkeys resultdata --------------------------------------------------------- 2020.07.22 hdb1 bycount quote sym (+(,`sym)!,`AAPL`AIG`AMD`DELL`DOW`GOOG`HPQ`IBM`INTC`MSFT)!+(,`i)!,209356 208544 209003 208687 208420 209706 208319 207438 207455 209588 date - Date that the data was saved. procs - The process that the query function was running on. funct - The query function that was used. table - Table that the function ran on. If the query was not performed on a table, the section is left blank. resultkeys - Keys or variables used to generate the result table, or the table in resultdata. resultdata - The table retrned from the function that was ran. New Custom Queries To add custom queries, create a new q file in /code/dqe. The new q script should be under the namespace dqe, and should contain the function. The function should return a dictionary, With the key being the function name, and the value being the statistic that the query should return. The value from the dictionary will be shown as the resvalue in the resultstab table. Below is a sample of a qscript named customquery.q, with the function customquery, written in pseudo-code for reference: \\d .dqe customquery:{[variable1;variable2] (enlist variable1)!enlist count variable2 } To use the function to run check, proceed to /appconfig/dqengineconfig.csv, and modify how you would want the check to be ran. As an example, to run the customquery above with the following settings - params - Variable being abc . proc - running on the process hdb1 . querytype - other as it is not a table starttime - 9AM The line in the config csv should be: query,params,proc,querytype,starttime customquery,`table`quote,`hdb1,other,09:00:00.000000000","title":"Processes"},{"location":"Processes/#processes","text":"A set of processes is included. These processes build upon AquaQ TorQ, providing specific functionality. All the process scripts are contained in $KDBCODE/processes. All processes should have an entry in $KDBCONFIG/process.csv. All processes can have any type and name, except for discovery services which must have a process type of \u201cdiscovery\u201d. An example process.csv is: aquaq$ cat config/process.csv host,port,proctype,procname aquaq,9998,rdb,rdb_europe_1 aquaq,9997,hdb,rdb_europe_1aquaq,9999,hdb,hdb1 aquaq,9996,discovery,discovery1 aquaq,9995,discovery,discovery2 aquaq,8000,gateway,gateway1 aquaq,5010,tickerplant,tickerplant1 aquaq,5011,rdb,rdb1 aquaq,5012,hdb,hdb1 aquaq,5013,hdb,hdb2 aquaq,9990,tickerlogreplay,tpreplay1 aquaq,20000,kill,killhdbs aquaq,20001,monitor,monitor1 aquaq,20002,housekeeping,hk1","title":"Processes"},{"location":"Processes/#discovery-service","text":"","title":"Discovery Service"},{"location":"Processes/#overview","text":"Processes use the discovery service to register their own availability, find other processes (by process type) and subscribe to receive updates for new process availability (by process type). The discovery service does not manage connections- it simply returns tables of registered processes, irrespective of their current availability. It is up to each individual process to manage its own connections. The discovery service uses the process.csv file to make connections to processes on start up. After start up it is up to each individual process to attempt connections and register with the discovery service. This is done automatically, depending on the configuration parameters. Multiple discovery services can be run in which case each process will try to register and retrieve process details from each discovery process it finds in its process.csv file. Discovery services do not replicate between themselves. A discovery process must have its process type listed as discovery. To run the discovery service, use a start line such as: aquaq $ q torq.q -load code/processes/discovery.q -p 9995 -proctype discovery -procname discovery1 Modify the configuration as required.","title":"Overview"},{"location":"Processes/#operation","text":"Processes register with the discovery service. Processes use the discovery service to locate other processes. When new services register, any processes which have registered an interest in that process type are notified.","title":"Operation"},{"location":"Processes/#available-processes","text":"The list of available processes can be found in the .servers.SERVERS table. q).servers.SERVERS procname proctype hpup w hits startp lastp endp attributes ------------------------------------------------------------------------------------- discovery1 discovery :aquaq:9995 0 2014.01.22D17:00:40.947470000 ()!() discovery2 discovery :aquaq:9996 0 2014.01.22D17:00:40.947517000 ()!() hdb2 hdb :aquaq:5013 0 2014.01.22D17:00:40.947602000 ()!() killtick kill :aquaq:20000 0 2014.01.22D17:00:40.947602000 ()!() tpreplay1 tickerlogreplay :aquaq:20002 0 2014.01.22D17:00:40.947602000 ()!() tickerplant1 tickerplant :aquaq:5010 6 0 2014.01.22D17:00:40.967699000 2014.01.22D17:00:40.967698000 ()!() monitor1 monitor :aquaq:20001 9 0 2014.01.22D17:00:40.971344000 2014.01.22D17:00:40.971344000 ()!() rdb1 rdb :aquaq:5011 7 0 2014.01.22D17:06:13.032883000 2014.01.22D17:06:13.032883000 `date`tables!(,2014.01.22;`fxquotes`heartbeat`logmsg`quotes`trades) hdb3 hdb :aquaq:5012 8 0 2014.01.22D17:06:18.647349000 2014.01.22D17:06:18.647349000 `date`tables!(2014.01.13 2014.01.14;`fxquotes`heartbeat`logmsg`quotes`trades) gateway1 gateway :aquaq:5020 10 0 2014.01.22D17:06:32.152836000 2014.01.22D17:06:32.152836000 ()!()","title":"Available Processes"},{"location":"Processes/#gateway","text":"A synchronous and asynchronous gateway is provided. The gateway can be used for load balancing and/or to join the results of queries across heterogeneous servers (e.g. an RDB and HDB). Ideally the gateway should only be used with asynchronous calls. Prior to KDB v3.6, synchronous calls caused the gateway to block which limits the gateway to serving one query at a time (although if querying across multiple backend servers the backend queries will be run in parallel). For v3.6+, deferred synchronous requests to the gateway are supported. This allows the gateway to process multiple synchronous requests at once, therefore removing the requirement for the gateway to allow only one type of request. When using asynchronous calls, the client can either block and wait for the result (deferred synchronous) or post a call back function which the gateway will call back to the client with. The backend servers to be queried against with asynchronous and synchronous queries are selected using process type. The gateway API can be seen by querying .api.p\u201c.gw.*\u201d within a gateway process.","title":"Gateway"},{"location":"Processes/#asynchronous-behaviour","text":"Asynchronous queries allow much greater flexibility. They allow multiple queries to be serviced at once, prioritisation, and queries to be timed out. When an asynchronous query is received the following happens: the query is placed in a queue; the list of available servers is retrieved; the queue is prioritised, so those queries with higher priority are serviced first; queries are sent to back end servers as they become available. Once the backend server returns its result, it is given another query; when all the partial results from the query are returned the results are aggregated and returned to the client. They are either returned directly, or wrapped in a callback and posted back asynchronously to the client. The two main customisable features of the gateway are the selection of available servers (.gw.availableservers) and the queue prioritisation (.gw.getnextqueryid). With default configuration, the available servers are those servers which are not currently servicing a query from the gateway, and the queue priority is a simple FIFO queue. The available servers could be extended to handle process attributes, such as the available datasets or the location of the process, and the queue prioritisation could be modified to anything required e.g. based on the query itself, the username, host of the client etc. An asynchronous query can be timed out using a timeout defined by the client. The gateway will periodically check if any client queries have not completed in the alotted time, and return a timeout error to the client. If the query is already running on any backend servers then they cannot be timed out other than by using the standard -T flag.","title":"Asynchronous Behaviour"},{"location":"Processes/#synchronous-behaviour","text":"Prior to KDB v3.6, when using synchronous queries the gateway could only handle one query at a time and cannot timeout queries other than with the standard -T flag. The variable .gw.synccallsallowed is by default set to 0b prior to KDB v3.6. To send synchronous calls, edit the gateway.q file so that .gw.synccallsallowed is set to true. (The exception being with TorQ-FSP, in which case it is set to 1b by default.) For v3.6+, deferred synchronous calls are supported, allowing the gateway to process multiple requests at a time. All synchronous queries will be immediately dispatched to the back end processes. They will be dispatched using an asynchronous call, allowing them to run in parallel rather than serially. When the results are received they are aggregated and returned to the client.","title":"Synchronous Behaviour"},{"location":"Processes/#process-discovery","text":"The gateway uses the discovery service to locate processes to query across. The discovery service will notify the gateway when new processes become available and the gateway will automatically connect and start using them. The gateway can also use the static information in process.csv, but this limits the gateway to a predefined list of processes rather than allowing new services to come online as demand requires.","title":"Process Discovery"},{"location":"Processes/#error-handling","text":"All errors and results can now be formatted with the formatresult function. Each response to the client is passed through this function with inputs status (1b=result,0b=error), sync (1b=sync,0b=async) and result (result/error) to allow different errors/results to be handled appropriately. As default, when synchronous calls are used, q errors are returned to clients as they are encountered. When using asynchronous calls, appropriately prefixed strings are used. It is up to the client to check the type of the received result and if it is a string then whether it contains the error prefix. The error prefix can be changed, but the default is \u201cerror: \u201d. Alternatively, the formatresult function can be altered as necessary. Errors will be returned when: the client requests a query against a server type which the gateway does not currently have any active instances of (this error is returned immediately); the client requests a query with the wrong servertype types; the client requests a query with null servers; the query is timed out; a back end server returns an error; a back end server fails; the join function fails. If postback functions are used, the error string will be posted back within the postback function (i.e. it will be packed the same way as a valid result).","title":"Error Handling"},{"location":"Processes/#client-calls","text":"There are four main client calls. The .gw.sync* methods should only be invoked synchronously, and the .gw.async* methods should only be invoked asynchronously. Each of these are documented more extensively in the gateway api. Use .api.p\u201c.gw.*\u201d for more details. Function Description .gw.syncexec[query; servertypes] Execute the specified query synchronously against the required list of servers. If more than one server, the results will be razed. .gw.syncexecj[query; servertypes; joinfunction] Execute the specified query against the required list of servers. Use the specified join function to aggregate the results. .gw.asyncexec[query; servertypes] Execute the specified query against the required list of servers. If more than one server, the results will be razed. The client must block and wait for the results. .gw.asyncexecjpt[query; servertypes; joinfunction; postback; timeout] Execute the specified query against the required list of servers. Use the specified join function to aggregate the results. If the postback function is not set, the client must block and wait for the results. If it is set, the result will be wrapped in the specified postback function and returned asynchronously to the client. The query will be timed out if the timeout value is exceeded.","title":"Client Calls"},{"location":"Processes/#client-call-examples","text":"Here are some examples for using client calls via a handle to the gateway process. To reiterate, v3.6+ users can use synchronous calls, whilst asynchronous calls are only relevant for users on < v3.6.","title":"Client Call Examples"},{"location":"Processes/#calls-to-the-rdb-only","text":"For synchronous calls // To return the avg price per sym for the day so far q) h(`.gw.syncexec;\"select avp:avg price by sym from trade where time.date=.z.d\";`rdb) // hloc function in RDB process q) h(`.gw.syncexec;`hloc;`rdb) {[startdate;enddate;bucket] $[.z.d within (startdate;enddate); select high:max price, low:min price, open:first price,close:last price,totalsize:sum `long$size, vwap:size wavg price by sym, bucket xbar time from trade; ([sym:`symbol$();time:`timestamp$()] high:`float$();low:`float$();open:`float$();close:`float$();totalsize:`long$();vwap:`float$())]} // Using the hloc function - change query for appropriate date q) h(`.gw.syncexec;(`hloc;2020.01.08;2020.01.08;10);`rdb) // Returns following table sym time | high low open close totalsize vwap ----------------------------------| ---------------------------------------------- AAPL 2020.01.08D00:00:00.836801000| 103.62 103.62 103.62 103.62 88 103.62 AAPL 2020.01.08D00:00:01.804684000| 103.64 103.64 103.64 103.64 86 103.64 AAPL 2020.01.08D00:00:02.405682000| 103.86 103.86 103.86 103.86 90 103.86 AAPL 2020.01.08D00:00:03.005465000| 104.06 104.06 104.06 104.06 78 104.06 AAPL 2020.01.08D00:00:03.404383000| 103.9 103.9 103.9 103.9 49 103.9 .. For asynchronous calls // To return the sum size per sym for the day so far q) neg[h](`.gw.asyncexec;\"select sum size by sym from trade\";`rdb);h[]","title":"Calls to the RDB only"},{"location":"Processes/#calls-to-the-hdb-only","text":"For synchronous calls // For the high, low, open and close prices of the day before q) h(`.gw.syncexec;\"select h:max price, l:min price, o:first price, c:last price by sym from trade where date=.z.d-1\";`hdb) For asynchronous calls q) neg[h](`.gw.asyncexec;\"`$last .z.x\";`hdb);h[]","title":"Calls to the HDB only"},{"location":"Processes/#calls-to-the-hdb-and-rdb","text":"For synchronous calls q) h(`.gw.syncexec;\"$[.proc.proctype=`hdb; select from trade where date within (.z.d-2;.z.d-1); select from trade]\";`rdb`hdb) For asynchronous calls q) neg[h](`.gw.asyncexec;\"$[.proc.proctype=`hdb; select from trade where date within (.z.d-2;.z.d-1); select from trade]\";`rdb`hdb);h[]","title":"Calls to the HDB and RDB"},{"location":"Processes/#demonstrating-aggregation-of-data","text":"For the purposes of demonstration, assume that the following queries must be run across a single RDB and a single HDB process, and the gateway has one RDB and two HDB processes available to it. q).gw.servers handle| servertype inuse active querycount lastquery usage attributes ------| -------------------------------------------------------------------- 7 | rdb 0 1 17 2014.01.07D17:05:03.113927000 0D00:00:52.149069000 `datacentre`country!`essex`uk 8 | hdb 0 1 17 2014.01.07D17:05:03.113927000 0D00:01:26.143564000 `datacentre`country!`essex`uk 9 | hdb 0 1 2 2014.01.07D16:47:33.615538000 0D00:00:08.019862000 `datacentre`country!`essex`uk 12 | rdb 0 1 2 2014.01.07D16:47:33.615538000 0D00:00:04.018349000 `datacentre`country!`essex`uk Both the RDB and HDB processes have a function f and table t defined. f will run for 2 seconds longer on the HDB processes then it will the RDB. q)f {system\"sleep \",string x+$[`hdb=.proc.proctype;2;0]; t} //if process type is HDB, sleep for x+2 seconds and then return table t. If not, sleep for x seconds and return table t q)t:([]a:(5013;5014;5015;5016;5017)) q)t a ---- 5013 5014 5015 5016 5017 Run the gateway. The main parameter which should be set is the .servers.CONNECTIONS parameter, which dictates the process types the gateway queries across. Also, we need to explicitly allow sync calls. We can do this from the config or from the command line using the following line: q torq.q -load code/processes/gateway.q -p 8000 -.gw.synccallsallowed 1 -.servers.CONNECTIONS hdb rdb -proctype gateway -procname gateway1 Start a client and connect to the gateway. Start with a sync query. The HDB query should take 4 seconds and the RDB query should take 2 seconds. If the queries run in parallel, the total query time should be 4 seconds. q)h:hopen 8000 q)h(`.gw.syncexec;(`f;2);`hdb`rdb) a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 q)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb) 4009 If a query is done for a server type which is not registered, an error is returned: q)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb`other) `not all of the requested server types are available; missing other Custom join functions can be specified: q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by a from x} each x}) //[query;servertype;joinfunction(lambda)] a | x ----| - 5014| 2 5015| 2 5016| 2 5017| 1 5018| 1 5012| 1 5013| 1 Custom joins can fail with appropriate errors: q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by b from x} each x}) `failed to apply supplied join function to results: b Asynchronous queries must be sent in async and blocked: q)(neg h)(`.gw.asyncexec;(`f;2);`hdb`rdb); r:h(::) /- This white space is from pressing return /- the client is blocked and unresponsive q)q)q) q) q)r a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 q) We can send multiple async queries at once. Given the gateway has two RDBs and two HDBs avaialble to it, it should be possible to service two of these queries at the same time. q)h:hopen each 8000 8000 q)\\t (neg h)@\\:(`.gw.asyncexec;(`f;2);`hdb`rdb); (neg h)@\\:(::); r:h@\\:(::) 4012 q)r +(,`a)!,5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 +(,`a)!,5013 5014 5015 5016 5017 9999 10000 10001 10002 10003 Alternatively async queries can specify a postback so the client does not have to block and wait for the result. The postback function must take two parameters- the first is the function that was sent up, the second is the results. The postback can either be a lambda, or the name of a function eg. handleresults. q)h:hopen 8000 q)handleresults:{-1(string .z.z),\" got results\"; -3!x; show y} //postback with timestamp, got results and an output of the results q)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;{-1(string .z.z),\" got results\"; -3!x; show y};0Wn) //[.gw.asyncexecjpt[query;servertypes(list of symbols);joinfunction(lambda);postbackfunction(lambda or symbol);timeout(timespan)] q) q) /- These q prompts are from pressing enter q) /- The q client is not blocked, unlike the previous example q) q)2014.01.07T16:53:42.481 got results a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 /- Can also use a named function rather than a lambda q)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;`handleresults;0Wn) q) q) q)2014.01.07T16:55:12.235 got results a ---- 5014 5015 5016 5017 5018 5012 5013 5014 5015 5016 Asynchronous queries can also be timed out. This query will run for 22 seconds, but should be timed out after 5 seconds. There is a tolerance of +5 seconds on the timeout value, as that is how often the query list is checked. This can be reduced as required. q)(neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::) q)q)q)r \"error: query has exceeded specified timeout value\" q)\\t (neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::) 6550","title":"Demonstrating Aggregation of data"},{"location":"Processes/#non-kdb-clients","text":"All the examples in the previous section are from clients written in q. However it should be possible to do most of the above from non kdb+ clients. The officially supported APIs for Java, C# and C allow the asynchronous methods above. For example, we can modify the try block in the main function of the Java Grid Viewer : import java.awt.BorderLayout; import java.awt.Color; import java.io.IOException; import java.lang.reflect.Array; import java.util.logging.Level; import java.util.logging.Logger; import javax.swing.JFrame; import javax.swing.JScrollPane; import javax.swing.JTable; import javax.swing.table.AbstractTableModel; import kx.c; public class Main { public static class KxTableModel extends AbstractTableModel { private c.Flip flip; public void setFlip(c.Flip data) { this.flip = data; } public int getRowCount() { return Array.getLength(flip.y[0]); } public int getColumnCount() { return flip.y.length; } public Object getValueAt(int rowIndex, int columnIndex) { return c.at(flip.y[columnIndex], rowIndex); } public String getColumnName(int columnIndex) { return flip.x[columnIndex]; } }; public static void main(String[] args) { KxTableModel model = new KxTableModel(); c c = null; try { c = new c(\"localhost\", 8000,\"username:password\"); // Create the query to send String query=\".gw.asyncexec[(`f;2);`hdb`rdb]\"; // Send the query c.ks(query); // Block on the socket and wait for the result model.setFlip((c.Flip) c.k()); } catch (Exception ex) { Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex); } finally { if (c != null) {try{c.close();} catch (IOException ex) {} } } JTable table = new JTable(model); table.setGridColor(Color.BLACK); String title = \"kdb+ Example - \"+model.getRowCount()+\" Rows\"; JFrame frame = new JFrame(title); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.getContentPane().add(new JScrollPane(table), BorderLayout.CENTER); frame.setSize(300, 300); frame.setVisible(true); } } Some of the unofficially supported APIs may only allow synchronous calls to be made.","title":"Non kdb+ Clients"},{"location":"Processes/#tickerplant","text":"The tickerplant is a modified version of the standard kdb+tick tickerplant. The modifications from the standard tick.q include: Applies timestamps as timestamp rather than timespan; Tracks per table record counts in .u.icounts dictionary for faster recovery of real time subscribers; Allows configuration of timezones for timestamping data and performing end of day rollover (see eodtime.q ); The tickerplant log file will be written to hdb/database.","title":"Tickerplant"},{"location":"Processes/#segmented-tickerplant","text":"The idea behind the STP was to create a process which retained all the functionality of the Tickerplant while adding flexibility in terms of logging, publishing and subscriptions. The functionality of the STP is almost fully backwardly compatible with a few minor (and we believe seldom utilised) exceptions. We have introduced: ability to create more granular log files a new batch publication mode ability to easily introduce custom table modifications upon message receipt more flexible subscriptions error handling for easier debugging when developing data feeds performance improvements for several use cases faster restart All the TorQ based subscriber processes (e.g. RDB and WDB), and any subscribers that use the TorQ subscription library, can switch between the TP and STP. For the minor modifications that must be made to data consumers, please see the Subscriptions section.","title":"Segmented Tickerplant"},{"location":"Processes/#logging-modes","text":"The default TP logging behaviour is to write all updates to disk in a single log file which is rolled on a daily basis. The log file may become large. A consumer of the data must replay all the data from the log file, even if only a subset is required. Additionally, when the TP restarts, it must count through the log file in full to ensure that it has the correct message number. To add more flexibility, the following logging modes have been added which are set with the .stplg.multilog variable. Additionally a table of meta data is stored to describe the log files, including the full schema of all the tables contained within it. The .stpm.metatable table is saved as a q object to the STPLOG folder along with any error logs generated by the error mode. ``` singular: This mode is essentially the default TP behaviour, where all ticks across all tables for a given day are stored in a single file, eg. database20201026154808 . This is the simplest form of logging as everything is in one place. Note that in this mode, and all other logging modes, a new log file will be created on STP restart, and the stpmeta table updated accordingly. Note also that in all modes the timestamp on the end of the log file (YYYYMMDDHHMMSS) will come from the system clock when the log file is created, and will not be rounded to any particular value. stplogs \u251c\u2500\u2500stp1_2020.11.05/ \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 stpmeta \u2502 \u2514\u2500\u2500 stp1_20201105000000 \u2514\u2500\u2500stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 stpmeta \u2514\u2500\u2500 stp1_20201106000000 periodic: In this mode all the updates are stored in a the same file but the logs are rolled according to a custom period, set with .stplg.multilogperiod . For example, if the period is set to an hour a new log file will be created every hour and stored in a daily partitioned directory. As a result, this mode potentially allows for easier management by breaking the log files into smaller sizes, all while maintaining message arrival order during replays. Note that if the memory batch publish mode is used then the order of updates across different tables is no longer guaranteed (the order for a single table is guaranteed). stplogs \u251c\u2500\u2500stp1_2020.11.05/ \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 periodic20201105000000 \u2502 \u251c\u2500\u2500 periodic20201105010000 \u2502 \u251c\u2500\u2500 periodic20201105020000 \u2502 \u2514\u2500\u2500 stpmeta \u2514\u2500\u2500stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 periodic20201106000000 \u251c\u2500\u2500 periodic20201106010000 \u251c\u2500\u2500 periodic20201106020000 \u2514\u2500\u2500 stpmeta tabular: This mode is similar to the default behaviour except that each table has its own log file which is rolled daily in the form tradeYYYYMMDDHHMMSS . This allows for more granular log file management, and prioritisation of data recovery. stplogs/ \u251c\u2500\u2500 stp1_2020.11.05 \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 logmsg_20201105000000 \u2502 \u251c\u2500\u2500 packets_20201105000000 \u2502 \u251c\u2500\u2500 quote_20201105000000 \u2502 \u251c\u2500\u2500 quote_iex_20201105000000 \u2502 \u251c\u2500\u2500 stpmeta \u2502 \u251c\u2500\u2500 trade_20201105000000 \u2502 \u2514\u2500\u2500 trade_iex_20201105000000 \u2514\u2500\u2500 stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 logmsg_20201106000000 \u251c\u2500\u2500 packets_20201106000000 \u251c\u2500\u2500 quote_20201106000000 \u251c\u2500\u2500 quote_iex_20201106000000 \u251c\u2500\u2500 stpmeta \u251c\u2500\u2500 trade_20201106000000 \u2514\u2500\u2500 trade_iex_20201106000000 tabperiod: As the name suggests this mode combines the behaviour of the tabular and periodic logging modes, whereby each table has its own log file, each of which are rolled periodically as defined in the process. This adds the flexibility of both those modes when it comes to replays. stplogs/ \u251c\u2500\u2500 stp1_2020.11.05 \u2502 \u251c\u2500\u2500 err20201105000000 \u2502 \u251c\u2500\u2500 err20201105010000 \u2502 \u251c\u2500\u2500 logmsg_20201105000000 \u2502 \u251c\u2500\u2500 logmsg_20201105010000 \u2502 \u251c\u2500\u2500 packets_20201105000000 \u2502 \u251c\u2500\u2500 packets_20201105010000 \u2502 \u251c\u2500\u2500 quote_20201105000000 \u2502 \u251c\u2500\u2500 quote_20201105010000 \u2502 \u251c\u2500\u2500 quote_iex_20201105000000 \u2502 \u251c\u2500\u2500 quote_iex_20201105010000 \u2502 \u251c\u2500\u2500 stpmeta \u2502 \u251c\u2500\u2500 trade_20201105000000 \u2502 \u251c\u2500\u2500 trade_20201105010000 \u2502 \u251c\u2500\u2500 trade_iex_20201105000000 \u2502 \u2514\u2500\u2500 trade_iex_20201105010000 \u2514\u2500\u2500 stp1_2020.11.06 \u251c\u2500\u2500 err20201106000000 \u251c\u2500\u2500 err20201106010000 \u251c\u2500\u2500 logmsg_20201106000000 \u251c\u2500\u2500 logmsg_20201106010000 \u251c\u2500\u2500 packets_20201106000000 \u251c\u2500\u2500 packets_20201106010000 \u251c\u2500\u2500 quote_20201106000000 \u251c\u2500\u2500 quote_20201106010000 \u251c\u2500\u2500 quote_iex_20201106000000 \u251c\u2500\u2500 quote_iex_20201106010000 \u251c\u2500\u2500 stpmeta \u251c\u2500\u2500 trade_20201106000000 \u251c\u2500\u2500 trade_20201106010000 \u251c\u2500\u2500 trade_iex_20201106000000 \u2514\u2500\u2500 trade_iex_20201106010000 custom This mode allows the user to have more granular control over how each table is logged. The variable .stplg.customcsv points to a CSV file containing two columns, table and mode, and this allows the user to decide which logging mode to use for each table. An example CSV is below: table,mode trade,periodic trade_iex,periodic quote,tabular quote_iex,tabluar heartbeat,tabperiod Here we have the trade and trade_iex tables both being saved to the same periodic log file, the quote and quote_iex tables both having their own daily log file and the heartbeat table having a periodic log file all to itself. This mode may be advantageous in the case where some tables receive far more updates than others, so they can have more rigorously partitioned logs, and the sparser tables can be pooled together. There is some complexity associated with this mode, as there can be different log files rolling at different times. As part of these new changes it is important to note that if the STP process is restarted, the stp will open new log files in the existing log directory and will not immediately replay the previous log files. Note that each of the new logging modes includes a q object saved in the directory called stpmeta, this is a table that contains information on the stp logs present in the directory. The table stpmeta contains multiple columns including: seq, the order of the logfiles, for periodic data all files for the same time period have the same seq number logname, pathway to the logfile start, the time and date that the logfile was created end, the time and date that the logfile was closed tbls, the tables present in each logfile msgcount, the count of the messages in each logfile schema, schemas for each of the tables in the tbls column additional, any additional information about the logfile Note that both end and msgcount are informative and not guaranteed, and the STP doesn't have any dependency on them. If the STP is killed or dies in an unclean manner, then they will not be populated.","title":"Logging Modes"},{"location":"Processes/#batching-modes","text":"There are named modes which are set with the .stplg.batchmode variable and these allow the user to be flexible with process latency and throughput by altering the .u.upd and .z.ts functions: defaultbatch: This is effectively the standard TP batching mode where, upon receiving a tick, the STP immediately logs it to disk and batches the update which is published to subscribers whenever the timer function is next called. This mode represents a good balance of latency and overall throughput. immediate: In this mode no batching occurs, and the update is logged and published immediately upon entering the STP. This is less efficient in terms of overall throughput but allows for lower latency. memorybatch: In this mode, neither logging nor publishing happens immediately but everything is held in memory until the timer function is called, at which point the update is logged and published. High overall message throughput is possible with this mode, but there is a risk that some messages aren't logged in the case of STP failure. Also note that the the ordering of messages from different tables in the log file will not align with arrival order.","title":"Batching Modes"},{"location":"Processes/#starting-a-segmented-tickerplant-process","text":"Starting an STP process is similar to starting a tickerplant, we need to have an updated process.csv that contains a line for the STP process like the one below. Optional flags such as -.stplg.batchmode and -.stplg.errmode can be added to change settings for the process. localhost,${KDBBASEPORT}+103,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -.stplg.batchmode immediate -.stplg.errmode 0 -t 1,q The process can either be started using: bash torq.sh start stp1 or: q ${TORQHOME}/torq.q -proctype segmentedtickerplant -procname stp1 -load ${KDBCODE}/processes/segmentedtickerplant.q","title":"Starting a Segmented Tickerplant process"},{"location":"Processes/#subscriptions","text":"It is easy for a subscriber to subscribe to a STP process. It follows the same process as subscribing to a TP through .u.sub however some changes have been made. Each subscriber connecting to the STP needs to be updated to search for the STP instead of the original tickerplant. This is done using .servers.CONNECTIONS in the settings config file for that process, for example: .servers.CONNECTIONS:enlist `segmentedtickerplant The STP requires these functions to be defined in subscriber processes (the definitions will be unique to the requirements of each subscriber): - upd[t;x] Called for updates from the STP. Arguments are t (the table the data is for), x (the data to be inserted) - endofperiod[currentpd;nextpd;data] Called at the end of a period for periodic STP modes. Takes 3 arguments currentpd (the current period), nextpd (the next period) and data (a dictionary containing some basic information on the stp process and the current time on the stp) - endofday[date;data] Called at the end of day for all modes. Takes 2 arguments date (current date) and data (a dictionary containing some basic information on the stp process and the current time on the stp) The data dictionary contains the STP name and type, list of subscribable tables in STP and the time at which the message is sent from the STP. In order to add further information to data, simply add additional elements in the endofdaydata function defined in code/segmentedtickerplant/stplg.q script. For more information on subscriptions, see the documentation on the pubsub.q utility script.","title":"Subscriptions"},{"location":"Processes/#error-trapping","text":"If the .stplg.errmode Boolean variable is set to true, an error log is opened on start up and the .u.upd function is wrapped in an error trap. If an error is thrown by the STP when it receives and update from the feed, then the update is written to the error log. This should allow easier debugging during onboarding of new data feeds. Unless the feed is very unstable, this should not be necessary in production usage as it incurs a small overhead on each update. This mode is really designed for development/testing purposes, it shouldn't be necessary for a stable production feed and will add a small overhead to each update.","title":"Error Trapping"},{"location":"Processes/#time-zone-behaviour","text":"A key tickerplant function is to timestamp the incoming data before it gets published to the rest of the system. Similar to the existing TP, the STP allows definition of the timezone that data is timestamped at upon arrival the timezone that the STP executes it's end-of-day roll in the offset from midnight that the STP executes its end-of-day roll in This allows for more complex configurations, such as an Foreign Exchange data capture system which timestamps data in UTC but rolls at 5 PM EST. The key variable used is .eodtime.dailyadj but more information on setting up a TorQ process in a different time zone can be found here .","title":"Time Zone Behaviour"},{"location":"Processes/#per-table-customisation","text":"Each table has its own upd function, meaning that some additional processing, such as adding a sequence number or a time-zone offset, can be done in the STP itself rather than needing to be done in a separate process. This is done by altering the .stplg.updtab dictionary in the segmentedtickerplant settings config file. The default behaviour is for every update to automatically have the current timestamp applied to it. // In file $TORQHOME/appconfig/settings/segmentedtickerplant.q // Apply a sequence number to 'tabname' .stplg.updtab[`tabname]:{((count first x)#'(y;.stplg.seqnum),x} // In the STP process q) .stplg.updtab quote | {(enlist(count first x)#y),x} trade | {(enlist(count first x)#y),x} tabname | {((count first x)#'(y;.stplg.seqnum),x} ...","title":"Per Table Customisation"},{"location":"Processes/#custom-batching-modes","text":"The batching behaviour depends on two functions: .u.upd and .z.ts . The former is called every time an update arrives in the STP and the latter whenever the timer function is called (e.g. if the process is started with -t 1000 , this will be called every second). In the default batching mode, .u.upd inserts the update into a local table and writes it to the log file, and .z.ts publishes the contents of the local table before clearing it. To customise these functions, the .stplg.upd and .stplg.zts dictionaries will need to be customised. For example, the default batching code looks like the following: \\d .stplg // Standard batch mode - write to disk immediately, publish in batches upd[`defaultbatch]:{[t;x;now] t insert x:.stplg.updtab[t] . (x;now); `..loghandles[t] enlist(`upd;t;x); // track tmp counts, and add these after publish @[`.stplg.tmpmsgcount;t;+;1]; @[`.stplg.tmprowcount;t;+;count first x]; }; zts[`defaultbatch]:{ // publish and clear all tables, increment counts .stpps.pubclear[.stpps.t]; // after data has been published, updated the counts .stplg.msgcount+:.stplg.tmpmsgcount; .stplg.rowcount+:.stplg.tmprowcount; // reset temp counts .stplg.tmpmsgcount:.stplg.tmprowcount:()!(); }; \\d .","title":"Custom Batching Modes"},{"location":"Processes/#multiple-updates-from-publishers","text":"The STP handles single messages which contain multiple updates. The standard interface for publishers is the .u.upd interface supported by the original tickerplant (two arguments, the table to be updated followed by the data). The arguments for this can now but lists, i.e. a list of table names followed by a list of associated data. These updates can all be stamped with the same sequence number, and is useful where upstream publishers receive a single update which is split across downstream tables. Once this is done, simply update .stplg.batchmode with the name of the new mode and start the process.","title":"Multiple Updates From Publishers"},{"location":"Processes/#performance-comparison","text":"A custom performance stack was set up comprising a feed, a consumer, an STP, a vanilla TP (normal TorQ tickerplant) and a kdb+ tick process along with an observer process which was responsible for coordinating the tests and processing the results. When the tests begin, the feed pushes single row updates to the selected TP process in a loop for one minute before pushing updates in batches of 100 rows for one minute. The observer then collects the results from the consumer which is subscribed to the TP and clears the table before resetting things so that the feed is pointing at either the same process in a different batching mode or a new process. In this way all the process modes are tested, including the immediate and batched modes for the TP and tick processes. These tests were run on a shared host with dual Intel Xeon Gold 6128 CPUs with a total of 12 cores and 24 threads with 128GB of memory. Each tickerplant will only have been using a single core. The results below show the average number of messages per second (mps) received by the subscriber. The results for single updates can be seen below. It should be noted that the message rates achieved will be dependent on hardware configuration. The purpose of the testing below is to demonstrate the relative performance between the different implementations and batching modes. Process Batch Mode Average mps STP Default batch 103k STP Immediate 89k STP Memory batch 174k TorQ TP Immediate 75k TorQ TP Batch 98k Tick Immediate 87k Tick Batch 103k And the following are for batched updates (note that each message contains 100 ticks): Process Batching Mode Average mps STP Default batch 19k STP Immediate 18k STP Memory batch 21k TorQ TP Immediate 18k TorQ TP Batch 18k Tick Immediate 17k Tick Batch 19k The first obvious thing to be noticed is that batching the updates results in greater performance as there are fewer IPC operations and disk writes, and while some insight can be gleaned from these figures the single update results provide a better comparison of the actual process code performance. The memory batching mode is the clear leader in terms of raw performance as it does not write to disk on every update. The three 'default' batching modes are roughly equivalent in terms of performance and all have similar functionality. The three Immediate modes bring up the rear in terms of raw throughput, though the STP version is the performance leader here as it stores table column names in a dictionary which can be easily accessed rather than having to read the columns of a table in the root namespace. In this set up each message contained either one or 100 ticks, each of which had 11 fields, and there was one feed pushing to one TP which had one simple subscriber and varying these figures will impact performance. Here the size of a single tick is 141 bytes and the 100-tick update is 6137 bytes. Increasing the fields in each tick or the number of ticks in a message will results in more costly IPC and IO operations which will decrease performance. Having more subscribers and increasing the complexity of subscriptions, ie. having complex where clause conditions on the subscription, will also reduce performance. These are all things worth bearing in mind as one builds an application. When writing the code for these STP modes some compromise was taken between performance and maintenance. All the UPD functions are written in a standard way and have certain common elements abstracted away in namespaces, which does technically reduce performance. We will see later on how custom modes can be defined which can be more tailored to a given application.","title":"Performance Comparison"},{"location":"Processes/#chained-stp","text":"A chained tickerplant (CTP) is a TP that is subscribed to another TP. This is useful for the following: protecting the main TP by offloading publication load. Examples would be distributing data to multiple consumers, or executing complex subscription filtering logic. distributing a system across multiple hosts where data is pushed once across a host boundary and then further distributed within that host. The CTP can also be used to create a local version of the TP log files within that host. The CTP inherits functionality from the STP- essentially, the code can be run in \"chained\" mode. In this case it subscribes to an STP, and forwards data. Timings (e.g. end-of-day, end-of-period) are driven from the STP. A chained STP can be in a different batching mode from the STP. The main choice is around what type of logging, if any, the CTP is required to do: none: Chained STP does not create or access any log files. create: Chained STP creates its own log files independent of the STP logs. Subscribers then access the chained STP log files during replays parent: STP logs are passed to subscribers during replays. Chained STP does not create any logs itself The 'parent' logging mode is useful when all of the Torq processes have access to the same disk. In this case, the subscriber can access the logs of the STP and the data is replayed through the Chained STP. This prevents the SCTP from needing to create duplicate logs and so saves on storage. This replay would look like the following: The 'create' logging mode should be used when the chained STP is running on a separate host to the STP, as illustrated in the diagram below. Here RDB1 may access the STP logs as they are running on the same host. However, RDB2 does not have access to the STP logs and so they cannot be replayed through the SCTP. Therefore, the chained STP needs to create its own logs for RDB2 to access.","title":"Chained STP"},{"location":"Processes/#backward-compatibility","text":"Not everything about the STP is exactly the same as the TP, a couple of things have been changed: All updates must be lists of lists, meaning that single updates must be enlisted. Consumers must have endofday[currentdate;DATA], endofperiod[currentperiod;nextperiod;DATA] and upd[newdata;table] defined in order to successfully subscribe to the STP (.u.end is no longer used). Here DATA is a dictionary of metadata about the STP containing the STP name and type, subscribable tables and the time at which the message was sent from the STP. First two columns in tables do not have to be time and sym By default, every update entering the STP will be timestamped as the first column. However, this behaviour can easily be modified by altering the dictionary of updtab functions. To read more on this, visit the Per Table Customisation subsection of the STP documentation in this file.","title":"Backward Compatibility"},{"location":"Processes/#real-time-database-rdb","text":"The Real Time Database is a modified version of r.q found in kdb+tick. The modifications from the standard r.q include: Tickerplant (data source) and HDB location derived from processes defined by the discovery service or from config file; Automatic re-connection and resubscription to tickerplant; List of tables to subscribe to supplied as configuration setting; More pre-built flexibility in end-of-day; More verbose end-of-day logging; Reload multiple authenticated HDBs after end-of-day; End-of-day save down manipulation code is shared between RDB, WDB and tickerplant log replay See the top of the file for more information.","title":"Real Time Database (RDB)"},{"location":"Processes/#write-database-wdb","text":"The Write Database or WDB is based on w.q. This process features a number of modifications and enhancements over w.q: Provides the option to write down to a custom partition scheme, defined by parted columns in sort.csv, which removes the need for end of day sorting; Greater configuration options; max rows on a per table basis, list subscription tables, upd function etc. See the top of the process file for the options; Use of common code with the RDB and Tickerplant Log Replay process to manipulate tables before save, sort and apply attributes; Checks whether to persist data to disk on a timer rather than on each tick; Informs other RDB, HDB and GW processes that end of day save and sort has completed; More log information supplied; End of day timezone can be configured (see eodtime.q ). The WDB process can broken down into two main functions: Periodically saving data to disk and Sorting data at end of day The WDB process provides flexibility so it can be set-up as a stand-alone process that will both save and sort data or two separate processes (one that saves the data and another that will sort the data on disk). This allows greater flexibility around the end of day event as sorting data can be time consuming. It is also helps when implementing seemless rollovers (i.e. no outage window at end-of-day). The behaviour of the WDB process is controlled by the .wdb.mode parameter. This should be set to one of following three values: saveandsort - the process will subscribe for data, periodically write data to disk and at EOD it will flush remaining data to disk before sorting it and informing GWs, RDBs and HDBs etc. save - the process will subscribe for data, periodically write data to disk and at EOD it will flush remaining data to disk. It will then inform its respective sort mode process to sort the data sort - the process will wait to get a trigger from its respective save mode process. When this is triggered it will sort the data on disk, apply attributes and the trigger a reload on the RDB, HDB and GW processes When running a system with separate save and sort process, the sort process should be configured in the processes.csv file with a proctype of sort. The save process will check for processes with a proctype of sort when it attempts to trigger the end of day sort of the data. The wdb process provides two methods for persisting data to disk and sorting at the end of the day. default - Data is persisted into a partition defined by the [partitiontype] variable, similar to the hdb partition scheme. The general scheme is of the form [wdbdir]/[partitiontype]/[table]/. And a typical partition directory would be similar to wdb/database/2015.11.26/trades/. At the end of the day, before being moved to the hdb, the data is sorted according to parameters defined in sort.csv. For each table, sort.csv will specify the columns to sort (using xasc) and apply attributes to. partbyattr - Data is persisted to a custom partition scheme, derived from parameters in the sort.csv file. The write down scheme is taken from sort.csv, to reflect the effect of using xasc at the end of day. For each table, the columns defined in sort.csv, with the parted attribute, are used to create custom partitions in the wdb. Multiple columns can be defined with the parted attribute and distinct combinations of each are generated for custom partitions. The general partition scheme is of the form [wdbdir]/[partitiontype]/[table]/[parted column(s)]/. And a typical partition directory would be similar to wdb/database/2015.11.26/trade/MSFT_N. In the above example, the data is parted by sym and source, and so a unique partition directory MSFT_N is created in the wdb directory. At the end of the day, data is upserted into the hdb without the need for sorting. The number of rows that are joined at once is limited by the mergenumrows and mergenumtab parameters. The optional partbyattr method may provide a significant saving in time at the end of day, allowing the hdb to be accessed sooner. For large data sets with a low cardinality (ie. small number of distinct elements) the optional method may provide a significant time saving, upwards of 50%. The optional method should also reduce the memory usage at the end of day event, as joining data is generally less memory intensive than sorting.","title":"Write Database (WDB)"},{"location":"Processes/#tickerplant-log-replay","text":"The Tickerplant Log Replay script is for replaying tickerplant logs. This is useful for: handling end of day save down failures; handling large volumes of data (larger than can fit into RAM). The process takes as the main input either an individual log file to replay, or a directory containing a set of log files. Amongst other functionality, the process can: replay specific message ranges; replay in manageable message chunks; recover as many messages as possible from a log file rather than just stopping at the first bad message; ignore specific tables; modify the tables before or after they are saved; apply sorting and parting after all the data is written out. The process must have some variables set (the tickerplant log file or directory, the schema file, and the on-disk database directory to write to) or it will fail on startup. These can either be set in the config file, or overridden from the command line in the usual way. An example start line would be: q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.tplogfile ../test/tplogs/marketdata2013.12.17 -.replay.schemafile ../test/marketdata.q -.replay.hdbdir ../test/hdb1 -proctype tickerlogreplay -procname tplogreplay1 In order to replay log files from a segmented tickerplant, the directory containing those log files can be passed in and the variable .replay.segmentedmode must be true. It should be noted that a directory must be passed in and the directory must contain the STP meta table for that day. Only one log directory can be replayed at a time. The tickerplant log replay script has extended usage information which can be accessed with -.replay.usage. q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.usage -proctype tickerlogreplay -procname tplogreplay1","title":"Tickerplant Log Replay"},{"location":"Processes/#housekeeping","text":"The housekeeping process is used to undertake periodic system housekeeping and maintenance, such as compressing or removing files which are no longer required. The process will run the housekeeping jobs periodically on a timer. Amongst other functionality the process: Allows for removing and zipping of directory files; Provides an inbuilt search utility and selectively searches using a \u2018find\u2019 and \u2018exclude\u2019 string, and an \u2018older than\u2019 parameter; Reads all tasks from a single CSV; Runs on a user defined timer; Can be run immediately from command line or within the process; Can be easily extended to include new user defined housekeeping tasks. The process has two main parameters that should be set prior to use; runtimes and inputcsv.\u2018Runtimes\u2019 sets the timer to run housekeeping at the set time(s), and \u2018Inputcsv\u2019 provides the location of the housekeeping csv file. These can either be set in the config file, or overridden via the command line. If these are not set, then default parameters are used; 12.00 and \u2018KDBCONFIG/housekeeping.csv\u2019 respectively. The process is designed to run from a single csv file with seven headings: Function details the action that you wish to be carried out on the files or directories. Initially, this can be rm (remove) and zip (zipping) for files, and tardir (zipping) for directories; Path specifies the directory that the files/directories are in; Match provides the search string to the find function, files/directories returned will have names that match this string; Exclude provides a second string to the find function, and these files/directories are excluded from the match list; Age is the \u2018older than\u2019 parameter, and the function will only be carried out on files/directories older than the age given (in days); Agemin switches the age measurement from days to minutes; Checkfordirectory specifies whether to search for directories, instead of files. An example csv file would be: function,path,match,exclude,age,agemin,checkfordirectory zip,./logs/,*.log,*tick*,2,, rm,./logs/,*.log*,*tick*,4,, zip,./logs/,*tick*,,1,, rm,./logs/,*tick*,,3,, tardir,./stplogs/,database*,,1,,1 function path match exclude age agemin checkfordirectory ----------------------------------------------------------------------- zip \"./logs/\" \"*.log\" \"*tick*\" 2 0 0 rm \"./logs/\" \"*.log*\" \"*tick*\" 4 0 0 zip \"./logs/\" \"*tick*\" \"\" 1 0 0 rm \"./logs/\" \"*tick*\" \"\" 3 0 0 tardir \"./stplogs/\" \"database*\" \"\" 1 0 1 The process reads in the csv file, and passes it line by line to a \u2018find\u2019 function; providing a dictionary of values that can be used to locate the files/directories required. The find function takes advantage of system commands to search for matches according to the specifications in the dictionary. A search is performed for both the match string and the exclude string, and cross referenced to produce a list of files/directories that match the parameters given. The matches are then each passed to a further set of system commands to perform the task of either zipping or removing. Note that an incomplete csv or non-existant path will throw an error. The remove and zipping functions form only basic implementations of the housekeeping process; it is designed to be exended to include more actions than those provided. Any user function defined in the housekeeping code can be employed in the same fashion by providing the name of the function,search string and age of files to the csv. As well as being scheduled on a timer, the process can also be run immediately. Adding \u2018-hk.runnow 1\u2019 to the command line when starting the process will force immediate running of the actions in the housekeeping csv. Likewise, setting runnow to 1b in the config file will immediately run the cleaning process. Both methods will cause the process to exit upon completion. Calling hkrun[] from within the q process will also run the csv instructions immediately. This will not affect any timer scheduling and the process will remain open upon completion. Housekeeping works both on windows and unix based systems. Since the process utilizes inbuilt system commands to perform maintenances, a unix/windows switch detects the operating system of the host and applies either unix or widows functions appropriately. Extensions need only be made in the namespace of the hosting operating system (i.e. if you are using a unix system, and wish to add a new function, you do not need to add the function to the windows namespace to). Usage information can be accessed using the \u2018-hkusage\u2019 flag: q torq.q -load code/processes/housekeeping.q -p 9999 -proctype housekeeping -procname hk1 -debug -hkusage","title":"Housekeeping"},{"location":"Processes/#file-alerter","text":"The file alerter process is a long-running process which periodically scans a set of directories for user-specified files. If a matching file is found it will then carry out a user-defined function on it. The files to search for and the functions to run are read in from a csv file. Additionally, the file alerter process can: run more than one function on the specified file. optionally move the file to a new directory after running the function. store a table of files that have already been processed. run the function only on new files or run it every time the file is modified. ignore any matching files already on the system when the process starts and only run a function if a new file is added or a file is modified. The file alerter process has four parameters which should be set prior to use. These parameters can either be set in the config file or overridden on the command-line. If they are not set, the default parameters will be used. The parameters are as follows. inputcsv - The name and location of the csv file which defines the behaviour of the process. The default is KDBCONFIG/filealerter.csv. polltime - How often the process will scan for matching files. The default is 0D:00:01, i.e., every minute. alreadyprocessed - The name and location of the already-processed table. The default is KDBCONFIG/filealerterprocessed. This table will be created automatically the first time the process is ran. skipallonstart - If this is set to 1, it will ignore all files already on the system; if it is set to 0, it will not. The default value is 0. The files to find and the functions to run are read in from a csv file created by the user. This file has five columns, which are detailed below. path - This is the path to the directory that will be scanned for the file. match - This is a search string matching the name of the file to be found. Wildcards can be used in this search, for example, \u201cfile*\u201d will find all files starting with \u201cfil\u201d. function - This is the name of the function to be run on the file. This function must be defined in the script KDBCODE/processes/filealerter.q. If the function is not defined or fails to run, the process will throw an error and ignore that file from then on. newonly - This is a boolean value. If it is set to 1, it will only run the function on the file if it has been newly created. If it is set to 0, then it will run the function every time the file is modified. movetodirectory - This is the path of the directory you would like to move the file to after it has been processed. If this value is left blank, the file will not be moved. It is possible to run two separate functions on the same file by adding them as separate lines in the csv file. If the file is to be moved after it is processed, the file alerter will run both functions on the file and then attempt to move it. A typical csv file to configure the file alerter would look like: path,match,function,newonly,movetodirectory /path/to/dirA,fileA.*,copy,0,/path/to/newDir /path/to/dirB,fileB.txt,email,1, /path/to/dirA,fileA.*,delete,0,/path/to/newDir path match function newonly movetodirectory --------------------------------------------------- \"/path/to/dirA\" \"fileA.*\" copy 0 \"/path/to/newDir\" \"/path/to/dirB\" \"fileB.txt\" email 1 \"\" \"/path/to/dirA\" \"fileA.*\" delete 0 \"/path/to/newDir\" The file alerter process reads in each line of the csv file and searches files matching the search string specified in that line. Note that there may be more than one file found if a wildcard is used in the search string. If it finds any files, it will check that they are not in the already processed table. If newonly is set to 1, it only checks if the filename is already in the table. If newonly is set to 0, it checks against the filename, filesize and a md5 hash of the file. The md5 hash and the filesize are used to determine if the file has been modified since it was processed last. If the found files have not been processed already, it then attempts to run the specified function to these files. After the process has run through each line of the csv, it generates a table of all files that were processed on that run. These files are appended to the already processed table which is then saved to disk. The file alerter will attempt to move the files to the \u2018movetodirectory\u2019, if specified. If the file has already been moved during the process (for example, if the function to run on it was \u2018delete\u2019), the file alerter will not attempt to move it. The file alerter is designed to be extended by the user. Customised functions should be defined within the filealerter.q script. They should be diadic functions, i.e., they take two parameters: the path and the filename. As an example, a simple function to make a copy of a file in another directory could be: copy:{[path;file] system \"cp \", path,\"/\", file, \" /path/to/newDir\"} Although the process is designed to run at regular intervals throughout the day, it can be called manually by invoking the FArun[] command from within the q session. Similarly, if new lines are added to the csv file, then it can be re-loaded by calling the loadcsv[] command from the q session. Each stage of the process, along with any errors which may occur, are appropriately logged in the usual manner. The file alerter process is designed to work on both Windows and Unix based systems. Since many of the functions defined will use inbuilt system command they will be need to written to suit the operating system in use. It should also be noted that Windows does not have an inbuilt md5 hashing function so the file alerter will only detect different versions of files if the filename or filesize changes.","title":"File Alerter"},{"location":"Processes/#reporter","text":"","title":"Reporter"},{"location":"Processes/#overview_1","text":"The reporter process is used to run periodic reports on specific processes. A report is the result of a query that is run on a process at a specific time. The result of the query is then handled by one of the inbuilt result handlers, with the ability to add custom result handlers. Features: Easily create a report for information that you want; Fully customizable scheduling such as start time, end time and days of the week; Run reports repeatedly with a custom period between them; Asynchronous querying with custom timeout intervals; Inbuilt result handlers allow reports to be written to file or published; Custom result handlers can be defined; Logs each step of the report process; Fully integrated with the TorQ gateway to allow reports to be run across backend processes. The reporter process has three parameters that are read in on initialisation from the reporter.q file found in the $KDBCONFIG/settings directory. These settings are the string filepath of the input csv file, a boolean to output log messages and timestamp for flushing the query log table. To run the reporter process: q torq.q -load code/processes/reporter.q -p 20004 -proctype reporter -procname reporter1 Once the reporter process has been initiated, the reports will be scheduled and no further input is required from the user.","title":"Overview"},{"location":"Processes/#report-configuration","text":"By default, the process takes its inputs from a file called reporter.csv which is found in the $KDBCONFIG directory. This allows the user complete control over the configuration of the reports. As the queries are evaluated on the target process, local variables can be referenced or foreign functions can be run. Table [table:reportertable] shows the meaning of the csv schema. Column Header Description and Example name Report name e.g. Usage query Query to be evaluated on that process. It can be a string query or function resulthandler Result handlers are run on the returned result. Custom result handlers can be added. The result handler must be a monadic function with the result data being passed in e.g. writetofile[\u201c./output\u201d;\u201cusage\u201d] gateway If non null the reporter will query processes route the query to the proctype specified in this field. The values in the proctype field will be the process types on which the gateway runs the backend query. e.g. `gateway joinfunction Used to join the results when a gateway query is being used. The choice of joinfunction must take into account the result that will be received. The function must be monadic and the parameter will be the list of results returned from the backend processes e.g. raze proctype The type of process that the report will be run on. If the gateway field is not empty this may be a list of process types, otherwise the reporter will throw an error on startup. e.g. `rdb procname The name of a specific process to run the report on. If left null, the reporter process will select a random process with the specified proctype. If the gateway field is not null, this field specifies the specific gateway process name to run the query against e.g. `hdb1 start Time on that day to start at e.g. 12:00 end Time on that day that the report will stop at e.g. 23:00 period The period between each report query e.g. 00:00:10 timeoutinterval The amount of time the reporter waits before timing out a report e.g. 00:00:30 daysofweek Numeric value required for the day of the week. Where 0 is Saturday and 2 is Monday When running a report on a gateway, the gateway field must be set to the proctype of the gateway that will be queried. It will then run the report on the processes which are listed in the proctype field and join the results by using the function specified in the joinfunction field. If there is no join function then the reporter process will not start. Multiple entries in the proctype field must be separated by a space and are only allowed when the gateway field is not empty. If gateway field is empty and there are multiple entries in the proctype field then the reporter process will not load. Listing [code:csvschema] shows an example of the schema needed in the input csv file. name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek usage|10#.usage.usage|writetofiletype[\"./output/\";\"usage\";\"csv\"]|||rdb||00:01|23:50|00:01|00:00:01|0 1 2 3 4 5 6 memory|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|||rdb|rdb1|00:05|18:00|00:01|00:00:08|0 1 2 3 4 5 6 usage_gateway|10#.usage.usage||gateway|raze|rdb hdb||00:02|22:00|00:01|00:00:10|0 1 2 3 4 5 6","title":"Report Configuration"},{"location":"Processes/#result-handlers","text":"There are several default result handlers which are listed below. Custom result handlers can be defined as required. The result handler will be invoked with a single parameter (the result of the query). writetofiletype - Accepts 3 parameters: path, filename, filetype and data. When writing to file it uses a date time suffix so the resultant filename will be usage_rdb_2014_01_02_15_00_12.txt e.g. writetofiletype[\"./output/\";\"usage\";\"csv\"] splaytable - This accepts 3 parameters: path, file and data. This splays the result to a directory. The result must be a table in order to use this function e.g. splaytable[\"./output/\";\"usage\"] emailalert - This accepts 3 parameters: period, recipient list and data. The period dictates the throttle i.e. emails will be sent at most every period. The result of the report must be a table with a single column called messages which contains the character list of the email message. This is used with the monitoring checks to raise alerts, but can be used with other functions. emailalert[0D00:30;(\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\")] emailreport - This accepts 3 parameters: temporary path, recipient list, file name, file type and data. The data is written out as the file type (e.g. csv, xml, txt, xls, json) with the given file name to the temporary path. It is then emailed to the recipient list, and the temporary file removed. emailreport[\"./tempdir/\"; (\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\"); \"EndOfDayReport\"; \"csv\"] publishresult - Accepts 1 parameter and that is the data. This is discussed later in the subsection subresults. Custom result handlers can be added to $KDBCODE/processes/reporter.q . It is important to note that the result handler is referencing local functions as it is executed in the reporter process and not the target process. When the query has been successful the result handler will be passed a dictionary with the following keys: queryid, time, name, procname, proctype and result.","title":"Result Handlers"},{"location":"Processes/#report-process-tracking","text":"Each step of the query is logged by the reporter process. Each query is given a unique id and regular system messages are given the id 0. The stage column specifies what stage the query is in and these are shown in table [table:stagetable]. An appropriate log message is also shown so any problems can easily be diagnosed. The in memory table is flushed every interval depending on the value of the flushqueryloginterval variable in the reporter.q file found in the $KDBCONFIG/settings directory. Stage symbol Explanation R The query is currently running E An error has occurred during the query C The query has been completed with no errors T The query has exceeded the timeout interval S System message e.g. \u201cReporter Process Initialised\u201d time | queryid stage message -----------------------------| ------------------------------------------------------------------------ 2014.10.20D22:20:06.597035000| 37 R \"Received result\" 2014.10.20D22:20:06.600692000| 37 R \"Running resulthandler\" 2014.10.20D22:20:06.604455000| 37 C \"Finished report\" 2014.10.20D22:30:00.984572000| 38 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\" 2014.10.20D22:30:00.991862000| 38 R \"Received result\" 2014.10.20D22:30:00.995527000| 38 R \"Running resulthandler\" 2014.10.20D22:30:00.999236000| 38 C \"Finished report\" 2014.10.20D22:30:06.784419000| 39 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\" 2014.10.20D22:30:06.796431000| 39 R \"Received result\"","title":"Report Process Tracking"},{"location":"Processes/#subscribing-for-results","text":"To publish the results of the report, the reporter process uses the pub sub functionality of TorQ. This is done by using the using the inbuilt result handler called publishresult. In order to subscribe to this feed, connect to the reporter process and send the function shown below over the handle. To subscribe to all reports use a backtick as the second parameter and to subscribe to a specific reports results include the reporter name as a symbol. /- define a upd function upd:insert /- handle to reporter process h: hopen 20004 /- Subscribe to all results that use the publishresult handler h(`.ps.subscribe;`reporterprocessresults;`) /- Subscribe to a specific report called testreport h(`.ps.subscribe;`reporterprocessresults;`testreport)","title":"Subscribing for Results"},{"location":"Processes/#example-reports","text":"The following are examples of reports that could be used in the reporter process. The rdbtablecount report will run hourly and return the count of all the tables in a rdb process. The memoryusage report will run every 10 minutes against the gateway for multiple processes and will return the .Q.w[] information. Both of these reports run between 9:30am to 4:00pm during the weekdays. The report onetimequery is an example of a query that is run one time, in order to run a query once, the period must be the same as the difference between the start and end time. name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek rdbtablecount|ts!count each value each ts:tables[]|{show x`result}|||rdb|rdb1|09:30|16:00|01:00|00:00:10|2 3 4 5 6 memoryusage|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|gateway1|{enlist raze x}|rdb hdb||09:30|16:00|00:10|00:00:10|2 3 4 5 6 onetimequery|10#.usage.usage|writetofile[\"./output/\";\"onetime.csv\"]|||rdb||10:00|10:01|00:01|00:00:10|2 3 4 5 6","title":"Example reports"},{"location":"Processes/#monitor","text":"The Monitor process is a simple process to monitor the health of the other processes in the system. It connects to each process that it finds (by default using the discovery service, though can use the static file as well) and subscribes to both heartbeats and log messages. It maintains a keyed table of heartbeats, and a table of all log messages received. Run it with: aquaq $ q torq.q -load code/processes/monitor.q -p 20001 -proctype monitor -procname monitor1 It is probably advisable to run the monitor process with the -trap flag, as there may be some start up errors if the processes it is connecting to do not have the necessary heartbeating or publish/subscribe code loaded. aquaq $ q torq.q -load code/processes/monitor.q -p 20001 -trap -proctype monitor -procname monitor1 The current heartbeat statuses are tracked in .hb.hb, and the log messages in logmsg q)show .hb.hb sym procname | time counter warning error ----------------------| --------------------------------------------------- discovery discovery2 | 2014.01.07D13:24:31.848257000 893 0 0 hdb hdb1 | 2014.01.07D13:24:31.866459000 955 0 0 rdb rdb_europe_1| 2014.01.07D13:23:31.507203000 901 1 0 rdb rdb1 | 2014.01.07D13:24:31.848259000 34 0 0 q)show select from logmsg where loglevel=`ERR time sym host loglevel id message ------------------------------------------------------------------------------------- 2014.01.07D12:25:17.457535000 hdb1 aquaq ERR reload \"failed to reload database\" 2014.01.07D13:29:28.784333000 rdb1 aquaq ERR eodsave \"failed to save tables : trade, quote\"","title":"Monitor"},{"location":"Processes/#checkmonitor","text":"The checkmonitor.q script extends the functionality of the monitor process. The script takes a set of user defined configuration settings for a set of process specific checks. These can initially be provided in the form of a CSV, a sample of which is shown here: family|metric|process|query|resultchecker|params|period|runtime datacount|tradecount|rdb1|{count trade}|checkcount|`varname`count`cond!(`trade;10;`morethan)|0D00:01|0D00:00:01 Upon start up, the CSV file is loaded and inserted into the in-memory table, checkconfig . During this insertion, each check will also be assigned a unique checkid number. q)checkconfig checkid| family metric process query resultchecker params period runtime active -------| ----------------------------------------------------------------------------------------------------------------------------------------------------- 1 | datacount tradecount rdb1 \"{count trade}\" \"checkcount\" `varname`count`cond!(`trade;10;`morethan) 0D00:01:00.000000000 0D00:00:01.000000000 1 For each check, the query will be sent via asynchronous requests to the specified processes and waits for postback of the results. Once the monitoring process receives the result of the query, it will then be checked by the resultchecker function to identify whether it will pass or fail. Result checker functions must only take two parameters: p- a parameter dictionary, and r- the result row. The status in r will be modified based on whether the r result value passes the conditions specified by the resultchecker function. q)checkcount {[p;r] if[`morethan=p`cond; if[p[`count]<r`result; :`status`result!(1h;\"\")]; :`status`result!(0h;\"variable \",(string p`varname),\" has count of \",(string r`result),\" but requires \",string p`count)]; if[`lessthan=p`cond; if[p[`count]>r`result; :`status`result!(1h;\"\")]; :`status`result!(0h;\"variable \",(string p`varname),\" has count of \",(string r`result),\" but should be less than \",string p`count)]; } q)p varname| `trade count | 10 cond | `morethan q)r status| 1h result| \"\" This example checks whether the trade table within the rdb is larger than 10. As this is true, the status has been set to 1h and no error message has been returned. This information is inserted into the checkstatus table, which is the primary table where all results are stored. q)checkstatus checkid| family metric process lastrun nextrun status executiontime totaltime timerstatus running result -------| -------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | datacount tradecount rdb1 2019.02.18D10:58:45.908919000 2019.02.18D10:59:45.911635000 1 0D00:00:00.000017000 0D00:00:00.002677000 1 0 \"\" In addition to tracking the status of the specified queries, a number of metrics are also returned in the checkstatus table. Column Header Value Type Description lastrun Timestamp Last time check was run nextrun Timestamp Next time check is scheduled to run status Boolean Indicates whether query result has passed resultchecker function executiontime Timespan Time taken for last query to be executed totaltime Timespan Total time taken for check to be run, including sending times and execution timerstatus Boolean Indicates whether last totaltime was executed under threshold value running Boolean Indicates whether check is currently running results String Will display any errors or additional information regarding running checks The function checkruntime uses the running column to identify functions that are running extremely slow, and set their status and timerstatus to 0h. When the process is exited, the .z.exit has been modified to save the checkconfig table as a flat binary file. This will then be preferentially loaded next time the process is started up again. The process of saving down the in-memory functions makes altering configuration parameters easier. Four functions are available to do so: addcheck , updateconfig , updateconfigfammet and forceconfig . Function Name Description addcheck[dictionary] addcheck allows a new check to be added, and accepts a dictionary as its argument. The keys must be a match to the current checkconfig table, and the values must be of the correct type. updateconfig[checkid;paramkey;newval] updateconfig changes the parameter key of an existing check, using the checkid to specify which check to alter. The type of the new parameter value must match the current value type. forceconfig[checkid;newconfig] forceconfig changes the parameter keys of an existing check and will not check for types. updateconfigfammet[family;metric;paramkey;newval] updateconfig changes the parameter key of an existing check, using the family and metric combination to specify which check to alter. If this combination does not exist, the function will return an error. The type of the new parameter value must match the current value type. There are other additional functions that are useful for using the check monitor. Function Name Value Type currentstatus Will return only status, timerstatus, result and running from the checktracker table. It accepts a list of checkids, or will return all checks if passed a null. timecheck Will check the median time for current checks to be run against a user-defined timespan. It returns a table displaying the median time and a boolean value. statusbyfam Function will return a table of all checks from specified families, ordered firstly by status, and then by timestatus. If a null is provided, ordered checks from all families will be returned. All checks can be tracked using the table checktracker . Here, each run is assigned a unique runid- thus individual runs for each check can be tracked. For each run, it tracks the time tkane for target process to recieve the query, as well as the execution time. The result value will also be displayed.","title":"Checkmonitor"},{"location":"Processes/#html5-front-end","text":"A HTML5 front end has been built to display important process information that is sent from the monitor process. It uses HTML5, WebSockets and JavaScript on the front end and interacts with the monitor process in the kdb+ side. The features of the front end include: Heartbeat table with processes that have warnings highlighted in orange and errors in red Log message table displaying the last 30 errors Log message error chart that is by default displayed in 5 minute bins Chart\u2019s bin value can be changed on the fly Responsive design so works on all main devices i.e. phones, tablets and desktop It is accessible by going to the url http://HOST:PORT/.non?monitorui","title":"HTML5 front end"},{"location":"Processes/#compression","text":"The compression process is a thin wrapper around the compression utility library. It allows periodic compression of whole or parts of databases (e.g. data is written out uncompressed and then compressed after a certain period of time). It uses four variables defined in KDBCONFIG/settings/compression.q which specify the compression configuration file to use the database directory to compress the maximum age of data to attempt to compress whether the process should exit upon completion The process is run like other TorQ processes: q torq.q -load code/processes/compression.q -p 20005 -proctype compression -procname compression1 Modify the settings file or override variables from the command line as appropriate.","title":"Compression"},{"location":"Processes/#kill","text":"The kill process is used to connect to and terminate currently running processes. It kills the process by sending the exit command therefore the kill process must have appropriate permissions to send the command, and it must be able to create a connection (i.e. it will not be able to kill a blocked process in the same way that the unix command kill -9 would). By default, the kill process will connect to the discovery service(s), and kill the processes of the specified types. The kill process can be modified to not use the discovery service and instead use the process.csv file via the configuration in the standard way. If run without any command line parameters, kill.q will try to kill each process it finds with type defined by its .servers.CONNECTIONS variable. q torq.q -load code/processes/kill.q -p 20000 -proctype kill -procname killtick .servers.CONNECTIONS can optionally be overridden from the command line (as can any other process variable): q torq.q -load code/processes/kill.q -p 20000 -.servers.CONNECTIONS rdb tickerplant -proctype kill -procname killtick The kill process can also be used to kill only specific named processes within the process types: q torq.q -load code/processes/kill.q -p 20000 -killnames hdb1 hdb2 -proctype kill -procname killtick","title":"Kill"},{"location":"Processes/#chained-tickerplant","text":"In tick+ architecture the main tickerplant is the most important component, as it is relied upon by all the real time subscribers. When the tickerplant goes down data will be lost, compare this to an rdb which can be recovered after it fails. The chained tickerplant process is an additional tickerplant that is a real time subscriber to the main tickerplant but replicates its behaviour. It will have its own real time subscribers and can be recovered when it fails. This is the recommended approach when users want to perform their own custom real time analysis. The chained tickerplant can: subscribe to specific tables and syms batch publish at an interval or publish tick by tick create a tickerplant log that its real time subscribers can replay replay the source tickerplant log To launch the chained tickerplant q torq.q -load code/processes/chainedtp.q -p 12009 -proctype chainedtp -procname chainedtp1 Chained tickerplant settings are found in config/settings/chainedtp.q and are under the .ctp namespace. Setting Explanation Default tickerplantname list of tickerplant names to try and make a connection to `tickerplant1 pubinterval publish batch updates at this interval. If the value is 0D00:00:00 then it will publish tick by tick 0D00:00:00 tpconnsleep number of seconds between attempts to connect to the source tickerplant 10 createlogfile create a log file 0b logdir directory containing chained tickerplant logs `:hdb subscribeto subscribe to these tables only (null for all) ` subscribesyms subscribe to these syms only (null for all) ` replay replay the tickerplant log file 0b schema retrieve schema from tickerplant 1b clearlogonsubscription clear log on subscription, only called if createlogfile is also enabled 0b","title":"Chained Tickerplant"},{"location":"Processes/#torq-data-quality-system-architecture","text":"Whilst the Monitor process checks the health of other processes in the system, it does not check the quality of the data captured. An RDB process could be running, but capturing and populating its tables with null values, or not running at all. These errors would not be caught by the Monitor process, so the Data Quality System has been developed with the purpose of capturing such errors. The system periodically runs a set of user-specified checks on selected processes to ensure data quality. For example, you can place checks on a table to periodically check that the percentage of nulls it contains in a certain column is below a given threshold, or check that the values of a column stay within a specified range that changes throughout the day. The system behaves based on Data Quality Config files. The metrics from the config files and the results of the checks performed on databases in the data capturing system are saved to Data Quality Databases. The results can then be used for monitoring tools to alert users. The Data Quality System consists of four processes: the Data Quality Checker(DQC) and the Data Quality Engine(DQE), as well as a Database process for each (DQCDB and DQEDB). These processes are explained in detail below.","title":"TorQ Data Quality System Architecture"},{"location":"Processes/#data-quality-checker-dqc","text":"The Data Quality Checker process runs checks on other TorQ processes to check the quality of data in the system. The specific parameters of the checks that are being run can be configured in config/dqcconfig.csv . Configuration from dqcconfig.csv is then loaded to configtable in the dqe namespace, and the checks are then run based on the parameters. The results of the checks are saved to the results table in the dqe namespace. The results table that contains all check results will periodically be saved to the DQCDB intraday. The configuration of the checks will also be periodically saved to the DQCDB throughout the day. Example of configtable is shown below: action params proc mode starttime endtime period checkid -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- tableticking \"`comp`vars!(0b;(`quote;5;`minute))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:05:00.000000000 0 chkslowsub \"`comp`vars!(0b;1000000)\" \"`tickerplant\" repeat 2020.03.20D09:00:00.000000000 2020.03.20D19:00:00.000000000 0D00:10:00.000000000 1 tableticking \"`comp`vars!(0b;(`quote;5;`minute))\" \"`rdb1\" single 2020.03.20D12:00:00.000000000 2 constructcheck \"`comp`vars!(0b;(`quote;`table))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:02:00.000000000 3 constructcheck \"`comp`compallow`compproc`vars!(1b;0;`hdb1;(`date;`variable))\" \"`hdb2\" repeat 2020.03.20D09:00:00.000000000 0D00:02:00.000000000 4 attrcheck \"`comp`vars!(0b;(`quote;`s`g;`time`sym))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:02:00.000000000 5 schemacheck \"`comp`vars!(0b;(`quote;`time`sym`bid`ask`bsize`asize`mode`ex`src;\\\"psffjjccs\\\";`````````;`s`g```````))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:15:00.000000000 6 freeform \"`comp`vars!(0b;\\\"select from trade where date=2020.01.02\\\")\" \"`hdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:03:00.000000000 7 symfilecheck \"(`comp`vars!(0b;(.dqe.hdbdir;`sym)))\" \"`hdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:10:00.000000000 8 xmarketalert \"`comp`vars!(0b;(`quote))\" \"`rdb1\" repeat 2020.03.20D09:00:00.000000000 0D00:05:00.000000000 9 action - The check function that you would like to perform. Specific checks and usages can be found in the directory code/dqc . params - The parameters that are used by the function. The Checker accepts params as a dictionary, with comp and vars being the two keys when function is only used on one process, and comp , compallow , compproc , and vars being the four keys when function is used to compare two processes. comp should be assigned a boolean value - when it is 1b , comparison is ON and the function is then used to compare two processes. compallow would then be the percentage allowed for for the difference between the results that are returned from the two processes. compproc would be the process that you are comparing proc with. vars would be the variable used for the function provided. When it is 0b , comparison is OFF and the function is only used in one process. The var key should have a value of all the variables that are used by the function. Details of the variables used by checker functions can be found in config/dqedetail.csv . proc - The process(es) that you would want the check function to be used for - in symbol datatype. mode - Mode could be set to either repeat or single . When it is set to repeat the check will be periodically run based on the period parameter, which sets the time it will take before running the same check again. When it is set to single , the check will only be ran once. starttime - The start time of the check function - in timespan datatype. endtime - When mode is repeat , end time specifies when the check will run for the last time for the day. When mode is single , endtime should be set to 0N . Should be in timespan datatype. period - When mode is repeat , period represents the time it takes for the next run to start. When mode is single , period should be set to 0N . Should be in timespan datatype. An example of .dqe.results is shown below: id funct params procs procschk starttime endtime result descp chkstatus chkruntype --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2 .dqc.tableticking quote,5,minute rdb rdb1 2020.03.19D20:04:59.777402000 2020.03.19D20:04:59.784107000 1 \"there are 44 records\" complete scheduled 0 .dqc.tableticking quote,5,minute rdb rdb1 2020.03.19D20:05:00.178288000 2020.03.19D20:05:00.186770000 1 \"there are 82 records\" complete scheduled 9 .dqc.xmarketalert quote rdb rdb1 2020.03.19D20:05:00.179664000 2020.03.19D20:05:00.193734000 1 \"bid has not exceeded the ask in market data\" complete scheduled 3 .dqc.constructcheck quote,table rdb rdb1 2020.03.19D20:06:00.178885000 2020.03.19D20:06:00.196380000 1 \"quote table exists\" complete scheduled 4 .dqc.constructcheck date,variable,comp(hdb1,0) hdb hdb2 2020.03.19D20:06:00.180247000 2020.03.19D20:06:00.203920000 1 \"hdb1 | match hdb2\" complete scheduled 5 .dqc.attrcheck quote,s,g,time,sym rdb rdb1 2020.03.19D20:06:00.182379000 2020.03.19D20:06:00.201300000 1 \"attribute of time,sym matched expectation\" complete scheduled 7 .dqc.freeform select from trade where date=2020.01.02 hdb hdb1 2020.03.19D20:06:00.184577000 2020.03.19D20:06:00.205209000 1 \"select from trade where date=2020.01.02 passed\" complete scheduled 3 .dqc.constructcheck quote,table rdb rdb1 2020.03.19D20:08:00.378141000 2020.03.19D20:08:00.398781000 1 \"quote table exists\" complete scheduled 4 .dqc.constructcheck date,variable,comp(hdb1,0) hdb hdb2 2020.03.19D20:08:00.379574000 2020.03.19D20:08:00.404501000 1 \"hdb1 | match hdb2\" complete scheduled The columns of results table provide information about the checks performed: id - The id of the check after being loaded into the process. In the sample table above, .dqc.construckcheck with the id 3 that checks whether quote table exists and .dqc.construckcheck with the id 4 that checks whether data in hdb1 matches hdb2 had two results showing as they have been ran twice within that time period, but the parameters of the checks have not been changed. The id column is also useful for manually running checks. funct - The check function that was performed. params - The variables that were input while the check function was performed. procs - The process(es) type that the function was performed on. procschk - The specific process(es) that the function was performed on. starttime - When the function was started. endtime - When the function stopped running. result - Returns a boolean value. If the result is 1 , then the function was ran successfully, and no data anomaly was found. If the result is 0 , then the function may not have been run successfully, or the data quality may be corrupted. descp - A string description describing the result of the check function. chkstatus - Could display either complete or failed . When the check function runs successfully, whether the result column is 0 or 1, chkstatus would be complete . However, if there was an error that caused the check to not run successfully(Ex: variables being a wrong type), failed would be displayed instead. chkruntype - Could display either scheduled or manual , meaning either the check was ran as scheduled from the configtable, or it was forced to run manually from the console. Below, we list all the built-in checks that we offer as part of the Data Quality Checker. .dqc.constructcheck - Checks if a construct exists. .dqc.tableticking - Checks if a table has obtained records within a specified time period. .dqc.chkslowsub - Checks queues on handles to make sure there are no slow subscribers. .dqc.tablecount - Checks a table count against a number. This can be a > , < or = relationship. .dqc.tablehasrecords - A projection of .dqc.tablecount that is used to check if a table is non-empty. .dqc.attrcheck - Checks that a tables actual schema matches the expectation. .dqc.infinitychk - Checks the percentage of infinities in certain columns of a table are below a given threshold. .dqc.freeform - Checks if a query has passed correctly. .dqc.schemacheck - Checks if the meta of a table matches expectation. .dqc.datechk - Checks the date vector contains the latest date in a HDB. .dqc.nullchk - Checks percentages of nulls in columns of a table are below a given threshold. .dqc.symfilecheck - Checks that the sym file exists. .dqc.xmarketalert - Tests whether the bid price has exceeded in the ask price in market data. .dqc.dfilechk - Checks the .d file in the latest date partition matches the previous date values. .dqc.rangechk - Checks whether the values of columns of a table are within a given range. .dqc.tablecomp - Counts the number of rows in a table, used for comparison between two processes .dqc.pctAvgDailyChange - Checks if a function applied to a table is within the threshold limits of an n-day average. .dqc.symfilegrowth - Checks if today's sym file count has grown more than a certain percentage .dqc.timdiff - Checks if idfferences between time columns are over a certain limit .dqc.memoryusage - Checks percentage of memory usage compared to max memory .dqc.refdatacheck - Checks whether the referenced column of a table is in another column of another table New Custom Check To add custom checks, create a new q file in /code/dqc. The new q script should be under the namespace dqc, and should contain the function. The function should return a mixed list with two atoms: first atom being boolean, and second being string. The first atom would contain whether the check was successful (1b) or was there an error/failed check(0b). This will be shown in the result column of the results table. The second atom would contain the description of the result of the check. Below is a sample of a qscript named customcheck.q, with the function customcheck, written in pseudo-code for reference: \\d .dqc customcheck:{[variable1;variable2] $[variable1~variable2; (1b;\"Description if the function ran successfully\"); (0b;\"Description if the function failed to run\")] } To use the function to run check, proceed to /appconfig/dqcconfig.csv, and modify how you would want the check to be ran. As an example, to run the customcheck above with the following settings - params - Not comparing two processes, and running with the two variables being abc and def . proc - running on the processes rdb1 and rdb2 . mode - the function being run repeatedly. starttime - 9AM. endtime - not specified. period - the check to be run every 10 minutes. The line in dqcconfig.csv should be: action,params,proc,mode,starttime,endtime,period customcheck,`comp`vars!(0b;(`abc;`def)),`rdb1;`rdb2,repeat,09:00:00.000000000,0Wn,0D00:10:00 To add a check that compares two processes, the check function would have to return a mixed listed with three atoms. The first two being the same as above, and the third being a numeric value. The numeric value would be what is compared between the two processes. Below is a sample of a qscript named customcheck.q, with the function customcheck, written in pseudo-code for reference to compare two processes: \\d .dqc customcheck:{[variable1;variable2] $[variable1~variable2; (1b;\"Description if the function ran successfully\";count variable1); (0b;\"Description if the function failed to run\";avg variable2)] } Secondly, the line in dqcconfig.csv should be modified. As mentioned above, the params parameter would now have to be changed, and two additional dictionary keys would have to be added. As an example, if we were to run the function customcheck comparing hdb1 and hdb2 , with there being no difference allowed between the two results returned from the two processes, the line would be the following: action,params,proc,mode,starttime,endtime,period customcheck,`comp`compallow`compproc`vars!(1b;0;`hdb1;(`abc;`def)),`hdb2,repeat,09:00:00.000000000,0Wn,0D00:10:00 If we were to allow for 50% difference between the results returned from the two processes using the function customcheck, the line would be the following: action,params,proc,mode,starttime,endtime,period customcheck,`comp`compallow`compproc`vars!(1b;50;`hdb1;(`abc;`def)),`hdb2,repeat,09:00:00.000000000,0Wn,0D00:10:00","title":"Data Quality Checker (DQC)"},{"location":"Processes/#data-quality-engine-dqe","text":"The DQE process stores daily statistics of other TorQ processes. It is a separate process from the DQC because the DQE does not run checks. The DQE and the DQC could be used to track the percentage change of records in a table from day to day. The Checker can then use the data saved the DQEDB to perform advanced checks.The behaviour of the Engine is based on the config file stored in config/dqengineconfig.csv . The config csv file is shown below: query,params,proc,querytype,starttime ------------------------------------- tablecount,.z.d-1,`hdb1,table,09:00:00.000000000 symfilecheck,`sym,`hdb1,other,09:00:00.000000000 symcount,(`quote;`sym),`hdb1,table,09:00:00.000000000 query - The query function that is used to provide daily statistics of a process. params - the variables that is used for the query function. proc - The process that the query function is running on. querytype - Whether the query is ran for a table or other . starttime - What time should the query start. The daily statistics of other TorQ processes are saved to the resultstab and the advancedres tables in dqe namespace, which would be saved to Data Quality Engine Database (DQEDB). The resultstab table contains the results of simple qeuries that return intger values that is reflected by the column resvalue . The advancedres table contains the results of advanced queries that return whole tables, which is reflected by the column \"resultdata\". Example of resultstab is shown below: date procs funct table column resvalue ------------------------------------------------------- 2020.03.16 hdb1 tablecount quote 0 2020.03.16 hdb1 tablecount quote_iex 0 2020.03.16 hdb1 tablecount trade 0 2020.03.16 hdb1 tablecount trade_iex 0 2020.03.16 hdb1 symfilecheck 10 2020.03.16 hdb1 symcount quote sym 10 date - Date that the data was saved. procs - The process that the query function was running on. funct - The query function that was used. table - Table that the function ran on. If the query was not performed on a table, the section is left blank. column - Column of the table that the function ran on. If the query did not specify the column, the section is left blank. resvalue - The value returned from the function that was ran. Example of advancedres is shown below: date procs funct table resultkeys resultdata --------------------------------------------------------- 2020.07.22 hdb1 bycount quote sym (+(,`sym)!,`AAPL`AIG`AMD`DELL`DOW`GOOG`HPQ`IBM`INTC`MSFT)!+(,`i)!,209356 208544 209003 208687 208420 209706 208319 207438 207455 209588 date - Date that the data was saved. procs - The process that the query function was running on. funct - The query function that was used. table - Table that the function ran on. If the query was not performed on a table, the section is left blank. resultkeys - Keys or variables used to generate the result table, or the table in resultdata. resultdata - The table retrned from the function that was ran. New Custom Queries To add custom queries, create a new q file in /code/dqe. The new q script should be under the namespace dqe, and should contain the function. The function should return a dictionary, With the key being the function name, and the value being the statistic that the query should return. The value from the dictionary will be shown as the resvalue in the resultstab table. Below is a sample of a qscript named customquery.q, with the function customquery, written in pseudo-code for reference: \\d .dqe customquery:{[variable1;variable2] (enlist variable1)!enlist count variable2 } To use the function to run check, proceed to /appconfig/dqengineconfig.csv, and modify how you would want the check to be ran. As an example, to run the customquery above with the following settings - params - Variable being abc . proc - running on the process hdb1 . querytype - other as it is not a table starttime - 9AM The line in the config csv should be: query,params,proc,querytype,starttime customquery,`table`quote,`hdb1,other,09:00:00.000000000","title":"Data Quality Engine (DQE)"},{"location":"analyticslib/","text":"Analytics Library A set of analytical functions is provided for the simplification of common operations. This set of tools is targeted at new kdb+ developers or business users. Data visualization would be an example application of this library, as an effective visualization of data incurs various manipulations of data sets. For example, a user wishes to produce a 15 minute bucketed sample of a time series, and forward fill it, then pivot it. All of the above can be achieved with the functions defined below. General Usage These set of function are defined in the .al namespace. All of the functions below have an input parameter in the form of a dictionary. The specifics of each dictionary are detailed in the usage section of each function. This set of utilities also includes a general set of error traps which were designed to give informative errors specific to each function. For example if a dictionary has not been provided then the error would be: q).al.pivot[123] 'Input parameter must be a dictionary with keys: -table -by -piv -var These error traps and messages are to guide the user in the correct use of the utilities and are designed to be as informative as possible. ffill Usage This script contains the utility to dynamically forward fill a given table keyed by given columns. Input is a dictionary containing: table: Table to be forward filled by: List of column names to key the table (optional) keycols: List of columns names to be forward filled (optional) OR Table This utility is equivalent to: update fills col1, fills col2 ... by col2 from table If you have a large data set or just a table with multiple columns typing this statement out can be quite laborious. With this utility you simply specify the table or parameter dictionary and pass it to the function. This utility also has the added functionality of being able to forward fill mixed list columns, i.e. strings. Examples Example 1.1 By specifying the by condition in the input dictionary the function can forward fill keyed by specific column, for example: q)table time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 12 4 00:38:39.521 AAPL 121 3 5 a 3 00:40:41.670 MSFT 63 3 4 20 c 4 00:48:08.048 MSFT 63 40 3 00:48:39.290 IBM 63 3 12 40 d 2 00:57:47.067 AAPL 24 3 30 2 01:08:00.945 AAPL 121 12 3 20 b 3 We can create the input dictionary in the following way to specify the by clause of the ffill utility: q)args:(`table`by)!(table;`sym) Passing this to the function we can see the result: q).al.ffill args time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 32 12 30 b 4 00:38:39.521 AAPL 121 3 5 30 a 3 00:40:41.670 MSFT 63 3 4 20 c 4 00:48:08.048 MSFT 63 3 4 40 c 3 00:48:39.290 IBM 63 3 12 40 d 2 00:57:47.067 AAPL 121 24 3 30 a 2 01:08:00.945 AAPL 121 12 3 20 b 3 Example 1.2 By specifying the keycols condition in the input dictionary the function can forward fill only specific columns, for example: Using the data set below we can create an input specifying which column we want to forward fill: q)table time sym src price size mode ------------------------------------------------------ 2018.02.13D08:02:09.322000000 AAPL N 32 513 \"A\" 2018.02.13D08:03:23.511000000 AAPL N 32 344 \"\" 2018.02.13D08:06:35.424000000 AAPL N 32 1933 \"B\" 2018.02.13D08:13:03.067000000 AAPL N 76 1009 \"B\" 2018.02.13D08:15:09.130000000 AAPL O 43 5199 \"B\" 2018.02.13D08:22:21.528000000 AAPL N 76 427 \"A\" 2018.02.13D08:23:46.489000000 AAPL N 7918 \"B\" 2018.02.13D08:26:34.645000000 AAPL N 43 420 \"\" 2018.02.13D08:27:41.633000000 AAPL N 5391 \"A\" 2018.02.13D08:28:00.078000000 AAPL N 54 713 \"A\" 2018.02.13D08:28:39.200000000 AAPL N 8117 \"C\" 2018.02.13D08:32:21.651000000 AAPL N 43 178 \"A\" q)args:(`table`keycols)!(table;`price) q).al.ffill args time sym src price size mode ------------------------------------------------------ 2018.02.13D08:02:09.322000000 AAPL N 32 513 \"A\" 2018.02.13D08:03:23.511000000 AAPL N 32 344 \"\" 2018.02.13D08:06:35.424000000 AAPL N 32 1933 \"B\" 2018.02.13D08:13:03.067000000 AAPL N 76 1009 \"B\" 2018.02.13D08:15:09.130000000 AAPL O 43 5199 \"B\" 2018.02.13D08:22:21.528000000 AAPL N 76 427 \"A\" 2018.02.13D08:23:46.489000000 AAPL N 76 7918 \"B\" 2018.02.13D08:26:34.645000000 AAPL N 43 420 \"\" 2018.02.13D08:27:41.633000000 AAPL N 43 5391 \"A\" 2018.02.13D08:28:00.078000000 AAPL N 54 713 \"A\" 2018.02.13D08:28:39.200000000 AAPL N 54 8117 \"C\" 2018.02.13D08:32:21.651000000 AAPL N 43 178 \"A\" Note that without specifying the by condition the column was forward filled as it sits in the table. Example 1.3 Passing just a table into the function will forward fill all columns, for example: q)table time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 12 4 00:38:39.521 AAPL 121 3 5 a 3 00:40:41.670 MSFT 63 3 4 20 c 4 00:48:08.048 MSFT 63 40 3 00:48:39.290 IBM 63 3 12 40 d 2 00:57:47.067 AAPL 24 3 30 2 01:08:00.945 AAPL 121 12 3 20 b 3 q).al.ffill table time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 32 12 30 b 4 00:38:39.521 AAPL 121 3 5 30 a 3 00:40:41.670 MSFT 63 3 5 20 a 4 00:48:08.048 MSFT 63 3 4 40 c 3 00:48:39.290 IBM 63 3 12 40 c 2 00:57:47.067 AAPL 63 24 3 30 c 2 01:08:00.945 AAPL 121 12 3 20 b 3 pivot A pivot table is a design tool used to reorganize and summarize selected columns and rows of data to gain a better understanding of the data being provided. An indepth explanation of how to pivot a table is available here . This is a modified version of code available on code.kx Usage This script contains the utility to pivot a table, specifying the keyed columns, the columns you wish to pivot around and the values you wish to expose. Note that this method always produces the last value for the grouping. To circumvent this you can do your own aggregation on the table before using the pivot function. For example you can create a column to calculate the sum: update totsum:sum price by sym,src from table and then pivot the data. This utility takes a dictionary as input with the following parameters: table: The table you want to pivot by: The keyed columns piv: The pivot columns var: The variables you want to see f: The function to create your column names (optional) g: The function to sort your column names (optional) The optional function f is a function of var and piv which creates column names for your pivoted table. The optional function g is a function of the keyed columns, piv and the return of f, which sorts the columns in ascending order. Examples Example 1.1 We have a table of quotes: q)quote date sym time side level price size ----------------------------------------------------- 2009.01.05 bafc 09:30:00.619 B 0 88.31803 96 2009.01.05 oljg 09:30:15.770 A 2 24.72941 14 2009.01.05 mgab 09:30:30.993 B 0 33.80173 2 2009.01.05 cflm 09:30:45.457 A 4 13.08412 98 2009.01.05 jgjm 09:31:00.668 B 0 80.2705 26 2009.01.05 cnkk 09:31:15.988 A 1 23.27025 38 .. We can key the table by date,sym and time, pivot around both side and level showing price and size at each time. The input dictionary is created in the following way: args:(`table`by`piv`var)!(quote;`date`sym`time;`side`level;`price`size); Here we are specifying the arguments for f and g as the default functions: // create _ separated column headers f:{[v;P] `$\"_\" sv' string (v,()) cross P} // return the headers in order g:{[k;P;c] k,asc c} The pivot function is then called and the output can be seen below: q).al.pivot[args] date sym time | price_A_0 price_A_1 price_A_2 price_A_3 price_A_4 price_B_0 price_B_1 price_B_2 price_B_3 price_B_4 size_A_0 size_A_1 size_A_2 size_A_3 size_A_4 size_B_0 size_B_1 size_B_2 size_B_3 size_B_4 ----------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2009.01.05 bafc 09:30:00.619| 88.31803 96 2009.01.05 bafc 10:05:30.395| 60.77025 73 2009.01.05 bafc 10:11:00.990| 0.03195086 71 2009.01.05 bafc 10:41:00.031| 68.8886 60 2009.01.05 bafc 10:41:15.924| 4.926275 56 2009.01.05 bafc 11:06:30.361| 84.28182 10 2009.01.05 bafc 11:52:30.361| 8.733561 7 .. Example 1.2 We have the following table showing some FX data: q)t sym time price size lp region ----------------------------------- EURGBP 09:00 1 870 UBS LON EURGBP 09:07 7 296 BARX LON EURGBP 09:19 3 850 BARX LON EURGBP 09:27 1 540 HSBC NYC EURUSD 09:31 4 995 UBS LON AUDUSD 09:38 0 396 HSBC NYC EURUSD 09:41 6 152 UBS LON EURUSD 09:42 4 317 HSBC NYC EURGBP 09:43 6 670 HSBC LON EURGBP 09:47 7 345 BARX NYC .. We want to see the different prices each liquidity provider has for each currency pair. For this we need to pivot the table t by sym around the lp's to show the various prices. We first create out parameter dictionary: args:(`table`by`piv`var)!(t;`sym;`lp;`price`size) We then pass the dictionary into the pivot utility: q).al.pivot[args] sym | price_BARX price_HSBC price_UBS ------| ------------------------------- EURGBP| 7 1 1 EURUSD| 8 4 4 AUDUSD| 4 0 0 Another way to manipulate the same data would be to key the table by sym and lp (i.e. currency pair and liquidity provider) and pivot by region to show both price and size. Setting up this dictionary and passing it to the pivot function we see the result below: q)args:(`table`by`piv`v)!(t;`sym`lp;`region;`price`size) q).al.pivot[args] sym lp | price_LON price_NYC size_LON size_NYC -----------| ------------------------------------- EURGBP UBS | 1 3 870 32 EURGBP BARX| 7 7 296 345 EURGBP HSBC| 6 1 670 540 EURUSD UBS | 4 1 995 494 AUDUSD HSBC| 5 0 879 396 EURUSD HSBC| 1 4 459 317 EURUSD BARX| 6 8 62 299 AUDUSD BARX| 4 2 817 716 AUDUSD UBS | 3 0 744 217 intervals The intervals.q utility in the .al namespace is used to output a list of equally spaced intervals between given start and end points. Usage Input is a dictionary containing: start: Starting integer number end: Ending integer number interval: Interval spacing between values round: Toggle rounding to nearest specified interval (optional) Parameters should be passed in the form of a dictionary, where start and end must be of the same type and interval can be either a long int or of the same type as start and end (i.e if start:09:00 and end:12:00, and intervals of 5 minutes were required interval could equal 00:05 or 5) Allowed data types are: date month time minute second timestamp timespan integer short long Example Using minute datatype: q)params:`start`end`interval`round!(09:32;12:00;00:30;0b) q).al.intervals[params] 09:32 10:02 10:32 11:02 11:32 or with round applied. q)params:`start`end`interval`round!(09:32;12:00;00:30;1b) q).al.intervals[params] 09:30 10:00 10:30 11:00 11:30 12:00 by default round is set to 1b, hence the result above can be obtained without inputting a value for round via: q)params:`start`end`interval!(09:32;12:00;00:30) q).al.intervals[params] 09:30 10:00 10:30 11:00 11:30 12:00 q)params:`start`end`interval!(2001.04.07;2001.05.01;5) q).al.intervals[params] 2001.04.05 2001.04.10 2001.04.15 2001.04.20 2001.04.25 2001.04.30 and without rounding q)params:`start`end`interval`round!(2001.04.07;2001.05.01;5;0b) q).al.intervals[params] 2001.04.07 2001.04.12 2001.04.17 2001.04.22 2001.04.27 q)params:`start`end`interval!(00:20:30 01:00:00 00:10:00) q).al.intervals[params] 00:20:00 00:30:00 00:40:00 00:50:00 01:00:00 q)params:`start`end`interval`round!(00:20:30 01:00:00 00:10:00) q).al.intervals[params] 00:20:30 00:30:30 00:40:30 00:50:30 q)params:`start`end`interval!(00:01:00.000000007;00:05:00.000000001;50000000000) q).al.intervals[params] 0D00:00:50.000000000 0D00:01:40.000000000 0D00:02:30.000000000 0D00:03:20.000000000 0D00:04:10.000000000 0D00:05:00.000000000 rack The rack utility gives the user the ability to create a rack table (the cross product of distinct values at the input). Usage Input is be a dictionary containing: table: Keyed or unkeyed in memory table keycols: The columns of the table you want to create the rack from. base: This is an additional table, against which the rack can be created (optional) timeseries.start: Start time to create a timeseries rack (optional) timeseries.end: End time to create a time series rack (optional) timeseries.interval: The interval for the time racking (optional) timeseries.round: Should rounding be carried out when creating the timeseries (optional) fullexpansion: Determines whether the required columns of input table will be expanded themselves or not. (optional, default is 0b) A timeseries is optional but if it is required then start, end, and interval must be specified as a dictionary called 'timeseries' (round remains optional with a default value of 1b). Keyed tables can be provided, these will be unkeyed by the function and crossed as standard unkeyed tables. Full expansion in this case refers to the level to which the data is crossed with itself and the user-defined intervals. For example were full expansion disabled then the table would be crossed with the user defined intervals and then crossed with the base. If full expansion is enabled then all of the table columns are crossed with each other before being crossed with the intervals and then the base. Allowing for a more in depth and detailed representation of the data. Examples Example 1.1 no fullexpansion, only table and keycols specified q)t sym exch price -------------- a nyse 1 b nyse 2 a cme 3 q)k `sym`exch create a dictionary q)dic:`table`keycols!(t;k) q).al.rack[dic] sym exch -------- a nyse b nyse a cme (simplest case, only returns unaltered keycols) Example 1.2 timeseries,fullexpansion specified, table is a keyed table q)dic table | (+(,`sym)!,`a`b`a)!+`exch`price!(`nyse`nyse`cme;1 2 3) keycols | `sym`exch timeseries | `start`end`interval!09:00 12:00 01:00 fullexpansion| 1b q).al.rack[dic] sym exch interval ----------------- a nyse 09:00 a nyse 10:00 a nyse 11:00 a nyse 12:00 a cme 09:00 a cme 10:00 a cme 11:00 a cme 12:00 b nyse 09:00 b nyse 10:00 b nyse 11:00 b nyse 12:00 b cme 09:00 b cme 10:00 b cme 11:00 b cme 12:00 Example 1.3 timeseries,fullexpansion specified,base specified, table is keyed q)dic table | (+(,`sym)!,`a`b`a)!+`exch`price!(`nyse`nyse`cme;1 2 3) keycols | `sym`exch timeseries | `start`end`interval!00:00:00 02:00:00 00:30:00 base | +(,`base)!,`buy`sell`buy`sell fullexpansion| 1b q).al.rack[dic] base sym exch interval ---------------------- buy a nyse 00:00:00 buy a nyse 00:30:00 buy a nyse 01:00:00 buy a nyse 01:30:00 buy a nyse 02:00:00 buy a cme 00:00:00 buy a cme 00:30:00 buy a cme 01:00:00 buy a cme 01:30:00 buy a cme 02:00:00 buy b nyse 00:00:00 buy b nyse 00:30:00 buy b nyse 01:00:00 buy b nyse 01:30:00 buy b nyse 02:00:00 buy b cme 00:00:00 buy b cme 00:30:00 buy b cme 01:00:00 buy b cme 01:30:00 buy b cme 02:00:00","title":"Analytics Library"},{"location":"analyticslib/#analytics-library","text":"A set of analytical functions is provided for the simplification of common operations. This set of tools is targeted at new kdb+ developers or business users. Data visualization would be an example application of this library, as an effective visualization of data incurs various manipulations of data sets. For example, a user wishes to produce a 15 minute bucketed sample of a time series, and forward fill it, then pivot it. All of the above can be achieved with the functions defined below.","title":"Analytics Library"},{"location":"analyticslib/#general-usage","text":"These set of function are defined in the .al namespace. All of the functions below have an input parameter in the form of a dictionary. The specifics of each dictionary are detailed in the usage section of each function. This set of utilities also includes a general set of error traps which were designed to give informative errors specific to each function. For example if a dictionary has not been provided then the error would be: q).al.pivot[123] 'Input parameter must be a dictionary with keys: -table -by -piv -var These error traps and messages are to guide the user in the correct use of the utilities and are designed to be as informative as possible.","title":"General Usage"},{"location":"analyticslib/#ffill","text":"","title":"ffill"},{"location":"analyticslib/#usage","text":"This script contains the utility to dynamically forward fill a given table keyed by given columns. Input is a dictionary containing: table: Table to be forward filled by: List of column names to key the table (optional) keycols: List of columns names to be forward filled (optional) OR Table This utility is equivalent to: update fills col1, fills col2 ... by col2 from table If you have a large data set or just a table with multiple columns typing this statement out can be quite laborious. With this utility you simply specify the table or parameter dictionary and pass it to the function. This utility also has the added functionality of being able to forward fill mixed list columns, i.e. strings.","title":"Usage"},{"location":"analyticslib/#examples","text":"","title":"Examples"},{"location":"analyticslib/#example-11","text":"By specifying the by condition in the input dictionary the function can forward fill keyed by specific column, for example: q)table time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 12 4 00:38:39.521 AAPL 121 3 5 a 3 00:40:41.670 MSFT 63 3 4 20 c 4 00:48:08.048 MSFT 63 40 3 00:48:39.290 IBM 63 3 12 40 d 2 00:57:47.067 AAPL 24 3 30 2 01:08:00.945 AAPL 121 12 3 20 b 3 We can create the input dictionary in the following way to specify the by clause of the ffill utility: q)args:(`table`by)!(table;`sym) Passing this to the function we can see the result: q).al.ffill args time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 32 12 30 b 4 00:38:39.521 AAPL 121 3 5 30 a 3 00:40:41.670 MSFT 63 3 4 20 c 4 00:48:08.048 MSFT 63 3 4 40 c 3 00:48:39.290 IBM 63 3 12 40 d 2 00:57:47.067 AAPL 121 24 3 30 a 2 01:08:00.945 AAPL 121 12 3 20 b 3","title":"Example 1.1"},{"location":"analyticslib/#example-12","text":"By specifying the keycols condition in the input dictionary the function can forward fill only specific columns, for example: Using the data set below we can create an input specifying which column we want to forward fill: q)table time sym src price size mode ------------------------------------------------------ 2018.02.13D08:02:09.322000000 AAPL N 32 513 \"A\" 2018.02.13D08:03:23.511000000 AAPL N 32 344 \"\" 2018.02.13D08:06:35.424000000 AAPL N 32 1933 \"B\" 2018.02.13D08:13:03.067000000 AAPL N 76 1009 \"B\" 2018.02.13D08:15:09.130000000 AAPL O 43 5199 \"B\" 2018.02.13D08:22:21.528000000 AAPL N 76 427 \"A\" 2018.02.13D08:23:46.489000000 AAPL N 7918 \"B\" 2018.02.13D08:26:34.645000000 AAPL N 43 420 \"\" 2018.02.13D08:27:41.633000000 AAPL N 5391 \"A\" 2018.02.13D08:28:00.078000000 AAPL N 54 713 \"A\" 2018.02.13D08:28:39.200000000 AAPL N 8117 \"C\" 2018.02.13D08:32:21.651000000 AAPL N 43 178 \"A\" q)args:(`table`keycols)!(table;`price) q).al.ffill args time sym src price size mode ------------------------------------------------------ 2018.02.13D08:02:09.322000000 AAPL N 32 513 \"A\" 2018.02.13D08:03:23.511000000 AAPL N 32 344 \"\" 2018.02.13D08:06:35.424000000 AAPL N 32 1933 \"B\" 2018.02.13D08:13:03.067000000 AAPL N 76 1009 \"B\" 2018.02.13D08:15:09.130000000 AAPL O 43 5199 \"B\" 2018.02.13D08:22:21.528000000 AAPL N 76 427 \"A\" 2018.02.13D08:23:46.489000000 AAPL N 76 7918 \"B\" 2018.02.13D08:26:34.645000000 AAPL N 43 420 \"\" 2018.02.13D08:27:41.633000000 AAPL N 43 5391 \"A\" 2018.02.13D08:28:00.078000000 AAPL N 54 713 \"A\" 2018.02.13D08:28:39.200000000 AAPL N 54 8117 \"C\" 2018.02.13D08:32:21.651000000 AAPL N 43 178 \"A\" Note that without specifying the by condition the column was forward filled as it sits in the table.","title":"Example 1.2"},{"location":"analyticslib/#example-13","text":"Passing just a table into the function will forward fill all columns, for example: q)table time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 12 4 00:38:39.521 AAPL 121 3 5 a 3 00:40:41.670 MSFT 63 3 4 20 c 4 00:48:08.048 MSFT 63 40 3 00:48:39.290 IBM 63 3 12 40 d 2 00:57:47.067 AAPL 24 3 30 2 01:08:00.945 AAPL 121 12 3 20 b 3 q).al.ffill table time sym ask bid asize bsize a id ------------------------------------------ 00:00:38.184 AMD 121 12 3 30 a 1 00:01:25.332 AMD 121 3 3 30 b 4 00:09:37.574 AAPL 63 32 3 30 b 3 00:21:24.796 AAPL 63 32 12 30 b 4 00:38:39.521 AAPL 121 3 5 30 a 3 00:40:41.670 MSFT 63 3 5 20 a 4 00:48:08.048 MSFT 63 3 4 40 c 3 00:48:39.290 IBM 63 3 12 40 c 2 00:57:47.067 AAPL 63 24 3 30 c 2 01:08:00.945 AAPL 121 12 3 20 b 3","title":"Example 1.3"},{"location":"analyticslib/#pivot","text":"A pivot table is a design tool used to reorganize and summarize selected columns and rows of data to gain a better understanding of the data being provided. An indepth explanation of how to pivot a table is available here . This is a modified version of code available on code.kx","title":"pivot"},{"location":"analyticslib/#usage_1","text":"This script contains the utility to pivot a table, specifying the keyed columns, the columns you wish to pivot around and the values you wish to expose. Note that this method always produces the last value for the grouping. To circumvent this you can do your own aggregation on the table before using the pivot function. For example you can create a column to calculate the sum: update totsum:sum price by sym,src from table and then pivot the data. This utility takes a dictionary as input with the following parameters: table: The table you want to pivot by: The keyed columns piv: The pivot columns var: The variables you want to see f: The function to create your column names (optional) g: The function to sort your column names (optional) The optional function f is a function of var and piv which creates column names for your pivoted table. The optional function g is a function of the keyed columns, piv and the return of f, which sorts the columns in ascending order.","title":"Usage"},{"location":"analyticslib/#examples_1","text":"","title":"Examples"},{"location":"analyticslib/#example-11_1","text":"We have a table of quotes: q)quote date sym time side level price size ----------------------------------------------------- 2009.01.05 bafc 09:30:00.619 B 0 88.31803 96 2009.01.05 oljg 09:30:15.770 A 2 24.72941 14 2009.01.05 mgab 09:30:30.993 B 0 33.80173 2 2009.01.05 cflm 09:30:45.457 A 4 13.08412 98 2009.01.05 jgjm 09:31:00.668 B 0 80.2705 26 2009.01.05 cnkk 09:31:15.988 A 1 23.27025 38 .. We can key the table by date,sym and time, pivot around both side and level showing price and size at each time. The input dictionary is created in the following way: args:(`table`by`piv`var)!(quote;`date`sym`time;`side`level;`price`size); Here we are specifying the arguments for f and g as the default functions: // create _ separated column headers f:{[v;P] `$\"_\" sv' string (v,()) cross P} // return the headers in order g:{[k;P;c] k,asc c} The pivot function is then called and the output can be seen below: q).al.pivot[args] date sym time | price_A_0 price_A_1 price_A_2 price_A_3 price_A_4 price_B_0 price_B_1 price_B_2 price_B_3 price_B_4 size_A_0 size_A_1 size_A_2 size_A_3 size_A_4 size_B_0 size_B_1 size_B_2 size_B_3 size_B_4 ----------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 2009.01.05 bafc 09:30:00.619| 88.31803 96 2009.01.05 bafc 10:05:30.395| 60.77025 73 2009.01.05 bafc 10:11:00.990| 0.03195086 71 2009.01.05 bafc 10:41:00.031| 68.8886 60 2009.01.05 bafc 10:41:15.924| 4.926275 56 2009.01.05 bafc 11:06:30.361| 84.28182 10 2009.01.05 bafc 11:52:30.361| 8.733561 7 ..","title":"Example 1.1"},{"location":"analyticslib/#example-12_1","text":"We have the following table showing some FX data: q)t sym time price size lp region ----------------------------------- EURGBP 09:00 1 870 UBS LON EURGBP 09:07 7 296 BARX LON EURGBP 09:19 3 850 BARX LON EURGBP 09:27 1 540 HSBC NYC EURUSD 09:31 4 995 UBS LON AUDUSD 09:38 0 396 HSBC NYC EURUSD 09:41 6 152 UBS LON EURUSD 09:42 4 317 HSBC NYC EURGBP 09:43 6 670 HSBC LON EURGBP 09:47 7 345 BARX NYC .. We want to see the different prices each liquidity provider has for each currency pair. For this we need to pivot the table t by sym around the lp's to show the various prices. We first create out parameter dictionary: args:(`table`by`piv`var)!(t;`sym;`lp;`price`size) We then pass the dictionary into the pivot utility: q).al.pivot[args] sym | price_BARX price_HSBC price_UBS ------| ------------------------------- EURGBP| 7 1 1 EURUSD| 8 4 4 AUDUSD| 4 0 0 Another way to manipulate the same data would be to key the table by sym and lp (i.e. currency pair and liquidity provider) and pivot by region to show both price and size. Setting up this dictionary and passing it to the pivot function we see the result below: q)args:(`table`by`piv`v)!(t;`sym`lp;`region;`price`size) q).al.pivot[args] sym lp | price_LON price_NYC size_LON size_NYC -----------| ------------------------------------- EURGBP UBS | 1 3 870 32 EURGBP BARX| 7 7 296 345 EURGBP HSBC| 6 1 670 540 EURUSD UBS | 4 1 995 494 AUDUSD HSBC| 5 0 879 396 EURUSD HSBC| 1 4 459 317 EURUSD BARX| 6 8 62 299 AUDUSD BARX| 4 2 817 716 AUDUSD UBS | 3 0 744 217","title":"Example 1.2"},{"location":"analyticslib/#intervals","text":"The intervals.q utility in the .al namespace is used to output a list of equally spaced intervals between given start and end points.","title":"intervals"},{"location":"analyticslib/#usage_2","text":"Input is a dictionary containing: start: Starting integer number end: Ending integer number interval: Interval spacing between values round: Toggle rounding to nearest specified interval (optional) Parameters should be passed in the form of a dictionary, where start and end must be of the same type and interval can be either a long int or of the same type as start and end (i.e if start:09:00 and end:12:00, and intervals of 5 minutes were required interval could equal 00:05 or 5) Allowed data types are: date month time minute second timestamp timespan integer short long","title":"Usage"},{"location":"analyticslib/#example","text":"Using minute datatype: q)params:`start`end`interval`round!(09:32;12:00;00:30;0b) q).al.intervals[params] 09:32 10:02 10:32 11:02 11:32 or with round applied. q)params:`start`end`interval`round!(09:32;12:00;00:30;1b) q).al.intervals[params] 09:30 10:00 10:30 11:00 11:30 12:00 by default round is set to 1b, hence the result above can be obtained without inputting a value for round via: q)params:`start`end`interval!(09:32;12:00;00:30) q).al.intervals[params] 09:30 10:00 10:30 11:00 11:30 12:00 q)params:`start`end`interval!(2001.04.07;2001.05.01;5) q).al.intervals[params] 2001.04.05 2001.04.10 2001.04.15 2001.04.20 2001.04.25 2001.04.30 and without rounding q)params:`start`end`interval`round!(2001.04.07;2001.05.01;5;0b) q).al.intervals[params] 2001.04.07 2001.04.12 2001.04.17 2001.04.22 2001.04.27 q)params:`start`end`interval!(00:20:30 01:00:00 00:10:00) q).al.intervals[params] 00:20:00 00:30:00 00:40:00 00:50:00 01:00:00 q)params:`start`end`interval`round!(00:20:30 01:00:00 00:10:00) q).al.intervals[params] 00:20:30 00:30:30 00:40:30 00:50:30 q)params:`start`end`interval!(00:01:00.000000007;00:05:00.000000001;50000000000) q).al.intervals[params] 0D00:00:50.000000000 0D00:01:40.000000000 0D00:02:30.000000000 0D00:03:20.000000000 0D00:04:10.000000000 0D00:05:00.000000000","title":"Example"},{"location":"analyticslib/#rack","text":"The rack utility gives the user the ability to create a rack table (the cross product of distinct values at the input).","title":"rack"},{"location":"analyticslib/#usage_3","text":"Input is be a dictionary containing: table: Keyed or unkeyed in memory table keycols: The columns of the table you want to create the rack from. base: This is an additional table, against which the rack can be created (optional) timeseries.start: Start time to create a timeseries rack (optional) timeseries.end: End time to create a time series rack (optional) timeseries.interval: The interval for the time racking (optional) timeseries.round: Should rounding be carried out when creating the timeseries (optional) fullexpansion: Determines whether the required columns of input table will be expanded themselves or not. (optional, default is 0b) A timeseries is optional but if it is required then start, end, and interval must be specified as a dictionary called 'timeseries' (round remains optional with a default value of 1b). Keyed tables can be provided, these will be unkeyed by the function and crossed as standard unkeyed tables. Full expansion in this case refers to the level to which the data is crossed with itself and the user-defined intervals. For example were full expansion disabled then the table would be crossed with the user defined intervals and then crossed with the base. If full expansion is enabled then all of the table columns are crossed with each other before being crossed with the intervals and then the base. Allowing for a more in depth and detailed representation of the data.","title":"Usage"},{"location":"analyticslib/#examples_2","text":"","title":"Examples"},{"location":"analyticslib/#example-11_2","text":"no fullexpansion, only table and keycols specified q)t sym exch price -------------- a nyse 1 b nyse 2 a cme 3 q)k `sym`exch create a dictionary q)dic:`table`keycols!(t;k) q).al.rack[dic] sym exch -------- a nyse b nyse a cme (simplest case, only returns unaltered keycols)","title":"Example 1.1"},{"location":"analyticslib/#example-12_2","text":"timeseries,fullexpansion specified, table is a keyed table q)dic table | (+(,`sym)!,`a`b`a)!+`exch`price!(`nyse`nyse`cme;1 2 3) keycols | `sym`exch timeseries | `start`end`interval!09:00 12:00 01:00 fullexpansion| 1b q).al.rack[dic] sym exch interval ----------------- a nyse 09:00 a nyse 10:00 a nyse 11:00 a nyse 12:00 a cme 09:00 a cme 10:00 a cme 11:00 a cme 12:00 b nyse 09:00 b nyse 10:00 b nyse 11:00 b nyse 12:00 b cme 09:00 b cme 10:00 b cme 11:00 b cme 12:00","title":"Example 1.2"},{"location":"analyticslib/#example-13_1","text":"timeseries,fullexpansion specified,base specified, table is keyed q)dic table | (+(,`sym)!,`a`b`a)!+`exch`price!(`nyse`nyse`cme;1 2 3) keycols | `sym`exch timeseries | `start`end`interval!00:00:00 02:00:00 00:30:00 base | +(,`base)!,`buy`sell`buy`sell fullexpansion| 1b q).al.rack[dic] base sym exch interval ---------------------- buy a nyse 00:00:00 buy a nyse 00:30:00 buy a nyse 01:00:00 buy a nyse 01:30:00 buy a nyse 02:00:00 buy a cme 00:00:00 buy a cme 00:30:00 buy a cme 01:00:00 buy a cme 01:30:00 buy a cme 02:00:00 buy b nyse 00:00:00 buy b nyse 00:30:00 buy b nyse 01:00:00 buy b nyse 01:30:00 buy b nyse 02:00:00 buy b cme 00:00:00 buy b cme 00:30:00 buy b cme 01:00:00 buy b cme 01:30:00 buy b cme 02:00:00","title":"Example 1.3"},{"location":"blog/","text":"Creating a Custom Application Something that is not immediately clear is how a custom application built on TorQ should be structured and deployed. This blog outlines how a custom application should be structured. Avoiding End-of-Day Halts kdb+tick is great, but there\u2019s a problem- when the RDB (real time database) writes to disk on its daily schedule, users cannot access that day\u2019s data until the write out is complete. this blog post details how TorQ solves this problem. Fast, Flexible, Low Memory End-Of-Day Writes A discussion on which method you should use for an end-of-day sort in TorQ. End-of-Day Parallel Sorting Details on how TorQ utilises sortworkers to vastly speed up the end-of-day sort. Broadcast Publish kdb+ v3.4 introduces a new broadcast feature that reduces the work done when publishing messages to multiple subscribers. This blog post explains how and why to use this feature. Recovering Corrupt Tickerplant Logs Corrupt tickerplant logs are a curse that no one deserves but that doesn\u2019t stop them from happening even to the best of us. However, all hope is not lost as it is possible to recover the good messages and discard the bad. In this post we will extend upon the standard rescuelog procedure to recover as much as possible from the log file. kdb+ Gateways The advantages and methods of using a gateway in a kdb+ tick system. Faster Recovery With kdb+ tick How to effectively recover a process to its previous state after a crash using the RDB instead of the tickerplant log. Parallel kdb+ Database Access with QPad We\u2019ve been working with Oleg Zakharov, who created QPad, to implement asynchronous querying and allow users to run multiple concurrent queries more efficiently. This blog explains how it works. TorQ Permission Framework An in depth and interactive post explaining TorQ permissioning.","title":"TorQ Blog Posts"},{"location":"cheatsheet/","text":"Cheat Sheet The purpose of this cheatsheet is to provide a very condensed guide to the bits you need to know about TorQ to either debug a running process, or to extend a TorQ stack. It does not replace the main body of documentation, and ordering is presumed order-of-relevance. The below uses the default configuration of TorQ and TorQ Finance Starter Pack, though a lot of it is configurable. It's probably a good idea to read the About section at least. Debugging Running Processes Each TorQ process has several internal tables which are useful. Connect to these using standard tools (qcon, an IDE etc.). Default user:pass of admin:admin will usually work. .usage.usage .usage.usage is used to track the queries that are executed against the process and is usually the first place to look for problems. Data is stored in memory for 24 hours and also persisted to disk in a usage_ file which is rolled daily. Every query and timer function call is logged, except for `.u.upd or `upd messages as this would bloat the log file considerably. Queries are logged before they are executed (status=\"b\") and after (status=\"c\" (complete) or status=\"e\" (error)). If a query blocks a process and makes it unresponsive, it will have an entry (status=\"b\") in the log file on disk. .usage.usage can be queried like any other kdb+ table to diagnose problems e.g. 100 sublist `timer xdesc .usage.usage select from .usage.usage where time within ... Note that this table is not especially helpful for gateway queries which are executed in an async call back manner. The gateway part of the request will (should) usually have a very short run time so the back end services should be interrogated to see what the slow parts are. More info on usage logs. .timer.timer .timer.timer shows information about the timer calls which are scheduled / have been run. Pay attention to the \"active\" field- if a timer call fails it will be removed from the timer (active=0b). To avoid this if required, wrap the function being executed by the timer in an error trap in the standard way. Use .timer.timer in combination with .usage.usage to work out if there are slow running/too frequent timers which are causing problems. More info. Log Files Log files are stored in the log directory specified by the environment variable KDBLOG. Each process creates 3 log files: an out log (out_ ) with standard log messages an error log file (err_ ) with errors a usage log file (usage_ ) with a log of every request that hits the process. The error log file should be empty. Don't ignore the out_ log file, there is a lot of information in there which can be used to debug. One thing that is a bit awkward is that if there is an error then the error log message timestamp has to be matched off against the out message log messages. You can force a process to write to a single log file if the process is started with the -onelog command line parameter, or use system commands similar to below to sync them up when required. # format the out and err logs into a single output sorted on time sort -nk1 out_log err_log # combine into a single output, show the last n rows of output before an error sort -nk1 out_log err_log | grep -B n ERR .clients.clients This shows inbound connections (connections created into this process). It may have interesting information about connection open/close. If it has a lot of rows it means some clients are connecting and disconnecting frequently. More info. .servers.SERVERS This shows outbound connections (connections created by this process to other processes). It's useful for tracking connections which have died. Starting, Stopping and Debugging Processes 95% of TorQ installations in production run on Linux, and the below applies to Linux only. Use the torq.sh script to start/stop/debug processes. Become familiar with torq.sh, it has some very handy utilities. newdeploy$ ./deploy/bin/torq.sh Arguments: start all|<processname(s)> to start all|process(es) stop all|<processname(s)> to stop all|process(es) print all|<processname(s)> to view default startup lines debug <processname(s)> to debug a single process qcon <processname> <username>:<password> to qcon process procs to list all processes summary to view summary table top <processname> to show top.q statistics for a single process Optional flags: -csv <fullcsvpath> to run a different csv file -extras <args> to add/overwrite extras to the start line -csv <fullcsvpath> -extras <args> to run both -force to force stop process(es) using kill -9 It is very, very difficult to try to debug a running kdb+ process remotely. Do not do this. Use torq.sh to stop the process and run it in the foreground in a test environment. torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh stop rdb1 09:45:40 | Shutting down rdb1... torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh debug rdb1 09:45:45 | Executing... q /home/torquser/newdeploy/deploy/TorQ/latest/torq.q -stackid 43100 -proctype rdb -procname rdb1 -U /home/torquser/newdeploy/deploy/TorQApp/latest/appconfig/passwords/accesslist.txt -localtime 1 -g 1 -T 180 -load /home/torquser/newdeploy/deploy/TorQ/latest/code/processes/rdb.q -procfile /home/torquser/newdeploy/deploy/TorQApp/latest/appconfig/process.csv -debug KDB+ 4.0 2020.06.18 Copyright (C) 1993-2020 Kx Systems l64/ 24()core 128387MB torquser homer 127.0.1.1 EXPIRE 2021.06.30 AquaQ #59946 (::;::) ################################################################################ # # # TorQ v3.8.0 # # AquaQ Analytics # # kdb+ consultancy, training and support # # # # For questions, comments, requests or bug reports please contact us # # w : www.aquaq.co.uk # # e : support@aquaq.co.uk # # # # Running on kdb+ 4 2020.06.18 # # # # TorQ Finance Starter Pack v 1.9.1 # # # ################################################################################ 2020.11.23D09:45:45.294308000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0 2020.11.23D09:45:45.294327000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0 2020.11.23D09:45:45.294815000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|attempting to read required process parameters proctype,procname from file /home/torquser/newdeploy/deploy/TorQApp/latest/appconfig/process.csv 2020.11.23D09:45:45.295076000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|readprocfile|port set to 43102 2020.11.23D09:45:45.295109000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|read in process parameters of proctype=rdb; procname=rdb1 2020.11.23D09:45:45.295616000|homer|rdb|rdb1|INF|fileload|config file /home/torquser/newdeploy/deploy/TorQ/latest/config/settings/default.q found 2020.11.23D09:45:45.295627000|homer|rdb|rdb1|INF|fileload|loading /home/torquser/newdeploy/deploy/TorQ/latest/config/settings/default.q 2020.11.23D09:45:45.296164000|homer|rdb|rdb1|INF|fileload|successfully loaded /home/torquser/newdeploy/deploy/TorQ/latest/config/settings/default.q ... snip ... 2020.11.23D09:45:47.432583000|homer|rdb|rdb1|INF|setpartition|rdbpartition contains - 2020.11.23 2020.11.23D09:45:47.432643000|homer|rdb|rdb1|INF|fileload|successfully loaded /home/torquser/newdeploy/deploy/TorQ/latest/code/processes/rdb.q 2020.11.23D09:45:47.432670000|homer|rdb|rdb1|INF|init|Resetting .z.pi to kdb+ default value ################################################################################ # # # TorQ v3.8.0 # # AquaQ Analytics # # kdb+ consultancy, training and support # # # # For questions, comments, requests or bug reports please contact us # # w : www.aquaq.co.uk # # e : support@aquaq.co.uk # # # # Running on kdb+ 4 2020.06.18 # # # # TorQ Finance Starter Pack v 1.9.1 # # # ################################################################################ q) Deploying Deployments should be very simple on Linux using the installation script . # pull in latest TorQ FSP install script wget https://raw.githubusercontent.com/AquaQAnalytics/TorQ-Finance-Starter-Pack/master/installlatest.sh # execute it bash installlatest.sh # It will finish with a message like below: ============================================================= INSTALLATION COMPLETE ============================================================= Installation is finished. For a regular installation, run it as follows in the working directory: ./deploy/bin/torq.sh start all # you may need to change the value of KDBBASEPORT held in deploy/bin/setenv.sh to avoid conflicts with other TorQ stacks if running on a shared host # once done execute start line and check it: torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh start all 08:28:02 | Starting discovery1... 08:28:02 | Starting tickerplant1... ... snip ... 08:28:06 | Starting dqe1... 08:28:06 | Starting dqedb1... torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh summary TIME | PROCESS | STATUS | PID | PORT 08:28:30 | discovery1 | up | 23221 | 43101 08:28:30 | tickerplant1 | up | 23333 | 43100 08:28:30 | rdb1 | up | 23443 | 43102 08:28:30 | hdb1 | up | 23553 | 43103 ... snip ... Modifying Existing Installations We always try to esnure new versions of TorQ are backwardly compatible. Try to avoid modifying TorQ itself and instead make application specific modifications. The installation script deploys TorQ in a structure which keeps TorQ and the application separate, and this should be adhered to whenever possible. To modify the config for a process, do it either in the config directory or as a command line start up parameter as- almost all config variables that are defined in a config file and exists in a non-root namespace can be overridden from the command line. To make a process load additional files, you can: append the additional files to the -load parameter on the start line append a set of directories of files using -loaddir on the start line place the files in one of the directories that is loaded by default on start up Start line modifications can be made in process.csv and will be picked up by torq.sh. Of these approaches, the latter is probably preferable. Adding Custom Processes When adding custom processes to TorQ it is important to understand the significance of proctype . In a nutshell, only processes which do exactly the same thing should have the same proctype. If two processes do roughly the same thing then -parentproctype can be used to share common functionality. Also there isn't any formal association between proctype and code name file name e.g. a process that loads code/processes/rdb.q can have any proctype we like. How TorQ manages connections is important. Avoid using hopen, use TorQ connection management. TorQ uses the fail fast principle (if you are going to fail, may as well do it as quickly as possible). This helps avoid processes starting up in inconsistent or unexpected states. If running a process with the -debug option, add the -stop or -trap options to stop at, or trap and continue through, start up errors. Code is loaded in a specific order, which can be overridden. To determine the order, inspect the bottom of the torq.q script (the last 100 lines or so). Everything after the switch into the root namespace is relevant.","title":"Cheat Sheat"},{"location":"cheatsheet/#cheat-sheet","text":"The purpose of this cheatsheet is to provide a very condensed guide to the bits you need to know about TorQ to either debug a running process, or to extend a TorQ stack. It does not replace the main body of documentation, and ordering is presumed order-of-relevance. The below uses the default configuration of TorQ and TorQ Finance Starter Pack, though a lot of it is configurable. It's probably a good idea to read the About section at least.","title":"Cheat Sheet"},{"location":"cheatsheet/#debugging-running-processes","text":"Each TorQ process has several internal tables which are useful. Connect to these using standard tools (qcon, an IDE etc.). Default user:pass of admin:admin will usually work.","title":"Debugging Running Processes"},{"location":"cheatsheet/#usageusage","text":".usage.usage is used to track the queries that are executed against the process and is usually the first place to look for problems. Data is stored in memory for 24 hours and also persisted to disk in a usage_ file which is rolled daily. Every query and timer function call is logged, except for `.u.upd or `upd messages as this would bloat the log file considerably. Queries are logged before they are executed (status=\"b\") and after (status=\"c\" (complete) or status=\"e\" (error)). If a query blocks a process and makes it unresponsive, it will have an entry (status=\"b\") in the log file on disk. .usage.usage can be queried like any other kdb+ table to diagnose problems e.g. 100 sublist `timer xdesc .usage.usage select from .usage.usage where time within ... Note that this table is not especially helpful for gateway queries which are executed in an async call back manner. The gateway part of the request will (should) usually have a very short run time so the back end services should be interrogated to see what the slow parts are. More info on usage logs.","title":".usage.usage"},{"location":"cheatsheet/#timertimer","text":".timer.timer shows information about the timer calls which are scheduled / have been run. Pay attention to the \"active\" field- if a timer call fails it will be removed from the timer (active=0b). To avoid this if required, wrap the function being executed by the timer in an error trap in the standard way. Use .timer.timer in combination with .usage.usage to work out if there are slow running/too frequent timers which are causing problems. More info.","title":".timer.timer"},{"location":"cheatsheet/#log-files","text":"Log files are stored in the log directory specified by the environment variable KDBLOG. Each process creates 3 log files: an out log (out_ ) with standard log messages an error log file (err_ ) with errors a usage log file (usage_ ) with a log of every request that hits the process. The error log file should be empty. Don't ignore the out_ log file, there is a lot of information in there which can be used to debug. One thing that is a bit awkward is that if there is an error then the error log message timestamp has to be matched off against the out message log messages. You can force a process to write to a single log file if the process is started with the -onelog command line parameter, or use system commands similar to below to sync them up when required. # format the out and err logs into a single output sorted on time sort -nk1 out_log err_log # combine into a single output, show the last n rows of output before an error sort -nk1 out_log err_log | grep -B n ERR","title":"Log Files"},{"location":"cheatsheet/#clientsclients","text":"This shows inbound connections (connections created into this process). It may have interesting information about connection open/close. If it has a lot of rows it means some clients are connecting and disconnecting frequently. More info.","title":".clients.clients"},{"location":"cheatsheet/#serversservers","text":"This shows outbound connections (connections created by this process to other processes). It's useful for tracking connections which have died.","title":".servers.SERVERS"},{"location":"cheatsheet/#starting-stopping-and-debugging-processes","text":"95% of TorQ installations in production run on Linux, and the below applies to Linux only. Use the torq.sh script to start/stop/debug processes. Become familiar with torq.sh, it has some very handy utilities. newdeploy$ ./deploy/bin/torq.sh Arguments: start all|<processname(s)> to start all|process(es) stop all|<processname(s)> to stop all|process(es) print all|<processname(s)> to view default startup lines debug <processname(s)> to debug a single process qcon <processname> <username>:<password> to qcon process procs to list all processes summary to view summary table top <processname> to show top.q statistics for a single process Optional flags: -csv <fullcsvpath> to run a different csv file -extras <args> to add/overwrite extras to the start line -csv <fullcsvpath> -extras <args> to run both -force to force stop process(es) using kill -9 It is very, very difficult to try to debug a running kdb+ process remotely. Do not do this. Use torq.sh to stop the process and run it in the foreground in a test environment. torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh stop rdb1 09:45:40 | Shutting down rdb1... torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh debug rdb1 09:45:45 | Executing... q /home/torquser/newdeploy/deploy/TorQ/latest/torq.q -stackid 43100 -proctype rdb -procname rdb1 -U /home/torquser/newdeploy/deploy/TorQApp/latest/appconfig/passwords/accesslist.txt -localtime 1 -g 1 -T 180 -load /home/torquser/newdeploy/deploy/TorQ/latest/code/processes/rdb.q -procfile /home/torquser/newdeploy/deploy/TorQApp/latest/appconfig/process.csv -debug KDB+ 4.0 2020.06.18 Copyright (C) 1993-2020 Kx Systems l64/ 24()core 128387MB torquser homer 127.0.1.1 EXPIRE 2021.06.30 AquaQ #59946 (::;::) ################################################################################ # # # TorQ v3.8.0 # # AquaQ Analytics # # kdb+ consultancy, training and support # # # # For questions, comments, requests or bug reports please contact us # # w : www.aquaq.co.uk # # e : support@aquaq.co.uk # # # # Running on kdb+ 4 2020.06.18 # # # # TorQ Finance Starter Pack v 1.9.1 # # # ################################################################################ 2020.11.23D09:45:45.294308000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0 2020.11.23D09:45:45.294327000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0 2020.11.23D09:45:45.294815000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|attempting to read required process parameters proctype,procname from file /home/torquser/newdeploy/deploy/TorQApp/latest/appconfig/process.csv 2020.11.23D09:45:45.295076000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|readprocfile|port set to 43102 2020.11.23D09:45:45.295109000|homer|torq|/home/torquser/newdeploy/deploy/TorQ/latest/torq.q_3407_0|INF|init|read in process parameters of proctype=rdb; procname=rdb1 2020.11.23D09:45:45.295616000|homer|rdb|rdb1|INF|fileload|config file /home/torquser/newdeploy/deploy/TorQ/latest/config/settings/default.q found 2020.11.23D09:45:45.295627000|homer|rdb|rdb1|INF|fileload|loading /home/torquser/newdeploy/deploy/TorQ/latest/config/settings/default.q 2020.11.23D09:45:45.296164000|homer|rdb|rdb1|INF|fileload|successfully loaded /home/torquser/newdeploy/deploy/TorQ/latest/config/settings/default.q ... snip ... 2020.11.23D09:45:47.432583000|homer|rdb|rdb1|INF|setpartition|rdbpartition contains - 2020.11.23 2020.11.23D09:45:47.432643000|homer|rdb|rdb1|INF|fileload|successfully loaded /home/torquser/newdeploy/deploy/TorQ/latest/code/processes/rdb.q 2020.11.23D09:45:47.432670000|homer|rdb|rdb1|INF|init|Resetting .z.pi to kdb+ default value ################################################################################ # # # TorQ v3.8.0 # # AquaQ Analytics # # kdb+ consultancy, training and support # # # # For questions, comments, requests or bug reports please contact us # # w : www.aquaq.co.uk # # e : support@aquaq.co.uk # # # # Running on kdb+ 4 2020.06.18 # # # # TorQ Finance Starter Pack v 1.9.1 # # # ################################################################################ q)","title":"Starting, Stopping and Debugging Processes"},{"location":"cheatsheet/#deploying","text":"Deployments should be very simple on Linux using the installation script . # pull in latest TorQ FSP install script wget https://raw.githubusercontent.com/AquaQAnalytics/TorQ-Finance-Starter-Pack/master/installlatest.sh # execute it bash installlatest.sh # It will finish with a message like below: ============================================================= INSTALLATION COMPLETE ============================================================= Installation is finished. For a regular installation, run it as follows in the working directory: ./deploy/bin/torq.sh start all # you may need to change the value of KDBBASEPORT held in deploy/bin/setenv.sh to avoid conflicts with other TorQ stacks if running on a shared host # once done execute start line and check it: torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh start all 08:28:02 | Starting discovery1... 08:28:02 | Starting tickerplant1... ... snip ... 08:28:06 | Starting dqe1... 08:28:06 | Starting dqedb1... torquser@homer:/home/torquser/newdeploy$ ./deploy/bin/torq.sh summary TIME | PROCESS | STATUS | PID | PORT 08:28:30 | discovery1 | up | 23221 | 43101 08:28:30 | tickerplant1 | up | 23333 | 43100 08:28:30 | rdb1 | up | 23443 | 43102 08:28:30 | hdb1 | up | 23553 | 43103 ... snip ...","title":"Deploying"},{"location":"cheatsheet/#modifying-existing-installations","text":"We always try to esnure new versions of TorQ are backwardly compatible. Try to avoid modifying TorQ itself and instead make application specific modifications. The installation script deploys TorQ in a structure which keeps TorQ and the application separate, and this should be adhered to whenever possible. To modify the config for a process, do it either in the config directory or as a command line start up parameter as- almost all config variables that are defined in a config file and exists in a non-root namespace can be overridden from the command line. To make a process load additional files, you can: append the additional files to the -load parameter on the start line append a set of directories of files using -loaddir on the start line place the files in one of the directories that is loaded by default on start up Start line modifications can be made in process.csv and will be picked up by torq.sh. Of these approaches, the latter is probably preferable.","title":"Modifying Existing Installations"},{"location":"cheatsheet/#adding-custom-processes","text":"When adding custom processes to TorQ it is important to understand the significance of proctype . In a nutshell, only processes which do exactly the same thing should have the same proctype. If two processes do roughly the same thing then -parentproctype can be used to share common functionality. Also there isn't any formal association between proctype and code name file name e.g. a process that loads code/processes/rdb.q can have any proctype we like. How TorQ manages connections is important. Avoid using hopen, use TorQ connection management. TorQ uses the fail fast principle (if you are going to fail, may as well do it as quickly as possible). This helps avoid processes starting up in inconsistent or unexpected states. If running a process with the -debug option, add the -stop or -trap options to stop at, or trap and continue through, start up errors. Code is loaded in a specific order, which can be overridden. To determine the order, inspect the bottom of the torq.q script (the last 100 lines or so). Everything after the switch into the root namespace is relevant.","title":"Adding Custom Processes"},{"location":"conn/","text":"Connection Management trackservers.q is used to register and maintain handles to external servers. It is a heavily modified version of trackservers.q from code.kx. All the options are described in the default config file. All connections are tracked in the .servers.SERVERS table. When the handle is used the count and last query time are updated. q).servers.SERVERS procname proctype hpup w hits startp lastp endp attributes --------------------------------------------------------------------------------- discovery1 discovery :aquaq:9996 0 2014.01.08D11:13:10.583056000 ()!() discovery2 discovery :aquaq:9995 6 0 2014.01.07D16:44:47.175757000 2014.01.07D16:44:47.174408000 ()!() rdb_europe_1 rdb :aquaq:9998 12 0 2014.01.07D16:46:47.897910000 2014.01.07D16:46:47.892901000 2014.01.07D16:46:44.626293000 `datacentre`country!`essex`uk rdb1 rdb :aquaq:5011 7 0 2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000 `datacentre`country!`essex`uk rdb_europe_1 hdb :aquaq:9997 0 2014.01.08D11:13:10.757801000 ()!() hdb1 hdb :aquaq:9999 0 2014.01.08D11:13:10.757801000 ()!() hdb2 hdb :aquaq:5013 8 0 2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000 `datacentre`country!`essex`uk hdb1 hdb :aquaq:5012 9 0 2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000 `datacentre`country!`essex`uk q)last .servers.SERVERS procname | `hdb2 proctype | `hdb hpup | `:aquaq:5013 w | 8i hits | 0i startp | 2014.01.08D11:51:01.928045000 lastp | 2014.01.08D11:51:01.925078000 endp | 0Np attributes| `datacentre`country!`essex`uk Connections Processes locate other processes based on their process type. The location is done either statically using the process.csv file or dynamically using a discovery service. It is recommended to use the discovery service as it allows the process to be notified as new processes become available. The main configuration variable is .servers.CONNECTIONS, which dictates which process type(s) to create connections to. .servers.startup[] must be called to initialise the connections. When connections are closed, the connection table is automatically updated. The process can be set to periodically retry connections. Process Attributes Each process can report a set of attributes. When process A connects to process B, process A will try to retrieve the attributes of process B. The attributes are defined by the result of the .proc.getattributes function, which is by default an empty dictionary. Attributes are used to retrieve more detail about the capabilities of each process, rather than relying on the broad brush process type and process name categorization. Attributes can be used for intelligent query routing. Potential fields for attributes include: range of data contained in the process; available tables; instrument universe; physical location; any other fields of relevance. Connection Passwords The password used by a process to connect to external processes is retrieved using the .servers.loadpassword function call. By default, this will read the password from a txt file contained in $KDBCONFIG/passwords. A default password can be used, which is overridden by one for the process type, which is itself overridden by one for the process name. For greater security, the .servers.loadpassword function should be modified. Some non-torq processes require a username and password to allow connection. These will be stored in a passwords dictionary. Passing the host and port of a process into this dictionary will return the full connection string if it is present within the dictionary. If however it is not present in the dictionary then the default username and password will be returned. Retrieving and Using Handles A function .servers.getservers is supplied to return a table of handle information. .servers.getservers takes five parameters: type-or-name: whether the lookup is to be done by type or name (can be either proctype or procname); types-or-names: the types or names to retrieve e.g. hdb; required-attributes: the dictionary of attributes to match on; open-dead-connections: whether to re-open dead connections; only-one: whether we only require one handle. So for example if 3 services of the supplied type are registered, and we have an open handle to 1 of them, the open handle will be returned and the others left closed irrespective of the open-dead-connections parameter. .servers.getservers will compare the required parameters with the available parameters for each handle. The resulting table will have an extra column called attribmatch which can be used to determine how good a match the service is with the required attributes. attribmatch is a dictionary of (required attribute key) ! (Boolean full match; intersection of attributes). q).servers.SERVERS procname proctype hpup w hits startp lastp endp attributes --------------------------------------------------------------------------------- discovery1 discovery :aquaq:9996 0 2014.01.08D11:51:01.922390000 ()!() discovery2 discovery :aquaq:9995 6 0 2014.01.08D11:51:01.923812000 2014.01.08D11:51:01.922390000 ()!() rdb_europe_1 rdb :aquaq:9998 0 2014.01.08D11:51:38.347598000 ()!() rdb_europe_2 rdb :aquaq:9997 0 2014.01.08D11:51:38.347598000 ()!() rdb1 rdb :aquaq:5011 7 0 2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000 `datacentre`country!`essex`uk hdb3 hdb :aquaq:5012 9 0 2014.01.08D11:51:38.349472000 2014.01.08D11:51:38.347598000 `datacentre`country!`essex`uk hdb2 hdb :aquaq:5013 8 0 2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000 `datacentre`country!`essex`uk /- pull back hdbs. Leave the attributes empty q).servers.getservers[`proctype;`hdb;()!();1b;0b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- hdb3 hdb 2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk ()!() hdb2 hdb 2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk ()!() /- supply some attributes q).servers.getservers[`proctype;`hdb;(enlist`country)!enlist`uk;1b;0b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- hdb3 hdb 2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk) hdb2 hdb 2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk) q).servers.getservers[`proctype;`hdb;`country`datacentre!`uk`slough;1b;0b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- hdb3 hdb 2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$())) hdb2 hdb 2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$())) .servers.getservers will try to automatically re-open connections if required. q).servers.getservers[`proctype;`rdb;()!();1b;0b] 2014.01.08D12:01:06.023146000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9998 2014.01.08D12:01:06.023581000|aquaq|gateway1|INF|conn|connection to :aquaq:9998 failed: hop: Connection refused 2014.01.08D12:01:06.023597000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9997 2014.01.08D12:01:06.023872000|aquaq|gateway1|INF|conn|connection to :aquaq:9997 failed: hop: Connection refused procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- rdb1 rdb 2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!() /- If we only require one connection, and we have one open,then it doesn't retry connections q).servers.getservers[`proctype;`rdb;()!();1b;1b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- rdb1 rdb 2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!() There are two other functions supplied for retrieving server details, both of which are based on .servers.getservers. .servers.gethandlebytype returns a single handle value, .servers.gethpupbytype returns a single host:port value. Both will re-open connections if there are not any valid connections. Both take two parameters: types: the type to retrieve e.g. hdb; selection-algorithm: can be one of any, last or roundrobin. Connecting To Non-TorQ Processes Connections to non-torq (external) processes can also be established. This is useful if you wish to integrate TorQ with an existing infrastructure. Any process can connect to external processes, or it can be managed by the discovery service only. Every external process should have a type and name in the same way as TorQ processes, to enable them to be located and used as required. Non-TorQ processes need to be listed by default in $KDBCONFIG/settings/nontorqprocess.csv. This file has the same format as the standard process.csv file. The location of the non-TorQ process file can be adjusted using the .servers.NONTORQPROCESSFILE variable. To enable connections, set .servers.TRACKNONTORQPROCESS to 1b. Example of nontorqprocess.csv file: host,port,proctype,procname aquaq,5533,hdb,extproc01 aquaq,5577,hdb,extproc02 Manually Adding And Using Connections Connections can also be manually added and used. See .api.p\u201c.servers.*\u201d for details. IPC types In version kdb+ v3.4, two new IPC connection types were added. These new types are unix domain sockets and SSL/TLS (tcps). The incoming connections to a proctype can be set by updating .servers.SOCKETTYPE. In the settings example below, everything that connects to the tickerplant will use unix domain sockets. \\d .servers SOCKETTYPE:enlist[`tickerplant]!enlist `unix Attempting to open a unix domain socket connection to a process which has an older kdb+ version will fail. We allow for processes to fallback to tcp if this happens by setting .servers.SOCKETFALLBACK to true. It will not fallback if the connection error message returned is one of the following : timeout, access. It will also not fallback for SSL/TLS (tcps) due to security concerns. At the time of writing, using unix domain sockets syntax on windows will appear to work whilst it\u2019s actually falling back to tcp in the background. This can be misleading so we disabled using them on windows.","title":"Connection Management"},{"location":"conn/#connection-management","text":"trackservers.q is used to register and maintain handles to external servers. It is a heavily modified version of trackservers.q from code.kx. All the options are described in the default config file. All connections are tracked in the .servers.SERVERS table. When the handle is used the count and last query time are updated. q).servers.SERVERS procname proctype hpup w hits startp lastp endp attributes --------------------------------------------------------------------------------- discovery1 discovery :aquaq:9996 0 2014.01.08D11:13:10.583056000 ()!() discovery2 discovery :aquaq:9995 6 0 2014.01.07D16:44:47.175757000 2014.01.07D16:44:47.174408000 ()!() rdb_europe_1 rdb :aquaq:9998 12 0 2014.01.07D16:46:47.897910000 2014.01.07D16:46:47.892901000 2014.01.07D16:46:44.626293000 `datacentre`country!`essex`uk rdb1 rdb :aquaq:5011 7 0 2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000 `datacentre`country!`essex`uk rdb_europe_1 hdb :aquaq:9997 0 2014.01.08D11:13:10.757801000 ()!() hdb1 hdb :aquaq:9999 0 2014.01.08D11:13:10.757801000 ()!() hdb2 hdb :aquaq:5013 8 0 2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000 `datacentre`country!`essex`uk hdb1 hdb :aquaq:5012 9 0 2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000 `datacentre`country!`essex`uk q)last .servers.SERVERS procname | `hdb2 proctype | `hdb hpup | `:aquaq:5013 w | 8i hits | 0i startp | 2014.01.08D11:51:01.928045000 lastp | 2014.01.08D11:51:01.925078000 endp | 0Np attributes| `datacentre`country!`essex`uk","title":"Connection Management"},{"location":"conn/#connections","text":"Processes locate other processes based on their process type. The location is done either statically using the process.csv file or dynamically using a discovery service. It is recommended to use the discovery service as it allows the process to be notified as new processes become available. The main configuration variable is .servers.CONNECTIONS, which dictates which process type(s) to create connections to. .servers.startup[] must be called to initialise the connections. When connections are closed, the connection table is automatically updated. The process can be set to periodically retry connections.","title":"Connections"},{"location":"conn/#process-attributes","text":"Each process can report a set of attributes. When process A connects to process B, process A will try to retrieve the attributes of process B. The attributes are defined by the result of the .proc.getattributes function, which is by default an empty dictionary. Attributes are used to retrieve more detail about the capabilities of each process, rather than relying on the broad brush process type and process name categorization. Attributes can be used for intelligent query routing. Potential fields for attributes include: range of data contained in the process; available tables; instrument universe; physical location; any other fields of relevance.","title":"Process Attributes"},{"location":"conn/#connection-passwords","text":"The password used by a process to connect to external processes is retrieved using the .servers.loadpassword function call. By default, this will read the password from a txt file contained in $KDBCONFIG/passwords. A default password can be used, which is overridden by one for the process type, which is itself overridden by one for the process name. For greater security, the .servers.loadpassword function should be modified. Some non-torq processes require a username and password to allow connection. These will be stored in a passwords dictionary. Passing the host and port of a process into this dictionary will return the full connection string if it is present within the dictionary. If however it is not present in the dictionary then the default username and password will be returned.","title":"Connection Passwords"},{"location":"conn/#retrieving-and-using-handles","text":"A function .servers.getservers is supplied to return a table of handle information. .servers.getservers takes five parameters: type-or-name: whether the lookup is to be done by type or name (can be either proctype or procname); types-or-names: the types or names to retrieve e.g. hdb; required-attributes: the dictionary of attributes to match on; open-dead-connections: whether to re-open dead connections; only-one: whether we only require one handle. So for example if 3 services of the supplied type are registered, and we have an open handle to 1 of them, the open handle will be returned and the others left closed irrespective of the open-dead-connections parameter. .servers.getservers will compare the required parameters with the available parameters for each handle. The resulting table will have an extra column called attribmatch which can be used to determine how good a match the service is with the required attributes. attribmatch is a dictionary of (required attribute key) ! (Boolean full match; intersection of attributes). q).servers.SERVERS procname proctype hpup w hits startp lastp endp attributes --------------------------------------------------------------------------------- discovery1 discovery :aquaq:9996 0 2014.01.08D11:51:01.922390000 ()!() discovery2 discovery :aquaq:9995 6 0 2014.01.08D11:51:01.923812000 2014.01.08D11:51:01.922390000 ()!() rdb_europe_1 rdb :aquaq:9998 0 2014.01.08D11:51:38.347598000 ()!() rdb_europe_2 rdb :aquaq:9997 0 2014.01.08D11:51:38.347598000 ()!() rdb1 rdb :aquaq:5011 7 0 2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000 `datacentre`country!`essex`uk hdb3 hdb :aquaq:5012 9 0 2014.01.08D11:51:38.349472000 2014.01.08D11:51:38.347598000 `datacentre`country!`essex`uk hdb2 hdb :aquaq:5013 8 0 2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000 `datacentre`country!`essex`uk /- pull back hdbs. Leave the attributes empty q).servers.getservers[`proctype;`hdb;()!();1b;0b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- hdb3 hdb 2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk ()!() hdb2 hdb 2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk ()!() /- supply some attributes q).servers.getservers[`proctype;`hdb;(enlist`country)!enlist`uk;1b;0b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- hdb3 hdb 2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk) hdb2 hdb 2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk) q).servers.getservers[`proctype;`hdb;`country`datacentre!`uk`slough;1b;0b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- hdb3 hdb 2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$())) hdb2 hdb 2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$())) .servers.getservers will try to automatically re-open connections if required. q).servers.getservers[`proctype;`rdb;()!();1b;0b] 2014.01.08D12:01:06.023146000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9998 2014.01.08D12:01:06.023581000|aquaq|gateway1|INF|conn|connection to :aquaq:9998 failed: hop: Connection refused 2014.01.08D12:01:06.023597000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9997 2014.01.08D12:01:06.023872000|aquaq|gateway1|INF|conn|connection to :aquaq:9997 failed: hop: Connection refused procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- rdb1 rdb 2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!() /- If we only require one connection, and we have one open,then it doesn't retry connections q).servers.getservers[`proctype;`rdb;()!();1b;1b] procname proctype lastp w hpup attributes attribmatch ------------------------------------------------------------------------------- rdb1 rdb 2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!() There are two other functions supplied for retrieving server details, both of which are based on .servers.getservers. .servers.gethandlebytype returns a single handle value, .servers.gethpupbytype returns a single host:port value. Both will re-open connections if there are not any valid connections. Both take two parameters: types: the type to retrieve e.g. hdb; selection-algorithm: can be one of any, last or roundrobin.","title":"Retrieving and Using Handles"},{"location":"conn/#connecting-to-non-torq-processes","text":"Connections to non-torq (external) processes can also be established. This is useful if you wish to integrate TorQ with an existing infrastructure. Any process can connect to external processes, or it can be managed by the discovery service only. Every external process should have a type and name in the same way as TorQ processes, to enable them to be located and used as required. Non-TorQ processes need to be listed by default in $KDBCONFIG/settings/nontorqprocess.csv. This file has the same format as the standard process.csv file. The location of the non-TorQ process file can be adjusted using the .servers.NONTORQPROCESSFILE variable. To enable connections, set .servers.TRACKNONTORQPROCESS to 1b. Example of nontorqprocess.csv file: host,port,proctype,procname aquaq,5533,hdb,extproc01 aquaq,5577,hdb,extproc02","title":"Connecting To Non-TorQ Processes"},{"location":"conn/#manually-adding-and-using-connections","text":"Connections can also be manually added and used. See .api.p\u201c.servers.*\u201d for details.","title":"Manually Adding And Using Connections"},{"location":"conn/#ipc-types","text":"In version kdb+ v3.4, two new IPC connection types were added. These new types are unix domain sockets and SSL/TLS (tcps). The incoming connections to a proctype can be set by updating .servers.SOCKETTYPE. In the settings example below, everything that connects to the tickerplant will use unix domain sockets. \\d .servers SOCKETTYPE:enlist[`tickerplant]!enlist `unix Attempting to open a unix domain socket connection to a process which has an older kdb+ version will fail. We allow for processes to fallback to tcp if this happens by setting .servers.SOCKETFALLBACK to true. It will not fallback if the connection error message returned is one of the following : timeout, access. It will also not fallback for SSL/TLS (tcps) due to security concerns. At the time of writing, using unix domain sockets syntax on windows will appear to work whilst it\u2019s actually falling back to tcp in the background. This can be misleading so we disabled using them on windows.","title":"IPC types"},{"location":"dataaccess/","text":"Data Access API Introduction and Key Features The Dataaccess API is a TorQ upgrade designed for seamless cross process data retrival. Other key upgrades of the API are: - Compatibility with non kdb+ processes such as Google BigQuery and qREST - Consistent queries across all processes - Data retrieval does not require q-SQL knowledge only q dictionary manipulation - User friendly interface including more comprehensible error messages - Queries are automatically optimised for each process - Thorough testing allowing ease of further development A more conceptual discussion of the API can be seen in this blog post Configuration The API can be initialised in a TorQ proccess by either: 1) Pass \"-dataaccess /path/to/tableproperties.csv\" on the startup line (see Example table properties file below for format) 2) Run \".dataaccess.init[`:path/to/tableproperties.csv]\" to initialise the code in a running process. In both cases the filepath should point to tableproperties.csv a .csv containing information about all the tables you want the API to query. The following table describes each of the columns of tableproperties.csv: Description of fields in tableproperties.csv Field Description Default proctype Denotes the type of process the table is loaded in (passing all will involke default behaviour) procs in .gw.servers tablename Table to query (Tables within namespaces are allowed) N/A primarytimecolumn Default timecolumn used to determine the partitioning of a process * attributecolumn Primary attribute column (see query optimisation) N/A instrumentcolumn Column containing instrument N/A rolltimeoffset Rollovertime offset from midnight .eodtime.rolltimeoffset rolltimezone Timezone of the Rollover Function .eodtime.rolltimezone datatimezone Timezone of the primary time column timestamps .eodtime.datatimezone partitionfield Partition field of the data $[.Q.qp[];.Q.pf;`] * The Default behaviour of primarytimecolumn is: If the table is defined in the tickerplant schema file then primarytimecolumn is set to be the time column defined by the tickerplant. Else if a unique column of type z or p exists it is used. (If uniqueness isn't satisfied an error will occur here.) Else if a unique column of type d exist then it is used. Else the API will error. Example Default Configuration File If the user wishes to use the TorQ FSP (see section below) the following example will suffice: proctype tablename primarytimecolumn attributecolumn instrumentcolumn rolltimeoffset rolltimezone datatimezone partitionfield trade time sym sym quote time sym sym This table will be configured as if it were the following proctype tablename primarytimecolumn attributecolumn instrumentcolumn rolltimeoffset rolltimezone datatimezone partitionfield rdb trade time sym sym 00:00 GMT GMT hdb trade time sym sym 00:00 GMT GMT date rdb quote time sym sym 00:00 GMT GMT hdb quote time sym sym 00:00 GMT GMT date A more complete explanation into the configuration can be seen in the Table Properties Configuration section. Usage When using the API to send queries direct to a process, the overarching function is getdata . getdata is a dynamic, lightweight function which takes in a uniform dictionary (see table below) and the above configuration to build a process bespoke query. Input consistency permits the user to disregard the pragmatics described in tableproperties.csv allowing getdata to be called either directly within a process or via .dataccess.getdata (discussed in the Gateway section). The getdata function is split into three sub functions: .dataaccess.checkinputs , .eqp.extractqueryparams and queryorder.orderquery . .dataaccess.checkinputs checks if the input dictionary is valid (See custom API errors) .eqp.extractqueryparams converts the arguments into q-SQL .queryorder.orderquery is the API's query optimiser (See Debugging and Optimisation) getdata's input takes the format of a dictionary who's keys represent attributes of a query and values that represent how these attributes are to look. Each of these parameter's in the input dictionary can map a very simplistic dictionary into queries that can become quite complex. The following table lists getdata's accepted arguments: Valid Inputs Parameter Required Example** Invalidpairs* Description tablename Yes `quote Table to query starttime Yes 2020.12.18D12:00 Start time - must be a valid time type (see timecolumn) endtime Yes 2020.12.20D12:00 End time - must be a valid time type (see timecolumn) timecolumn No `time Column to apply(startime;endime) filter to instruments No `AAPL`GOOG Instruments to filter on - will usually have an attribute applied (see tableproperties.csv) columns No `sym`bid`ask`bsize`asize aggregations Table columns to return - symbol list - assumed all if not present grouping No `sym Columns to group by - no grouping assumed if not present aggregations No `max`wavg!(`bidprice`askprice;(`asksize`askprice;`bidsize`bidprice)) columns|freeformcolumn dictionary of aggregations timebar No (10;`minute;`time) List of (bar size; time type;timegrouping column) valid types: `nanosecond`second`minute`hour`day) filters No `sym`bid`bsize!(enlist(like;\"AAPL\");((<;85);(>;83.5));enlist(not;within;5 43)) Dictionary of ordered filters to apply to keys of dictionary freeformwhere No \"sym=`AAPL, src=`BARX, price within 60 85\" Where clause in string format freeformby No \"sym:sym, source:src\" By clause in string format freeformcolumn No \"time, sym,mid\\:0.5*bid+ask\" aggregations Select clause in string format ordering No enlist(`desc`bidprice) List ordering results ascending or descending by column renamecolumn No `old1`old2`old3!`new1`new2`new3 Either a dictionary of old!new or list of column names postprocessing No {flip x} Post-processing of the data queryoptimisation No 0b Determines whether the query optimiser should be turned on/off, Default is 1b sublist No 42 Sublist getquery No 1b Runs .dataaccess.buildquery in each of the processes * Invalid pairs are two dictionary keys not allowed to be defined simultaneously, this is done to prevent unexpected behaviour such as the following query: select price,mprice:max price from trade Although the above is a valid query the result may be unexpected as the column lengths don't match up. If an invalid key pair is desired the user should convert all inputs to the q-SQL version. ** More complete examples are provided in the Examples section below Example function call q)getdata`tablename`starttime`endtime`instruments`columns!(`quote;2021.01.20D0;2021.01.23D0;`GOOG;`sym`bid`bsize) sym bid bsize ---------------- GOOG 71.57 1 GOOG 70.86 2 GOOG 70.91 8 GOOG 70.91 6 ... From within a kdb+ process the .dataaccess.buildquery function provides the developer with an insight into the query that has been built for example: q).dataaccess.buildquery `tablename`starttime`endtime`instruments`columns!(`quote;2021.01.20D0;2021.01.23D0;`GOOG;`sym`time`bid`bsize) ? `quote ((=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000)) 0b `sym`time`bid`bsize!`sym`time`bid`bsize Alternatively, the `getquery key can also be used to produce an identical result: q)getdata `tablename`starttime`endtime`instruments`columns`getquery!(`quote;2021.01.20D0;2021.01.23D0;`GOOG;`sym`time`bid`bsize;1b) ? `quote ((=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000)) 0b `sym`time`bid`bsize!`sym`time`bid`bsize This method is preferable as it has been extended to work from within the gateway (see Gateway section) or another exotic process: q).dataaccess.getdata `tablename`starttime`endtime`instruments`columns`getquery!(`quote;2021.01.20D0;.z.d+12:00;`GOOG;`sym`time`bid`bsize;1b) `rdb (?;`quote;((=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.03.16D12:00:00.000000000));0b;`sym`time`bid`bsize!`sym`time`bid`bsize) `hdb (?;`quote;((within;`date;2021.01.20 2021.03.17);(=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.03.16D12:00:00.000000000));0b;`sym`time`bid`bsize!`sym`time`bid`bsize) Aggregations The aggregations key is a dictionary led method of perfoming mathematical operations on columns of a table. The dictionary should be of the form: `agg1`agg2`...`aggn!((`col11`col12...`col1a);(`col21`col22...`col2b);...;(`coln1`coln2...`colnm) Certain aggregations are cross proccess enabled, that is they can be calculated across multiple proccess (See example in the Gateway section). The key accepts the following table of inputs: Table of Avaliable Aggregations Aggregation Description Example Cross Process Enabled (See Gateway) avg Return the mean of a list (enlist`avg)!enlist enlist `price No cor Return Pearson's Correlation coefficient of two lists (enlist `cor)!enlist enlist `bid`ask No count Return The length of a list (enlist`count)!enlist enlist `price Yes cov Return the covariance of a list pair (enlist `cov)!enlist enlist `bid`ask No dev Return the standard deviation of a list (enlist`dev)!enlist enlist `price No distinct Return distinct elements of a list (enlist`distinct)!enlist enlist `sym Yes first Return first element of a list (enlist`first)!enlist enlist `price Yes last Return the final value in a list (enlist`last)!enlist enlist `price Yes max Return the maximum value of a list (enlist`max)!enlist enlist `price Yes med Return the median value of a list (enlist`med)!enlist enlist `price No min Return the minimum value of a list (enlist`min)!enlist enlist `price Yes prd Return the product of a list (enlist`prd)!enlist enlist `price Yes sum Return the total of a list (enlist`sum)!enlist enlist `price No var Return the Variance of a list (enlist`var)!enlist enlist `price No wavg Return the weighted mean of two lists (enlist`wavg)!enlist enlist `asize`ask No wsum Return the weighted sum of two lists (enlist`wsum)!enlist enlist `asize`ask No The postprocessing key provides a work around for creating these cross process aggregations (see the postprocessing example in Further Examples section). The following function can be used to merge two aggregation dictionaries: q)f:{{(key x,y)!{:$[0=count raze x[z];y[z];$[2=count raze y[z];($[1=count x[z];raze x[z];x[z]];raze y[z]);raze x[z],raze y[z]]]}[x;y;] each key x,y}/[x]} q)A min| price q)B min| time q)C wavg| bid bsize q)f[(A;B;C)] min | `price`time wavg| ,`bid`bsize Filters The filters key is a dictionary led method of controlling which entries of a given table are being queried by setting out a criteria. The dictionary uses a table column as the key and the entries as the condition to be applied to that column. Any condition to be applied should be entered as a nest of two item lists for each condition and each sublist entered as an operator first followed by conditional values, for example: `col1`col2`...`coln!((op;cond);((op;cond);(op;cond));...;(op;cond) For negative conditionals, the not and ~: operators can be included as the first item of a three item list for the operators in, like and within, e.g. enlist`col1!enlist(not;within;`cond1`cond2) Table of Available Filters Operator Description Example < less than (enlist`col)!enlist(<;input) > greater than (enlist`col)!enlist(>;input) <> not equal (enlist`col)!enlist(<>;input) <= less than or equal to (enlist`col)!enlist(<=;input) >= greater than or equal to (enlist`col)!enlist(>=;input) = equal to (enlist`col)!enlist(=;input) ~ match/comparison (enlist`col)!enlist(~;input) in column value is an item of input list (enlist`col)!enlist(in;input) within column value is within bounds of two inputs (enlist`col)!enlist(within;input) like column symbol or string matches input string pattern (enlist`col)!enlist(like;input) not negative conditional when used with in,like or within (enlist`col)!enlist(not;in/like/within;input) Gateway The documentation for the gateway outside the API can be found here Accepting a uniform dictionary allows queries to be sent to the gateway using .dataaccess.getdata . Using .dataaccess.getdata allows the user to Leverage the checkinputs library from within the gateway and catch errors before they hit the process Uses .gw.servers to dynamically determine the appropriate processes to execute getdata in Determines the query type to send to the process(es) Provide further optional arguments to better determine the behaviour of the function see table below: Gateway Accepted Keys Input Key Example Default behaviour Description postback {0N!x} () Post back function for retuning async queries only join raze .dataaccess.multiprocjoin Join function to merge the tables timeout 00:00:03 0Wn Maximum time for query to run procs `rdb`hdb .dataaccess.attributesrouting Choose which processes to run getdata in * * By default, .dataaccess.forceservers is set to 0b . In this case, only a subset of .dataaccess.attributesrouting can be used. However, if .dataaccess.forceservers is set to 1b any server in .gw.servers can be used. One major benefit of using .dataaccess.getdata can be seen when performing aggregations across different processes. An example of this can be seen below, where the user gets the max/min of bid/ask across both the RDB and HDB. q)querydict:`tablename`starttime`endtime`aggregations!(`quote;2021.02.08D00:00:00.000000000;2021.02.09D09:00:00.000000000;`max`min!(`ask`bid;`ask`bid)) q)querydicttoday:`tablename`starttime`endtime`aggregations!(`quote;2021.02.09D00:00:00.000000000;2021.02.09D09:00:00.000000000;`max`min!(`ask`bid;`ask`bid)) q)querydictyesterday:`tablename`starttime`endtime`aggregations!(`quote;2021.02.09D00:00:00.000000000;2021.02.09D09:00:00.000000000;`max`min!(`ask`bid;`ask`bid)) // open connection to the gateway g q)g:hopen`::1234:admin:admin q)g(`.dataaccess.getdata;querydict) maxAsk maxBid minAsk minBid --------------------------- 214.41 213.49 8.43 7.43 q)g(`.dataaccess.getdata;querydictyesterday) maxAsk maxBid minAsk minBid --------------------------- 214.41 213.49 8.8 7.82 q)g(`.dataaccess.getdata;querydicttoday) maxAsk maxBid minAsk minBid --------------------------- 94.81 93.82 8.43 7.43 The cross process aggregations also work with groupings and freeformby keys, for example q)querydict1:`tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;2021.03.16D01:00:00.000000000;2021.03.17D18:00:00.000000000;\"sym\";`max`min!(`ask`bid;`ask`bid);`desc`maxAsk;-2) q)g(`.dataaccess.getdata;querydict1) sym | maxAsk maxBid minAsk minBid ----| --------------------------- DELL| 29.37 28.33 7.87 6.84 DOW | 24.52 23.48 2.56 1.55 Such behaviour is not demonstrated when using freeform queries, for example: q)querydict2:`tablename`starttime`endtime`freeformcolumn!(`quote;2021.02.08D00:00:00.000000000;2021.02.09D09:00:00.000000000;\"max ask,min ask,max bid, min bid\") q)g(`.dataaccess.getdata;querydict2) ask bid ask1 bid1 ----------------------- 214.41 213.49 8.8 7.82 94.81 93.82 8.43 7.43 Updates to the dataaccess gateway code sees an ability to perform all map-reducable aggregations (except median) currently available to be performed over multiple processes without the need for a grouping based upon the partitioning field. The gateway now collects all the appropriate aggregates needed to calculate a value, and then re-aggregates the collected data based upon groupings when brought back to the gateway process. For complete clarity the full list of aggregations that can span multiple processes without a partitioned grouping are as follows: avg , cor , count , cov , dev , first , last , max , min , prd , sum , var , wavg and wsum . Checkinputs A key goal of the API is to prevent unwanted behaviour and return helpful error messages- this is done by .dataaccess.checkinputs . which under the covers runs two different checking libraries: .checkinputs A set of universal basic input checks as defined in checkinputs.csv (example .csv below). These checks are performed from within the gateway if applicable. .dataaccess A set of process bespoke checks, performed from within the queried proccess. Description of Fields in checkinputs.csv Field Description parameter Dictionary key to pass to getdata required Whether this parameter is mandatory checkfunction Function to determine whether the given value is valid invalid pairs Whether a parameter is invalid in combination with some other parameter Example checkinputs.csv parameter required checkfunction invalidpairs description tablename 1 .checkinputs.checktable table to query starttime 1 .checkinputs.checktimetype starttime - see timecolumn endtime 1 .checkinputs.checkendtime endtime - see timecolumn timecolumn 0 .checkinputs.checktimecolumn column to apply (startime;endime) filter to instruments 0 .checkinputs.checkinstruments instruments of interest - see tableproperties.csv columns 0 .checkinputs.checkcolumns table columns to return - assumed all if not present grouping 0 .checkinputs.checkgrouping columns to group by - no grouping assumed if not present aggregations 0 .checkinputs.checkaggregations columns freeformcolumn timebar 0 .checkinputs.checktimebar list of (time column to group on;size;type - `nanosecond`second`minute`hour`day) filters 0 .checkinputs.checkfilters a dictionary of columns + conditions in string format ordering 0 .checkinputs.checkordering a list of pairs regarding the direction (`asc or `desc) of ordering and a column to order freeformwhere 0 .checkinputs.isstring where clause in string format freeformby 0 .checkinputs.isstring by clause in string format freeformcolumn 0 .checkinputs.isstring select clause in string format instrumentcolumn 0 .checkinputs.checkinstrumentcolumn column to select instrument parameter from renamecolumn 0 .checkinputs.checkrenamecolumn dictionary to rename a column in results postprocessing 0 .checkinputs.checkpostprocessing applies postback lambda functions to data join 0 .checkinputs.checkjoin Joins queries together postback 0 .checkinputs.checkpostback sends async queries back timeout 0 .checkinputs.checktimeout Checks the time of the timeout sublist 0 .checkinputs.checksublist checks the head parameter procs 0 .checkinputs.checkprocs Checks the procs is the correct servers sqlquery 0 .checkinputs.isstring Select clause in string format getquery 0 .checkinputs.isboolean Returns the queries in each of the process dryrun 0 .checkinputs.isboolean Calculates the number of MB processed firstlastsort 0 .checkinputs.checkinstrumentcolumn Allows for use of firstlastsort (not supported by dataaccess) optimisation 0 .checkinputs.isboolean Toggle optimastion in queryorder The csv file enables developers simple extension, modification or deletion of the accepted inputs. For example if the user want to add a key `docs which accepts a boolean input they would add the following line to checkinputs.csv docs|0|.checkinputs.isboolean||info about docs function| Furthermore, using the .checkinputs.isboolean function would provide the user with a more comprehesive error message than 'type see messages below. Custom API Errors Below is a list of all the errors the API will return: Error|Function|Library| |-----|---------|-------------| |Table:{tablename} doesn't exist|checktablename|dataaccess| |Column(s) {badcol} presented in {parameter} is not a valid column for {tab}|checkcolumns|dataaccess| | If the distinct function is used, it cannot be present with any other aggregations including more of itself|checkaggregations|dataaccess| | Aggregations dictionary contains undefined function(s)|checkaggregations|dataaccess| | Incorrect number of input(s) entred for the following aggregations|checkaggregations|dataaccess| | Aggregations parameter must be supplied in order to perform group by statements|checkaggregations|dataaccess| | In order to use a grouping parameter, only aggregations that return single values may be used|checkaggregations|dataaccess| | The inputted size of the timebar argument: {size}, is not an appropriate size. Appropriate sizes are:|checktimebar|dataaccess| | Timebar parameter's intervals are too small. Time-bucket intervals must be greater than (or equal to) one nanosecond|checktimebar|dataaccess| |Input dictionary must have keys of type 11h|checkdictionary|checkinputs| |Required parameters missing:{}|checkdictionary|checkinputs| |Invalid parameter present:{}|checkdictionary|checkinputs| |Input must be a dictionary|isdictionary|checkinputs| |Parameter:{parameter} cannot be used in conjunction with parameter(s):{invalidpairs}|checkeachpair|checkinputs| |{} parameter(s) used more than once|checkrepeatparams|checkinputs| |Starttime parameter must be <= endtime parameter|checktimeorder|checkinputs| |Aggregations parameter key must be of type 11h - example:|checkaggregations|checkinputs| |Aggregations parameter values must be of type symbol - example:|checkaggregations|checkinputs| |First argument of timebar must be either -6h or -7h|checktimebar|checkinputs| |Second argument of timebar must be of type -11h|checktimebar|checkinputs| |Third argument of timebar must be have type -11h|checktimebar|checkinputs| |Filters parameter key must be of type 11h - example:|checkfilters|checkinputs| |Filters parameter values must be paired in the form (filter function;value(s)) or a list of three of the form (not;filter function;value(s)) - example:|checkfilters|checkinputs| |Filters parameter values containing three elements must have the first element being the not keyword - example|checkfilters|checkinputs| |Allowed operators are: =, <, >, <=, >=, in, within, like. The last three may be preceeded with 'not' e.g. (not within;80 100)|checkfilters|checkinputs| |The 'not' keyword may only preceed the operators within, in and like.|checkfilters|checkinputs| |(not)within statements within the filter parameter must contain exatly two values associated with it - example:|withincheck|checkinputs| |The use of inequalities in the filter parameter warrants only one value|inequalitycheck|checkinputs| |The use of equalities in the filter parameter warrants only one value - example:|inequalitycheck|checkinputs| |Ordering parameter must contain pairs of symbols as its input - example:|checkordering|checkinputs| |Ordering parameter's values must be paired in the form (direction;column) - example:|checkordering|checkinputs| |The first item in each of the ordering parameter's pairs must be either `asc or `desc - example:|checkordering|checkinputs| |Ordering parameter vague. Ordering by a column that aggregated more than once|checkordering|checkinputs| |Ordering parameter contains column that is not defined by aggregations, grouping or timebar parameter|checkordering|checkinputs| Table Properties Configuration Although the default configuration is often the best, there are examples when the user will have to define there own tableproperties.csv file. This will happen whenever a process has tables spanning timezones or a table has two columns of type p. We provide a complete example for clearer explanation: Suppose a vanilla TorQ process has two tables trade and quote for a New York FX market (timezone ET). In our scenario the TorQ system is GMT based and the rollover times are as follows: - trade rolls over at midnight GMT - quote rolls over at 01:00 am New York time (ET) The meta for tables in the hdb are: q)meta trade c | t f a -----| ----- date | d time | p sym | s p price| f size | i stop | b cond | c ex | c side | s q)meta quote c | t f a ------| ----- date | d time | p extime| p sym | s p bid | f ask | f bsize | j asize | j mode | c ex | c src | s Determining the correct primary time column for the trade table is simple as time is the unique column with of type p. The quote table is more complicated as it has two time columns extime and time. - The extime column is the time when the trade was made - The time column is the time when the data entered the tickerplant The time column is the most illuminating into the partition structure. The reason extime is not the primary time column is due to the latency between the exchange and TorQ process. Suppose the latency from the feed to tickerplant was a consistent 200ms. Now consider the following quote Quote1 comes into the exchange at 2020.02.02D00:59:59.900000000(ET) Quote1 comes into the tickerplant at 2020.02.03D06:00:00.100000000(GMT) Quote1 will be in partition 2020.02.03 As such the partitioning structure is dependent on the time column not the extime column. The p attribute on the sym column shows why it should be used as the attribute column. For this example the following tableproperties.csv should be defined. proctype tablename primarytimecolumn attributecolumn instrumentcolumn rolltimeoffset rolltimezone datatimezone partitionfield rdb trade time sym sym 00:00 GMT GMT hdb trade time sym sym 00:00 GMT GMT date rdb quote time sym sym 01:00 ET GMT hdb quote time sym sym 01:00 ET GMT date Query Optimisation The queries are automatically optimised using .queryorder.orderquery this function is designed to improve the performance of certain queries as well as return intuative results. This is done by: Prioritising filters against the primary attribute column in tableproperties.csv Swapping in for multiple = statements and razing the result together Furthermore, columns are ordered to put date then sym columns to the left. Optimisation can be toggled off by setting the value of `queryoptimisation in the input dictionary to 0b . Debugging and Optimisation A key focus of the API is to improve accessibility whilst maintaining a strong performance. There are cases where the accessibilty impedes the usabilty or the query speed drops below what could be developed. In these situations one should ensure: The user has a filter against a table attributes The query only pulls in the essential data The output of dataaccess.buildquery is what is expected. Metrics Introduction A test was performed to determine the performance of: The getdata function with optimisation on The getdata function with optimisation off Raw kdb+ query Methodology Each query was run against 10/100/1000 sym TorQ stack each with a 5GB HDB (68 million rows over 22 partitions) and a 120 MB RDB (2 million rows) quotes table. Query List Queryname Call Optimised1 `tablename`starttime`endtime`freeformby`aggregations`freeformwhere)!(`quote;00:00+2020.12.17D10;.z.d+12:00;\\\"sym\\\";(`max`min)!((`ask`bid);(`ask`bid));\\\"sym in `AMD`HPQ`DOW`MSFT`AIG`IBM kdb1 select max ask,min bid,max bid,min ask by sym from quote where sym in `AMD`HPQ`DOW`MSFT`AIG`IBM Optimised2 (`tablename`starttime`endtime`aggregations`timebar)!(`quote;2021.02.23D1;.z.p;(enlist(`max))!enlist(enlist(`ask));(6;`hour;`time)) kdb2 select max ask by 21600000000000 xbar time from quote where time>2021.02.23 Optimised3 (`tablename`starttime`endtime`filters!(`quote;2021.01.20D0;2021.02.25D12;`bsize`sym`bid!(enlist(not;within;5 43);enlist(like;\\\"*OW\\\");((<;85);(>;83.5))))) kdb3 select from quote where bid within(83.5;85),not bsize within(5;43),sym like \"*OW\" Results The results show the average execution time in ms for each query Queryname 10 syms 100 syms 1000 syms Optimised1 17 53 22 Unoptimised1 8 58 17 kdb1 27 50 19 Optimised2 365 220 327 Unoptimised2 320 224 649 kdb2 404 362 488 Optimised3 291 153 145 Unoptimised3 283 360 391 kdb3 344 392 413 Discussion Case 1 - Limited difference amongst all three queries, this occurs whenever a query can't be optimised or the dataset is too small for a change to be noticed. Case 2 - The API's strong all round performance, this occurs whenever a kdb+ query doesn't use the semantics of a process Case 3 - Demonstrates the performance boost of the API's optimiser, this occurs whenever a kdb+ query is not optimised Testing Library Each subfunction of getdata has thorough tests found in ${KDBTESTS}/dataaccess/ . To run the tests: Set environment variables Ensure your TorQ stack is not running Navigate to the appropriate testing directory Run . run.sh -d Logging Upon calling either .dataaccess.getdata or getdata the corresponding user, startime, endtime, handle, success and any error messages are upserted to the .dataaccess.stats table for example when a good and bad query are sent to the gateway: // Good gateway query q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D0;.z.p;((enlist `max)!enlist `ask`bid);`sym)\" sym | maxAsk maxBid ----| ------------- AAPL| 246.33 245.26 AIG | 85.07 84 ... // Bad gateway query q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D0;15;((enlist `max)!enlist `ask`bid);`sym)\" '`endtime input type incorrect - valid type(s):-12 -14 -15h - input type:-7h [0] g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D0;15;((enlist `max)!enlist `ask`bid);`sym)\" ^ The following are the gateway and rdb logs: // The logging in the gateway returns q)g\".dataaccess.stats\" querynumber| user starttime endtime handle request success error -----------| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | admin 2021.03.25D14:52:03.380485000 2021.03.25D14:52:04.138481000 11 `tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D00:00:00.000000000;2021.03.25D14:52:03.380478000;(,`max)!,`ask`bid;`sym) 1 2 | admin 2021.03.25D14:52:20.546227000 2021.03.25D14:52:20.546341000 11 `tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D00:00:00.000000000;15;(,`max)!,`ask`bid;`sym) 0 `endtime input type incorrect - valid type(s):-12 -14 -15h - input type:-7h // The bad query errored out in the gateway, consequently only the good query is seen in the rdb logs q)g\".dataaccess.stats\" querynumber| user starttime endtime handle request success error -----------| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | gateway 2021.03.25D14:52:03.381980000 2021.03.25D14:52:03.407870000 10 `tablename`starttime`endtime`aggregations`grouping`checksperformed`procs!(`quote;2021.02.12D00:00:00.000000000;2021.03.25D14:52:03.380478000;(,`max)!,`ask`bid;`sym;1b;`hdb`rdb) 1 Logging can be toggled off from within a process by setting the value of .dataaccess.logging to 0b . Further Integration This section describes the remaining features of the API as well as how the API can be leveraged to work with other AquaQ technologies. Implementation with TorQ FSP The API is compatible with the most recent TorQ Finance-Starter-Package , the fastest way to import the API is opening {APPCONFIG}/processes.csv and adding the following flag -dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv to the rdb , hdb and gateway extras column. For example: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+2,rdb,rdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,-dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv ,q localhost,{KDBBASEPORT}+3,hdb,hdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,-dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv,q localhost,{KDBBASEPORT}+4,hdb,hdb2,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,,q localhost,{KDBBASEPORT}+7,gateway,gateway1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,,4000,${KDBCODE}/processes/gateway.q,1,-dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv,q Implementation with q-REST The API is compatible with q-REST. To do this: Download q-REST Open application.properties and point kdb.host/port to the gateway Use the execute function argument to send .json s of the form: { \"function_name\": \".dataaccess.qrest\", \"arguments\":{ \"tablename\":\"quote\", \"starttime\":\"2021.02.17D10:00:00.000000000\", \"endtime\":\"2021.02.18D12:00:00.000000000\", \"freeformby\":\"sym\", \"aggregations\":\" `max`min!(`ask`bid;`ask)\", \"filters\":\"`sym`bid`bsize!(enlist(like;'*PL');((<;85);(>;83.5));enlist(~:;within;5 43))\" } } q-REST requires some modifications to the input dictionary: All dictionary values must be in string format Nested quotion marks are not permitted (Even when escaped out using \\\" ) The second argument in a like filter should be have ' rather than \" e.g (like; 'AMD') Implementation with Google BigQuery As key goal of the API has been TorQ's integration with other SQL databases such as Google BigQuery the successful outcome is discussed in this blog . Further Examples For every key in the dictionary the following examples provide a query, output and the functional select executed from within the process. Time default If the time column isn't specified it defaults to the value of `primaryattributecolumn q)getdata`tablename`starttime`endtime!(`quote;2021.01.20D0;2021.01.23D0) date time sym bid ask bsize asize mode ex src ---------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 84.01 84.87 77 33 A N BARX 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.58 84.93 13 89 Y N SUN 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB ... .dataaccess.buildquery `tablename`starttime`endtime!(`quote;2021.01.20D0;2021.01.23D0) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b () Instrument Filter Use the `instruments parameter to filter for sym=`AAPL q)getdata`tablename`starttime`endtime`instruments!(`quote;2021.01.20D0;2021.01.23D0;`AAPL) date time sym bid ask bsize asize mode ex src ---------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 84.01 84.87 77 33 A N BARX 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.58 84.93 13 89 Y N SUN 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB .. ... q).dataaccess.buildquery `tablename`starttime`endtime`instruments!(`quote;2021.01.20D0;2021.01.23D0;`AAPL) ? `quote ((=;`sym;,`AAPL);(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000)) 0b () Columns Use the `columns parameter to extract the following columns - `sym`time`bid q)getdata`tablename`starttime`endtime`columns!(`quote;2021.01.20D0;2021.01.23D0;`sym`time`bid) sym time bid ---------------------------------------- AAPL 2021.01.21D13:36:45.714478000 84.01 AAPL 2021.01.21D13:36:45.714478000 83.1 AAPL 2021.01.21D13:36:45.714478000 83.3 AAPL 2021.01.21D13:36:45.714478000 83.58 AAPL 2021.01.21D13:36:46.113465000 83.96 ... q).dataaccess.buildquery `tablename`starttime`endtime`columns!(`quote;2021.01.20D0;2021.01.23D0;`sym`time`bid) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b `sym`time`bid!`sym`time`bid Free form select Run a free form select using the `freeformcolumn parameter q)getdata`tablename`starttime`endtime`freeformcolumn!(`quote;2021.01.20D0;2021.01.23D0;\"sym,time,mid:0.5*bid+ask\") sym time mid ----------------------------------------- AAPL 2021.01.21D13:36:45.714478000 84.44 AAPL 2021.01.21D13:36:45.714478000 83.81 AAPL 2021.01.21D13:36:45.714478000 83.965 AAPL 2021.01.21D13:36:45.714478000 84.255 AAPL 2021.01.21D13:36:46.113465000 84.1 ... q).dataaccess.buildquery `tablename`starttime`endtime`freeformcolumn!(`quote;2021.01.20D0;2021.01.23D0;\"sym,time,mid:0.5*bid+ask\") ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b `sym`time`mid!(`sym;`time;(*;0.5;(+;`bid;`ask))) This can be used in conjunction with the columns parameter, however the columns parameters will be returned first. It is advised to use the columns parameter for returning existing columns and the freeformcolumn for any derived columns. Grouping Use `grouping parameter to group average `mid , by `sym getdata`tablename`starttime`endtime`freeformcolumn`grouping!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";`sym) sym | avgmid ----| -------- AAPL| 70.63876 AIG | 31.37041 AMD | 36.46488 DELL| 8.34496 DOW | 22.8436 q).dataaccess.buildquery `tablename`starttime`endtime`freeformcolumn`grouping!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";`sym) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) (,`sym)!,`sym (,`avgmid)!,(avg;(*;0.5;(+;`bid;`ask))) String style grouping Group average `mid , by instru:sym using the `freeformby parameter q)getdata`tablename`starttime`endtime`freeformcolumn`freeformby!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";\"sym\") instr| avgmid -----| -------- AAPL | 70.63876 AIG | 31.37041 AMD | 36.46488 DELL | 8.34496 DOW | 22.8436 q).dataaccess.buildquery `tablename`starttime`endtime`freeformcolumn`freeformby!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";\"sym\") ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) (,`sym)!,`sym (,`avgmid)!,(avg;(*;0.5;(+;`bid;`ask))) Time bucket Group max ask by 6 hour buckets using the `timebar parameter q)getdata(`tablename`starttime`endtime`aggregations`instruments`timebar)!(`quote;2021.01.21D1;2021.01.28D23;(enlist(`max))!enlist(enlist(`ask));`AAPL;(6;`hour;`time)) time | maxAsk -----------------------------| ------ 2021.01.21D12:00:00.000000000| 98.99 2021.01.21D18:00:00.000000000| 73.28 2021.01.22D12:00:00.000000000| 97.16 2021.01.22D18:00:00.000000000| 92.58 ... q).dataaccess.buildquery (`tablename`starttime`endtime`aggregations`instruments`timebar)!(`quote;2021.01.21D1;2021.01.28D23;(enlist(`max))!enlist(enlist(`ask));`AAPL;(6;`hour;`time)) ? `quote ((=;`sym;,`AAPL);(within;`time;2021.01.21D01:00:00.000000000 2021.01.28D23:00:00.000000000)) (,`time)!,({[timebucket;x] typ:type x; if[typ~12h;:timebucket xbar x]; if[typ in 13 14h;:.. Aggregations Max of both `bidprice and `askprice q)getdata`tablename`starttime`endtime`aggregations!(`quote;2021.01.20D0;2021.01.23D0;((enlist `max)!enlist `ask`bid)) maxAsk maxBid ------------- 109.5 108.6 q).dataaccess.buildquery `tablename`starttime`endtime`aggregations!(`quote;2021.01.20D0;2021.01.23D0;((enlist `max)!enlist `ask`bid)) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b `maxAsk`maxBid!((max;`ask);(max;`bid)) Filters Use the `filters parameter to execute a functional select style where clause q)getdata`tablename`starttime`endtime`filters!(`quote;2021.01.20D0;2021.01.23D0;(enlist(`src))!enlist enlist(in;`GETGO`DB)) date time sym bid ask bsize asize mode ex src --------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.8 84.76 78 32 Z N DB 2021.01.21 2021.01.21D13:36:48.714396000 AAPL 83.5 84.99 42 71 R N DB .. q).dataaccess.buildquery `tablename`starttime`endtime`filters!(`quote;2021.01.20D0;2021.01.23D0;(enlist(`src))!enlist enlist(in;`GETGO`DB)) ? `quote ((within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000);(in;`src;,`GETGO`DB)) 0b () ... Free form Filters Use the `freefromwhere parameter to execute the same filter as above q)getdata`tablename`starttime`endtime`freeformwhere!(`quote;2021.01.20D0;2021.01.23D0;\"src in `DB`GETGO\") date time sym bid ask bsize asize mode ex src --------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.8 84.76 78 32 Z N DB 2021.01.21 2021.01.21D13:36:48.714396000 AAPL 83.5 84.99 42 71 R N DB ... q).dataaccess.buildquery `tablename`starttime`endtime`freeformwhere!(`quote;2021.01.20D0;2021.01.23D0;\"src in `DB`GETGO\") ? `quote ((within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000);(in;`src;,`DB`GETGO)) 0b () Ordering Use the `ordering parameter to sort results by column ascending or descending q)getdata`tablename`starttime`endtime`ordering!(`quote;2000.01.01D00:00:00.000000000;2000.01.06D10:00:00.000000000;enlist(`asc`asksize)) sym time sourcetime bidprice bidsize askprice asksize ---------------------------------------------------------------------------------------------------- AAPL 2000.01.01D02:24:00.000000000 2000.01.01D02:24:00.000000000 90.9 932.4 111.1 1139.6 AAPL 2000.01.01D04:48:00.000000000 2000.01.01D04:48:00.000000000 98.1 933.3 119.9 1140.7 GOOG 2000.01.01D10:24:00.000000000 2000.01.01D11:12:00.000000000 96.3 940.5 117.7 1149.5 AAPL 2000.01.01D00:00:00.000000000 2000.01.01D00:00:00.000000000 97.2 959.4 118.8 1172.6 GOOG 2000.01.01D00:48:00.000000000 2000.01.01D01:36:00.000000000 93.6 1008 114.4 1232 GOOG 2000.01.01D03:12:00.000000000 2000.01.01D04:00:00.000000000 101.7 1078.2 124.3 1317.8 ... q).dataaccess.buildquery `tablename`starttime`endtime`ordering!(`quote;2000.01.01D00:00:00.000000000;2000.01.06D10:00:00.000000000;enlist(`asc`asksize)) ? `quote ,(within;`time;2000.01.01D00:00:00.000000000 2000.01.06D10:00:00.000000000) 0b () Rename Columns Use the `renamecolumn parameter to rename the columns q)getdata (`tablename`starttime`endtime`freeformby`freeformcolumn`instruments`renamecolumn)!(`trade;2021.01.18D0;2021.01.20D0;\"sym,date\";\"max price\";`IBM`AAPL`INTC;`sym`price`date!`newsym`newprice`newdate) newdate newsym| newprice -----------------| -------- 2021.01.18 IBM | 69.64 2021.01.19 IBM | 55.91 2021.01.18 AAPL | 121.66 2021.01.19 AAPL | 111.67 2021.01.18 INTC | 70.77 2021.01.19 INTC | 65.6 q).dataaccess.buildquery (`tablename`starttime`endtime`freeformby`freeformcolumn`instruments`renamecolumn)!(`trade;2021.01.18D0;2021.01.20D0;\"sym,date\";\"max price\";`IBM`AAPL`INTC;`sym`price`date!`newsym`newprice`newdate) ? `trade ((=;`sym;,`IBM);(within;`time;2021.01.18D00:00:00.000000000 2021.01.20D00:00:00.000000000)) `date`sym!`time.date`sym (,`price)!,(max;`price) ? `trade ((=;`sym;,`AAPL);(within;`time;2021.01.18D00:00:00.000000000 2021.01.20D00:00:00.000000000)) `date`sym!`time.date`sym (,`price)!,(max;`price) ? `trade ((=;`sym;,`INTC);(within;`time;2021.01.18D00:00:00.000000000 2021.01.20D00:00:00.000000000)) `date`sym!`time.date`sym (,`price)!,(max;`price) Postprocessing Use the `postproccessing key to under go post proccessing on a table for example flipping the table into a dictionary q)getdata`tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;2021.02.12D12;((enlist `max)!enlist `ask`bid);{flip x}) maxAsk| 91.74 maxBid| 90.65 q).dataaccess.buildquery `tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;2021.02.12D12;((enlist `max)!enlist `ask`bid);{flip x}) ? `quote ,(within;`time;2021.02.12D00:00:00.000000000 2021.02.12D12:00:00.000000000) 0b `maxAsk`maxBid!((max;`ask);(max;`bid)) More complex example collecting the avg price across multiple processes in the gateway g q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;.z.p;(`sum`count)!2#`ask;{flip x})\" sumAsk | 1.288549e+09 countAsk| 28738958 q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;.z.p;(`sum`count)!2#`ask;{select avgprice: sumAsk%countAsk from x})\" avgprice -------- 44.83632 Sublist Use the `sublist key to return the first n rows of a table, for example we get the first 2 rows of the table. q)getdata `tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;00:00+2021.02.17D10;.z.d+18:00;\"sym\";(`max`min)!((`ask`bid);(`ask`bid));enlist(`desc;`maxAsk);2) sym | maxAsk maxBid minAsk minBid ----| --------------------------- AAPL| 171.23 170.36 56.35 55.32 GOOG| 101.09 99.96 45.57 44.47 q).dataaccess.buildquery `tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;00:00+2021.02.17D10;.z.d+18:00;\"sym\";(`max`min)!((`ask`bid);(`ask`bid));enlist(`desc;`maxAsk);2) ? `quote ,(within;`time;2021.02.17D10:00:00.000000000 2021.03.03D18:00:00.000000000) (,`sym)!,`sym `maxAsk`maxBid`minAsk`minBid!((max;`ask);(max;`bid);(min;`ask);(min;`bid)) q)getdata `tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;00:00+2021.02.17D10;.z.d+18:00;\"sym\";(`max`min)!((`ask`bid);(`ask`bid));enlist(`desc;`maxAsk);3) sym | maxAsk maxBid minAsk minBid ----| --------------------------- INTC| 68.51 67.56 44.59 43.63 IBM | 48.53 47.61 37.1 36.11 HPQ | 46.09 45.05 29.97 29.03","title":"Data Access Library"},{"location":"dataaccess/#data-access-api","text":"","title":"Data Access API"},{"location":"dataaccess/#introduction-and-key-features","text":"The Dataaccess API is a TorQ upgrade designed for seamless cross process data retrival. Other key upgrades of the API are: - Compatibility with non kdb+ processes such as Google BigQuery and qREST - Consistent queries across all processes - Data retrieval does not require q-SQL knowledge only q dictionary manipulation - User friendly interface including more comprehensible error messages - Queries are automatically optimised for each process - Thorough testing allowing ease of further development A more conceptual discussion of the API can be seen in this blog post","title":"Introduction and Key Features"},{"location":"dataaccess/#configuration","text":"The API can be initialised in a TorQ proccess by either: 1) Pass \"-dataaccess /path/to/tableproperties.csv\" on the startup line (see Example table properties file below for format) 2) Run \".dataaccess.init[`:path/to/tableproperties.csv]\" to initialise the code in a running process. In both cases the filepath should point to tableproperties.csv a .csv containing information about all the tables you want the API to query. The following table describes each of the columns of tableproperties.csv:","title":"Configuration"},{"location":"dataaccess/#description-of-fields-in-tablepropertiescsv","text":"Field Description Default proctype Denotes the type of process the table is loaded in (passing all will involke default behaviour) procs in .gw.servers tablename Table to query (Tables within namespaces are allowed) N/A primarytimecolumn Default timecolumn used to determine the partitioning of a process * attributecolumn Primary attribute column (see query optimisation) N/A instrumentcolumn Column containing instrument N/A rolltimeoffset Rollovertime offset from midnight .eodtime.rolltimeoffset rolltimezone Timezone of the Rollover Function .eodtime.rolltimezone datatimezone Timezone of the primary time column timestamps .eodtime.datatimezone partitionfield Partition field of the data $[.Q.qp[];.Q.pf;`] * The Default behaviour of primarytimecolumn is: If the table is defined in the tickerplant schema file then primarytimecolumn is set to be the time column defined by the tickerplant. Else if a unique column of type z or p exists it is used. (If uniqueness isn't satisfied an error will occur here.) Else if a unique column of type d exist then it is used. Else the API will error.","title":"Description of fields in tableproperties.csv"},{"location":"dataaccess/#example-default-configuration-file","text":"If the user wishes to use the TorQ FSP (see section below) the following example will suffice: proctype tablename primarytimecolumn attributecolumn instrumentcolumn rolltimeoffset rolltimezone datatimezone partitionfield trade time sym sym quote time sym sym This table will be configured as if it were the following proctype tablename primarytimecolumn attributecolumn instrumentcolumn rolltimeoffset rolltimezone datatimezone partitionfield rdb trade time sym sym 00:00 GMT GMT hdb trade time sym sym 00:00 GMT GMT date rdb quote time sym sym 00:00 GMT GMT hdb quote time sym sym 00:00 GMT GMT date A more complete explanation into the configuration can be seen in the Table Properties Configuration section.","title":"Example Default Configuration File"},{"location":"dataaccess/#usage","text":"When using the API to send queries direct to a process, the overarching function is getdata . getdata is a dynamic, lightweight function which takes in a uniform dictionary (see table below) and the above configuration to build a process bespoke query. Input consistency permits the user to disregard the pragmatics described in tableproperties.csv allowing getdata to be called either directly within a process or via .dataccess.getdata (discussed in the Gateway section). The getdata function is split into three sub functions: .dataaccess.checkinputs , .eqp.extractqueryparams and queryorder.orderquery . .dataaccess.checkinputs checks if the input dictionary is valid (See custom API errors) .eqp.extractqueryparams converts the arguments into q-SQL .queryorder.orderquery is the API's query optimiser (See Debugging and Optimisation) getdata's input takes the format of a dictionary who's keys represent attributes of a query and values that represent how these attributes are to look. Each of these parameter's in the input dictionary can map a very simplistic dictionary into queries that can become quite complex. The following table lists getdata's accepted arguments:","title":"Usage"},{"location":"dataaccess/#valid-inputs","text":"Parameter Required Example** Invalidpairs* Description tablename Yes `quote Table to query starttime Yes 2020.12.18D12:00 Start time - must be a valid time type (see timecolumn) endtime Yes 2020.12.20D12:00 End time - must be a valid time type (see timecolumn) timecolumn No `time Column to apply(startime;endime) filter to instruments No `AAPL`GOOG Instruments to filter on - will usually have an attribute applied (see tableproperties.csv) columns No `sym`bid`ask`bsize`asize aggregations Table columns to return - symbol list - assumed all if not present grouping No `sym Columns to group by - no grouping assumed if not present aggregations No `max`wavg!(`bidprice`askprice;(`asksize`askprice;`bidsize`bidprice)) columns|freeformcolumn dictionary of aggregations timebar No (10;`minute;`time) List of (bar size; time type;timegrouping column) valid types: `nanosecond`second`minute`hour`day) filters No `sym`bid`bsize!(enlist(like;\"AAPL\");((<;85);(>;83.5));enlist(not;within;5 43)) Dictionary of ordered filters to apply to keys of dictionary freeformwhere No \"sym=`AAPL, src=`BARX, price within 60 85\" Where clause in string format freeformby No \"sym:sym, source:src\" By clause in string format freeformcolumn No \"time, sym,mid\\:0.5*bid+ask\" aggregations Select clause in string format ordering No enlist(`desc`bidprice) List ordering results ascending or descending by column renamecolumn No `old1`old2`old3!`new1`new2`new3 Either a dictionary of old!new or list of column names postprocessing No {flip x} Post-processing of the data queryoptimisation No 0b Determines whether the query optimiser should be turned on/off, Default is 1b sublist No 42 Sublist getquery No 1b Runs .dataaccess.buildquery in each of the processes * Invalid pairs are two dictionary keys not allowed to be defined simultaneously, this is done to prevent unexpected behaviour such as the following query: select price,mprice:max price from trade Although the above is a valid query the result may be unexpected as the column lengths don't match up. If an invalid key pair is desired the user should convert all inputs to the q-SQL version. ** More complete examples are provided in the Examples section below Example function call q)getdata`tablename`starttime`endtime`instruments`columns!(`quote;2021.01.20D0;2021.01.23D0;`GOOG;`sym`bid`bsize) sym bid bsize ---------------- GOOG 71.57 1 GOOG 70.86 2 GOOG 70.91 8 GOOG 70.91 6 ... From within a kdb+ process the .dataaccess.buildquery function provides the developer with an insight into the query that has been built for example: q).dataaccess.buildquery `tablename`starttime`endtime`instruments`columns!(`quote;2021.01.20D0;2021.01.23D0;`GOOG;`sym`time`bid`bsize) ? `quote ((=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000)) 0b `sym`time`bid`bsize!`sym`time`bid`bsize Alternatively, the `getquery key can also be used to produce an identical result: q)getdata `tablename`starttime`endtime`instruments`columns`getquery!(`quote;2021.01.20D0;2021.01.23D0;`GOOG;`sym`time`bid`bsize;1b) ? `quote ((=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000)) 0b `sym`time`bid`bsize!`sym`time`bid`bsize This method is preferable as it has been extended to work from within the gateway (see Gateway section) or another exotic process: q).dataaccess.getdata `tablename`starttime`endtime`instruments`columns`getquery!(`quote;2021.01.20D0;.z.d+12:00;`GOOG;`sym`time`bid`bsize;1b) `rdb (?;`quote;((=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.03.16D12:00:00.000000000));0b;`sym`time`bid`bsize!`sym`time`bid`bsize) `hdb (?;`quote;((within;`date;2021.01.20 2021.03.17);(=;`sym;,`GOOG);(within;`time;2021.01.20D00:00:00.000000000 2021.03.16D12:00:00.000000000));0b;`sym`time`bid`bsize!`sym`time`bid`bsize)","title":"Valid Inputs"},{"location":"dataaccess/#aggregations","text":"The aggregations key is a dictionary led method of perfoming mathematical operations on columns of a table. The dictionary should be of the form: `agg1`agg2`...`aggn!((`col11`col12...`col1a);(`col21`col22...`col2b);...;(`coln1`coln2...`colnm) Certain aggregations are cross proccess enabled, that is they can be calculated across multiple proccess (See example in the Gateway section). The key accepts the following table of inputs: Table of Avaliable Aggregations Aggregation Description Example Cross Process Enabled (See Gateway) avg Return the mean of a list (enlist`avg)!enlist enlist `price No cor Return Pearson's Correlation coefficient of two lists (enlist `cor)!enlist enlist `bid`ask No count Return The length of a list (enlist`count)!enlist enlist `price Yes cov Return the covariance of a list pair (enlist `cov)!enlist enlist `bid`ask No dev Return the standard deviation of a list (enlist`dev)!enlist enlist `price No distinct Return distinct elements of a list (enlist`distinct)!enlist enlist `sym Yes first Return first element of a list (enlist`first)!enlist enlist `price Yes last Return the final value in a list (enlist`last)!enlist enlist `price Yes max Return the maximum value of a list (enlist`max)!enlist enlist `price Yes med Return the median value of a list (enlist`med)!enlist enlist `price No min Return the minimum value of a list (enlist`min)!enlist enlist `price Yes prd Return the product of a list (enlist`prd)!enlist enlist `price Yes sum Return the total of a list (enlist`sum)!enlist enlist `price No var Return the Variance of a list (enlist`var)!enlist enlist `price No wavg Return the weighted mean of two lists (enlist`wavg)!enlist enlist `asize`ask No wsum Return the weighted sum of two lists (enlist`wsum)!enlist enlist `asize`ask No The postprocessing key provides a work around for creating these cross process aggregations (see the postprocessing example in Further Examples section). The following function can be used to merge two aggregation dictionaries: q)f:{{(key x,y)!{:$[0=count raze x[z];y[z];$[2=count raze y[z];($[1=count x[z];raze x[z];x[z]];raze y[z]);raze x[z],raze y[z]]]}[x;y;] each key x,y}/[x]} q)A min| price q)B min| time q)C wavg| bid bsize q)f[(A;B;C)] min | `price`time wavg| ,`bid`bsize","title":"Aggregations"},{"location":"dataaccess/#filters","text":"The filters key is a dictionary led method of controlling which entries of a given table are being queried by setting out a criteria. The dictionary uses a table column as the key and the entries as the condition to be applied to that column. Any condition to be applied should be entered as a nest of two item lists for each condition and each sublist entered as an operator first followed by conditional values, for example: `col1`col2`...`coln!((op;cond);((op;cond);(op;cond));...;(op;cond) For negative conditionals, the not and ~: operators can be included as the first item of a three item list for the operators in, like and within, e.g. enlist`col1!enlist(not;within;`cond1`cond2) Table of Available Filters Operator Description Example < less than (enlist`col)!enlist(<;input) > greater than (enlist`col)!enlist(>;input) <> not equal (enlist`col)!enlist(<>;input) <= less than or equal to (enlist`col)!enlist(<=;input) >= greater than or equal to (enlist`col)!enlist(>=;input) = equal to (enlist`col)!enlist(=;input) ~ match/comparison (enlist`col)!enlist(~;input) in column value is an item of input list (enlist`col)!enlist(in;input) within column value is within bounds of two inputs (enlist`col)!enlist(within;input) like column symbol or string matches input string pattern (enlist`col)!enlist(like;input) not negative conditional when used with in,like or within (enlist`col)!enlist(not;in/like/within;input)","title":"Filters"},{"location":"dataaccess/#gateway","text":"The documentation for the gateway outside the API can be found here Accepting a uniform dictionary allows queries to be sent to the gateway using .dataaccess.getdata . Using .dataaccess.getdata allows the user to Leverage the checkinputs library from within the gateway and catch errors before they hit the process Uses .gw.servers to dynamically determine the appropriate processes to execute getdata in Determines the query type to send to the process(es) Provide further optional arguments to better determine the behaviour of the function see table below: Gateway Accepted Keys Input Key Example Default behaviour Description postback {0N!x} () Post back function for retuning async queries only join raze .dataaccess.multiprocjoin Join function to merge the tables timeout 00:00:03 0Wn Maximum time for query to run procs `rdb`hdb .dataaccess.attributesrouting Choose which processes to run getdata in * * By default, .dataaccess.forceservers is set to 0b . In this case, only a subset of .dataaccess.attributesrouting can be used. However, if .dataaccess.forceservers is set to 1b any server in .gw.servers can be used. One major benefit of using .dataaccess.getdata can be seen when performing aggregations across different processes. An example of this can be seen below, where the user gets the max/min of bid/ask across both the RDB and HDB. q)querydict:`tablename`starttime`endtime`aggregations!(`quote;2021.02.08D00:00:00.000000000;2021.02.09D09:00:00.000000000;`max`min!(`ask`bid;`ask`bid)) q)querydicttoday:`tablename`starttime`endtime`aggregations!(`quote;2021.02.09D00:00:00.000000000;2021.02.09D09:00:00.000000000;`max`min!(`ask`bid;`ask`bid)) q)querydictyesterday:`tablename`starttime`endtime`aggregations!(`quote;2021.02.09D00:00:00.000000000;2021.02.09D09:00:00.000000000;`max`min!(`ask`bid;`ask`bid)) // open connection to the gateway g q)g:hopen`::1234:admin:admin q)g(`.dataaccess.getdata;querydict) maxAsk maxBid minAsk minBid --------------------------- 214.41 213.49 8.43 7.43 q)g(`.dataaccess.getdata;querydictyesterday) maxAsk maxBid minAsk minBid --------------------------- 214.41 213.49 8.8 7.82 q)g(`.dataaccess.getdata;querydicttoday) maxAsk maxBid minAsk minBid --------------------------- 94.81 93.82 8.43 7.43 The cross process aggregations also work with groupings and freeformby keys, for example q)querydict1:`tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;2021.03.16D01:00:00.000000000;2021.03.17D18:00:00.000000000;\"sym\";`max`min!(`ask`bid;`ask`bid);`desc`maxAsk;-2) q)g(`.dataaccess.getdata;querydict1) sym | maxAsk maxBid minAsk minBid ----| --------------------------- DELL| 29.37 28.33 7.87 6.84 DOW | 24.52 23.48 2.56 1.55 Such behaviour is not demonstrated when using freeform queries, for example: q)querydict2:`tablename`starttime`endtime`freeformcolumn!(`quote;2021.02.08D00:00:00.000000000;2021.02.09D09:00:00.000000000;\"max ask,min ask,max bid, min bid\") q)g(`.dataaccess.getdata;querydict2) ask bid ask1 bid1 ----------------------- 214.41 213.49 8.8 7.82 94.81 93.82 8.43 7.43 Updates to the dataaccess gateway code sees an ability to perform all map-reducable aggregations (except median) currently available to be performed over multiple processes without the need for a grouping based upon the partitioning field. The gateway now collects all the appropriate aggregates needed to calculate a value, and then re-aggregates the collected data based upon groupings when brought back to the gateway process. For complete clarity the full list of aggregations that can span multiple processes without a partitioned grouping are as follows: avg , cor , count , cov , dev , first , last , max , min , prd , sum , var , wavg and wsum .","title":"Gateway"},{"location":"dataaccess/#checkinputs","text":"A key goal of the API is to prevent unwanted behaviour and return helpful error messages- this is done by .dataaccess.checkinputs . which under the covers runs two different checking libraries: .checkinputs A set of universal basic input checks as defined in checkinputs.csv (example .csv below). These checks are performed from within the gateway if applicable. .dataaccess A set of process bespoke checks, performed from within the queried proccess. Description of Fields in checkinputs.csv Field Description parameter Dictionary key to pass to getdata required Whether this parameter is mandatory checkfunction Function to determine whether the given value is valid invalid pairs Whether a parameter is invalid in combination with some other parameter Example checkinputs.csv parameter required checkfunction invalidpairs description tablename 1 .checkinputs.checktable table to query starttime 1 .checkinputs.checktimetype starttime - see timecolumn endtime 1 .checkinputs.checkendtime endtime - see timecolumn timecolumn 0 .checkinputs.checktimecolumn column to apply (startime;endime) filter to instruments 0 .checkinputs.checkinstruments instruments of interest - see tableproperties.csv columns 0 .checkinputs.checkcolumns table columns to return - assumed all if not present grouping 0 .checkinputs.checkgrouping columns to group by - no grouping assumed if not present aggregations 0 .checkinputs.checkaggregations columns freeformcolumn timebar 0 .checkinputs.checktimebar list of (time column to group on;size;type - `nanosecond`second`minute`hour`day) filters 0 .checkinputs.checkfilters a dictionary of columns + conditions in string format ordering 0 .checkinputs.checkordering a list of pairs regarding the direction (`asc or `desc) of ordering and a column to order freeformwhere 0 .checkinputs.isstring where clause in string format freeformby 0 .checkinputs.isstring by clause in string format freeformcolumn 0 .checkinputs.isstring select clause in string format instrumentcolumn 0 .checkinputs.checkinstrumentcolumn column to select instrument parameter from renamecolumn 0 .checkinputs.checkrenamecolumn dictionary to rename a column in results postprocessing 0 .checkinputs.checkpostprocessing applies postback lambda functions to data join 0 .checkinputs.checkjoin Joins queries together postback 0 .checkinputs.checkpostback sends async queries back timeout 0 .checkinputs.checktimeout Checks the time of the timeout sublist 0 .checkinputs.checksublist checks the head parameter procs 0 .checkinputs.checkprocs Checks the procs is the correct servers sqlquery 0 .checkinputs.isstring Select clause in string format getquery 0 .checkinputs.isboolean Returns the queries in each of the process dryrun 0 .checkinputs.isboolean Calculates the number of MB processed firstlastsort 0 .checkinputs.checkinstrumentcolumn Allows for use of firstlastsort (not supported by dataaccess) optimisation 0 .checkinputs.isboolean Toggle optimastion in queryorder The csv file enables developers simple extension, modification or deletion of the accepted inputs. For example if the user want to add a key `docs which accepts a boolean input they would add the following line to checkinputs.csv docs|0|.checkinputs.isboolean||info about docs function| Furthermore, using the .checkinputs.isboolean function would provide the user with a more comprehesive error message than 'type see messages below.","title":"Checkinputs"},{"location":"dataaccess/#custom-api-errors","text":"Below is a list of all the errors the API will return: Error|Function|Library| |-----|---------|-------------| |Table:{tablename} doesn't exist|checktablename|dataaccess| |Column(s) {badcol} presented in {parameter} is not a valid column for {tab}|checkcolumns|dataaccess| | If the distinct function is used, it cannot be present with any other aggregations including more of itself|checkaggregations|dataaccess| | Aggregations dictionary contains undefined function(s)|checkaggregations|dataaccess| | Incorrect number of input(s) entred for the following aggregations|checkaggregations|dataaccess| | Aggregations parameter must be supplied in order to perform group by statements|checkaggregations|dataaccess| | In order to use a grouping parameter, only aggregations that return single values may be used|checkaggregations|dataaccess| | The inputted size of the timebar argument: {size}, is not an appropriate size. Appropriate sizes are:|checktimebar|dataaccess| | Timebar parameter's intervals are too small. Time-bucket intervals must be greater than (or equal to) one nanosecond|checktimebar|dataaccess| |Input dictionary must have keys of type 11h|checkdictionary|checkinputs| |Required parameters missing:{}|checkdictionary|checkinputs| |Invalid parameter present:{}|checkdictionary|checkinputs| |Input must be a dictionary|isdictionary|checkinputs| |Parameter:{parameter} cannot be used in conjunction with parameter(s):{invalidpairs}|checkeachpair|checkinputs| |{} parameter(s) used more than once|checkrepeatparams|checkinputs| |Starttime parameter must be <= endtime parameter|checktimeorder|checkinputs| |Aggregations parameter key must be of type 11h - example:|checkaggregations|checkinputs| |Aggregations parameter values must be of type symbol - example:|checkaggregations|checkinputs| |First argument of timebar must be either -6h or -7h|checktimebar|checkinputs| |Second argument of timebar must be of type -11h|checktimebar|checkinputs| |Third argument of timebar must be have type -11h|checktimebar|checkinputs| |Filters parameter key must be of type 11h - example:|checkfilters|checkinputs| |Filters parameter values must be paired in the form (filter function;value(s)) or a list of three of the form (not;filter function;value(s)) - example:|checkfilters|checkinputs| |Filters parameter values containing three elements must have the first element being the not keyword - example|checkfilters|checkinputs| |Allowed operators are: =, <, >, <=, >=, in, within, like. The last three may be preceeded with 'not' e.g. (not within;80 100)|checkfilters|checkinputs| |The 'not' keyword may only preceed the operators within, in and like.|checkfilters|checkinputs| |(not)within statements within the filter parameter must contain exatly two values associated with it - example:|withincheck|checkinputs| |The use of inequalities in the filter parameter warrants only one value|inequalitycheck|checkinputs| |The use of equalities in the filter parameter warrants only one value - example:|inequalitycheck|checkinputs| |Ordering parameter must contain pairs of symbols as its input - example:|checkordering|checkinputs| |Ordering parameter's values must be paired in the form (direction;column) - example:|checkordering|checkinputs| |The first item in each of the ordering parameter's pairs must be either `asc or `desc - example:|checkordering|checkinputs| |Ordering parameter vague. Ordering by a column that aggregated more than once|checkordering|checkinputs| |Ordering parameter contains column that is not defined by aggregations, grouping or timebar parameter|checkordering|checkinputs|","title":"Custom API Errors"},{"location":"dataaccess/#table-properties-configuration","text":"Although the default configuration is often the best, there are examples when the user will have to define there own tableproperties.csv file. This will happen whenever a process has tables spanning timezones or a table has two columns of type p. We provide a complete example for clearer explanation: Suppose a vanilla TorQ process has two tables trade and quote for a New York FX market (timezone ET). In our scenario the TorQ system is GMT based and the rollover times are as follows: - trade rolls over at midnight GMT - quote rolls over at 01:00 am New York time (ET) The meta for tables in the hdb are: q)meta trade c | t f a -----| ----- date | d time | p sym | s p price| f size | i stop | b cond | c ex | c side | s q)meta quote c | t f a ------| ----- date | d time | p extime| p sym | s p bid | f ask | f bsize | j asize | j mode | c ex | c src | s Determining the correct primary time column for the trade table is simple as time is the unique column with of type p. The quote table is more complicated as it has two time columns extime and time. - The extime column is the time when the trade was made - The time column is the time when the data entered the tickerplant The time column is the most illuminating into the partition structure. The reason extime is not the primary time column is due to the latency between the exchange and TorQ process. Suppose the latency from the feed to tickerplant was a consistent 200ms. Now consider the following quote Quote1 comes into the exchange at 2020.02.02D00:59:59.900000000(ET) Quote1 comes into the tickerplant at 2020.02.03D06:00:00.100000000(GMT) Quote1 will be in partition 2020.02.03 As such the partitioning structure is dependent on the time column not the extime column. The p attribute on the sym column shows why it should be used as the attribute column. For this example the following tableproperties.csv should be defined. proctype tablename primarytimecolumn attributecolumn instrumentcolumn rolltimeoffset rolltimezone datatimezone partitionfield rdb trade time sym sym 00:00 GMT GMT hdb trade time sym sym 00:00 GMT GMT date rdb quote time sym sym 01:00 ET GMT hdb quote time sym sym 01:00 ET GMT date","title":"Table Properties Configuration"},{"location":"dataaccess/#query-optimisation","text":"The queries are automatically optimised using .queryorder.orderquery this function is designed to improve the performance of certain queries as well as return intuative results. This is done by: Prioritising filters against the primary attribute column in tableproperties.csv Swapping in for multiple = statements and razing the result together Furthermore, columns are ordered to put date then sym columns to the left. Optimisation can be toggled off by setting the value of `queryoptimisation in the input dictionary to 0b .","title":"Query Optimisation"},{"location":"dataaccess/#debugging-and-optimisation","text":"A key focus of the API is to improve accessibility whilst maintaining a strong performance. There are cases where the accessibilty impedes the usabilty or the query speed drops below what could be developed. In these situations one should ensure: The user has a filter against a table attributes The query only pulls in the essential data The output of dataaccess.buildquery is what is expected.","title":"Debugging and Optimisation"},{"location":"dataaccess/#metrics","text":"","title":"Metrics"},{"location":"dataaccess/#introduction","text":"A test was performed to determine the performance of: The getdata function with optimisation on The getdata function with optimisation off Raw kdb+ query","title":"Introduction"},{"location":"dataaccess/#methodology","text":"Each query was run against 10/100/1000 sym TorQ stack each with a 5GB HDB (68 million rows over 22 partitions) and a 120 MB RDB (2 million rows) quotes table.","title":"Methodology"},{"location":"dataaccess/#query-list","text":"Queryname Call Optimised1 `tablename`starttime`endtime`freeformby`aggregations`freeformwhere)!(`quote;00:00+2020.12.17D10;.z.d+12:00;\\\"sym\\\";(`max`min)!((`ask`bid);(`ask`bid));\\\"sym in `AMD`HPQ`DOW`MSFT`AIG`IBM kdb1 select max ask,min bid,max bid,min ask by sym from quote where sym in `AMD`HPQ`DOW`MSFT`AIG`IBM Optimised2 (`tablename`starttime`endtime`aggregations`timebar)!(`quote;2021.02.23D1;.z.p;(enlist(`max))!enlist(enlist(`ask));(6;`hour;`time)) kdb2 select max ask by 21600000000000 xbar time from quote where time>2021.02.23 Optimised3 (`tablename`starttime`endtime`filters!(`quote;2021.01.20D0;2021.02.25D12;`bsize`sym`bid!(enlist(not;within;5 43);enlist(like;\\\"*OW\\\");((<;85);(>;83.5))))) kdb3 select from quote where bid within(83.5;85),not bsize within(5;43),sym like \"*OW\"","title":"Query List"},{"location":"dataaccess/#results","text":"The results show the average execution time in ms for each query Queryname 10 syms 100 syms 1000 syms Optimised1 17 53 22 Unoptimised1 8 58 17 kdb1 27 50 19 Optimised2 365 220 327 Unoptimised2 320 224 649 kdb2 404 362 488 Optimised3 291 153 145 Unoptimised3 283 360 391 kdb3 344 392 413","title":"Results"},{"location":"dataaccess/#discussion","text":"Case 1 - Limited difference amongst all three queries, this occurs whenever a query can't be optimised or the dataset is too small for a change to be noticed. Case 2 - The API's strong all round performance, this occurs whenever a kdb+ query doesn't use the semantics of a process Case 3 - Demonstrates the performance boost of the API's optimiser, this occurs whenever a kdb+ query is not optimised","title":"Discussion"},{"location":"dataaccess/#testing-library","text":"Each subfunction of getdata has thorough tests found in ${KDBTESTS}/dataaccess/ . To run the tests: Set environment variables Ensure your TorQ stack is not running Navigate to the appropriate testing directory Run . run.sh -d","title":"Testing Library"},{"location":"dataaccess/#logging","text":"Upon calling either .dataaccess.getdata or getdata the corresponding user, startime, endtime, handle, success and any error messages are upserted to the .dataaccess.stats table for example when a good and bad query are sent to the gateway: // Good gateway query q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D0;.z.p;((enlist `max)!enlist `ask`bid);`sym)\" sym | maxAsk maxBid ----| ------------- AAPL| 246.33 245.26 AIG | 85.07 84 ... // Bad gateway query q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D0;15;((enlist `max)!enlist `ask`bid);`sym)\" '`endtime input type incorrect - valid type(s):-12 -14 -15h - input type:-7h [0] g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D0;15;((enlist `max)!enlist `ask`bid);`sym)\" ^ The following are the gateway and rdb logs: // The logging in the gateway returns q)g\".dataaccess.stats\" querynumber| user starttime endtime handle request success error -----------| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | admin 2021.03.25D14:52:03.380485000 2021.03.25D14:52:04.138481000 11 `tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D00:00:00.000000000;2021.03.25D14:52:03.380478000;(,`max)!,`ask`bid;`sym) 1 2 | admin 2021.03.25D14:52:20.546227000 2021.03.25D14:52:20.546341000 11 `tablename`starttime`endtime`aggregations`grouping!(`quote;2021.02.12D00:00:00.000000000;15;(,`max)!,`ask`bid;`sym) 0 `endtime input type incorrect - valid type(s):-12 -14 -15h - input type:-7h // The bad query errored out in the gateway, consequently only the good query is seen in the rdb logs q)g\".dataaccess.stats\" querynumber| user starttime endtime handle request success error -----------| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | gateway 2021.03.25D14:52:03.381980000 2021.03.25D14:52:03.407870000 10 `tablename`starttime`endtime`aggregations`grouping`checksperformed`procs!(`quote;2021.02.12D00:00:00.000000000;2021.03.25D14:52:03.380478000;(,`max)!,`ask`bid;`sym;1b;`hdb`rdb) 1 Logging can be toggled off from within a process by setting the value of .dataaccess.logging to 0b .","title":"Logging"},{"location":"dataaccess/#further-integration","text":"This section describes the remaining features of the API as well as how the API can be leveraged to work with other AquaQ technologies.","title":"Further Integration"},{"location":"dataaccess/#implementation-with-torq-fsp","text":"The API is compatible with the most recent TorQ Finance-Starter-Package , the fastest way to import the API is opening {APPCONFIG}/processes.csv and adding the following flag -dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv to the rdb , hdb and gateway extras column. For example: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+2,rdb,rdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,-dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv ,q localhost,{KDBBASEPORT}+3,hdb,hdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,-dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv,q localhost,{KDBBASEPORT}+4,hdb,hdb2,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,,q localhost,{KDBBASEPORT}+7,gateway,gateway1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,,4000,${KDBCODE}/processes/gateway.q,1,-dataaccess ${KDBCONFIG}/dataaccess/tableproperties.csv,q","title":"Implementation with TorQ FSP"},{"location":"dataaccess/#implementation-with-q-rest","text":"The API is compatible with q-REST. To do this: Download q-REST Open application.properties and point kdb.host/port to the gateway Use the execute function argument to send .json s of the form: { \"function_name\": \".dataaccess.qrest\", \"arguments\":{ \"tablename\":\"quote\", \"starttime\":\"2021.02.17D10:00:00.000000000\", \"endtime\":\"2021.02.18D12:00:00.000000000\", \"freeformby\":\"sym\", \"aggregations\":\" `max`min!(`ask`bid;`ask)\", \"filters\":\"`sym`bid`bsize!(enlist(like;'*PL');((<;85);(>;83.5));enlist(~:;within;5 43))\" } } q-REST requires some modifications to the input dictionary: All dictionary values must be in string format Nested quotion marks are not permitted (Even when escaped out using \\\" ) The second argument in a like filter should be have ' rather than \" e.g (like; 'AMD')","title":"Implementation with q-REST"},{"location":"dataaccess/#implementation-with-google-bigquery","text":"As key goal of the API has been TorQ's integration with other SQL databases such as Google BigQuery the successful outcome is discussed in this blog .","title":"Implementation with Google BigQuery"},{"location":"dataaccess/#further-examples","text":"For every key in the dictionary the following examples provide a query, output and the functional select executed from within the process.","title":"Further Examples"},{"location":"dataaccess/#time-default","text":"If the time column isn't specified it defaults to the value of `primaryattributecolumn q)getdata`tablename`starttime`endtime!(`quote;2021.01.20D0;2021.01.23D0) date time sym bid ask bsize asize mode ex src ---------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 84.01 84.87 77 33 A N BARX 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.58 84.93 13 89 Y N SUN 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB ... .dataaccess.buildquery `tablename`starttime`endtime!(`quote;2021.01.20D0;2021.01.23D0) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b ()","title":"Time default"},{"location":"dataaccess/#instrument-filter","text":"Use the `instruments parameter to filter for sym=`AAPL q)getdata`tablename`starttime`endtime`instruments!(`quote;2021.01.20D0;2021.01.23D0;`AAPL) date time sym bid ask bsize asize mode ex src ---------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 84.01 84.87 77 33 A N BARX 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.58 84.93 13 89 Y N SUN 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB .. ... q).dataaccess.buildquery `tablename`starttime`endtime`instruments!(`quote;2021.01.20D0;2021.01.23D0;`AAPL) ? `quote ((=;`sym;,`AAPL);(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000)) 0b ()","title":"Instrument Filter"},{"location":"dataaccess/#columns","text":"Use the `columns parameter to extract the following columns - `sym`time`bid q)getdata`tablename`starttime`endtime`columns!(`quote;2021.01.20D0;2021.01.23D0;`sym`time`bid) sym time bid ---------------------------------------- AAPL 2021.01.21D13:36:45.714478000 84.01 AAPL 2021.01.21D13:36:45.714478000 83.1 AAPL 2021.01.21D13:36:45.714478000 83.3 AAPL 2021.01.21D13:36:45.714478000 83.58 AAPL 2021.01.21D13:36:46.113465000 83.96 ... q).dataaccess.buildquery `tablename`starttime`endtime`columns!(`quote;2021.01.20D0;2021.01.23D0;`sym`time`bid) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b `sym`time`bid!`sym`time`bid","title":"Columns"},{"location":"dataaccess/#free-form-select","text":"Run a free form select using the `freeformcolumn parameter q)getdata`tablename`starttime`endtime`freeformcolumn!(`quote;2021.01.20D0;2021.01.23D0;\"sym,time,mid:0.5*bid+ask\") sym time mid ----------------------------------------- AAPL 2021.01.21D13:36:45.714478000 84.44 AAPL 2021.01.21D13:36:45.714478000 83.81 AAPL 2021.01.21D13:36:45.714478000 83.965 AAPL 2021.01.21D13:36:45.714478000 84.255 AAPL 2021.01.21D13:36:46.113465000 84.1 ... q).dataaccess.buildquery `tablename`starttime`endtime`freeformcolumn!(`quote;2021.01.20D0;2021.01.23D0;\"sym,time,mid:0.5*bid+ask\") ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b `sym`time`mid!(`sym;`time;(*;0.5;(+;`bid;`ask))) This can be used in conjunction with the columns parameter, however the columns parameters will be returned first. It is advised to use the columns parameter for returning existing columns and the freeformcolumn for any derived columns.","title":"Free form select"},{"location":"dataaccess/#grouping","text":"Use `grouping parameter to group average `mid , by `sym getdata`tablename`starttime`endtime`freeformcolumn`grouping!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";`sym) sym | avgmid ----| -------- AAPL| 70.63876 AIG | 31.37041 AMD | 36.46488 DELL| 8.34496 DOW | 22.8436 q).dataaccess.buildquery `tablename`starttime`endtime`freeformcolumn`grouping!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";`sym) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) (,`sym)!,`sym (,`avgmid)!,(avg;(*;0.5;(+;`bid;`ask)))","title":"Grouping"},{"location":"dataaccess/#string-style-grouping","text":"Group average `mid , by instru:sym using the `freeformby parameter q)getdata`tablename`starttime`endtime`freeformcolumn`freeformby!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";\"sym\") instr| avgmid -----| -------- AAPL | 70.63876 AIG | 31.37041 AMD | 36.46488 DELL | 8.34496 DOW | 22.8436 q).dataaccess.buildquery `tablename`starttime`endtime`freeformcolumn`freeformby!(`quote;2021.01.20D0;2021.01.23D0;\"avgmid:avg 0.5*bid+ask\";\"sym\") ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) (,`sym)!,`sym (,`avgmid)!,(avg;(*;0.5;(+;`bid;`ask)))","title":"String style grouping"},{"location":"dataaccess/#time-bucket","text":"Group max ask by 6 hour buckets using the `timebar parameter q)getdata(`tablename`starttime`endtime`aggregations`instruments`timebar)!(`quote;2021.01.21D1;2021.01.28D23;(enlist(`max))!enlist(enlist(`ask));`AAPL;(6;`hour;`time)) time | maxAsk -----------------------------| ------ 2021.01.21D12:00:00.000000000| 98.99 2021.01.21D18:00:00.000000000| 73.28 2021.01.22D12:00:00.000000000| 97.16 2021.01.22D18:00:00.000000000| 92.58 ... q).dataaccess.buildquery (`tablename`starttime`endtime`aggregations`instruments`timebar)!(`quote;2021.01.21D1;2021.01.28D23;(enlist(`max))!enlist(enlist(`ask));`AAPL;(6;`hour;`time)) ? `quote ((=;`sym;,`AAPL);(within;`time;2021.01.21D01:00:00.000000000 2021.01.28D23:00:00.000000000)) (,`time)!,({[timebucket;x] typ:type x; if[typ~12h;:timebucket xbar x]; if[typ in 13 14h;:..","title":"Time bucket"},{"location":"dataaccess/#aggregations_1","text":"Max of both `bidprice and `askprice q)getdata`tablename`starttime`endtime`aggregations!(`quote;2021.01.20D0;2021.01.23D0;((enlist `max)!enlist `ask`bid)) maxAsk maxBid ------------- 109.5 108.6 q).dataaccess.buildquery `tablename`starttime`endtime`aggregations!(`quote;2021.01.20D0;2021.01.23D0;((enlist `max)!enlist `ask`bid)) ? `quote ,(within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000) 0b `maxAsk`maxBid!((max;`ask);(max;`bid))","title":"Aggregations"},{"location":"dataaccess/#filters_1","text":"Use the `filters parameter to execute a functional select style where clause q)getdata`tablename`starttime`endtime`filters!(`quote;2021.01.20D0;2021.01.23D0;(enlist(`src))!enlist enlist(in;`GETGO`DB)) date time sym bid ask bsize asize mode ex src --------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.8 84.76 78 32 Z N DB 2021.01.21 2021.01.21D13:36:48.714396000 AAPL 83.5 84.99 42 71 R N DB .. q).dataaccess.buildquery `tablename`starttime`endtime`filters!(`quote;2021.01.20D0;2021.01.23D0;(enlist(`src))!enlist enlist(in;`GETGO`DB)) ? `quote ((within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000);(in;`src;,`GETGO`DB)) 0b () ...","title":"Filters"},{"location":"dataaccess/#free-form-filters","text":"Use the `freefromwhere parameter to execute the same filter as above q)getdata`tablename`starttime`endtime`freeformwhere!(`quote;2021.01.20D0;2021.01.23D0;\"src in `DB`GETGO\") date time sym bid ask bsize asize mode ex src --------------------------------------------------------------------------------- 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.1 84.52 58 84 Y N DB 2021.01.21 2021.01.21D13:36:45.714478000 AAPL 83.3 84.63 76 28 I N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.96 84.24 50 73 Y N DB 2021.01.21 2021.01.21D13:36:46.113465000 AAPL 83.8 84.76 78 32 Z N DB 2021.01.21 2021.01.21D13:36:48.714396000 AAPL 83.5 84.99 42 71 R N DB ... q).dataaccess.buildquery `tablename`starttime`endtime`freeformwhere!(`quote;2021.01.20D0;2021.01.23D0;\"src in `DB`GETGO\") ? `quote ((within;`time;2021.01.20D00:00:00.000000000 2021.01.23D00:00:00.000000000);(in;`src;,`DB`GETGO)) 0b ()","title":"Free form Filters"},{"location":"dataaccess/#ordering","text":"Use the `ordering parameter to sort results by column ascending or descending q)getdata`tablename`starttime`endtime`ordering!(`quote;2000.01.01D00:00:00.000000000;2000.01.06D10:00:00.000000000;enlist(`asc`asksize)) sym time sourcetime bidprice bidsize askprice asksize ---------------------------------------------------------------------------------------------------- AAPL 2000.01.01D02:24:00.000000000 2000.01.01D02:24:00.000000000 90.9 932.4 111.1 1139.6 AAPL 2000.01.01D04:48:00.000000000 2000.01.01D04:48:00.000000000 98.1 933.3 119.9 1140.7 GOOG 2000.01.01D10:24:00.000000000 2000.01.01D11:12:00.000000000 96.3 940.5 117.7 1149.5 AAPL 2000.01.01D00:00:00.000000000 2000.01.01D00:00:00.000000000 97.2 959.4 118.8 1172.6 GOOG 2000.01.01D00:48:00.000000000 2000.01.01D01:36:00.000000000 93.6 1008 114.4 1232 GOOG 2000.01.01D03:12:00.000000000 2000.01.01D04:00:00.000000000 101.7 1078.2 124.3 1317.8 ... q).dataaccess.buildquery `tablename`starttime`endtime`ordering!(`quote;2000.01.01D00:00:00.000000000;2000.01.06D10:00:00.000000000;enlist(`asc`asksize)) ? `quote ,(within;`time;2000.01.01D00:00:00.000000000 2000.01.06D10:00:00.000000000) 0b ()","title":"Ordering"},{"location":"dataaccess/#rename-columns","text":"Use the `renamecolumn parameter to rename the columns q)getdata (`tablename`starttime`endtime`freeformby`freeformcolumn`instruments`renamecolumn)!(`trade;2021.01.18D0;2021.01.20D0;\"sym,date\";\"max price\";`IBM`AAPL`INTC;`sym`price`date!`newsym`newprice`newdate) newdate newsym| newprice -----------------| -------- 2021.01.18 IBM | 69.64 2021.01.19 IBM | 55.91 2021.01.18 AAPL | 121.66 2021.01.19 AAPL | 111.67 2021.01.18 INTC | 70.77 2021.01.19 INTC | 65.6 q).dataaccess.buildquery (`tablename`starttime`endtime`freeformby`freeformcolumn`instruments`renamecolumn)!(`trade;2021.01.18D0;2021.01.20D0;\"sym,date\";\"max price\";`IBM`AAPL`INTC;`sym`price`date!`newsym`newprice`newdate) ? `trade ((=;`sym;,`IBM);(within;`time;2021.01.18D00:00:00.000000000 2021.01.20D00:00:00.000000000)) `date`sym!`time.date`sym (,`price)!,(max;`price) ? `trade ((=;`sym;,`AAPL);(within;`time;2021.01.18D00:00:00.000000000 2021.01.20D00:00:00.000000000)) `date`sym!`time.date`sym (,`price)!,(max;`price) ? `trade ((=;`sym;,`INTC);(within;`time;2021.01.18D00:00:00.000000000 2021.01.20D00:00:00.000000000)) `date`sym!`time.date`sym (,`price)!,(max;`price)","title":"Rename Columns"},{"location":"dataaccess/#postprocessing","text":"Use the `postproccessing key to under go post proccessing on a table for example flipping the table into a dictionary q)getdata`tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;2021.02.12D12;((enlist `max)!enlist `ask`bid);{flip x}) maxAsk| 91.74 maxBid| 90.65 q).dataaccess.buildquery `tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;2021.02.12D12;((enlist `max)!enlist `ask`bid);{flip x}) ? `quote ,(within;`time;2021.02.12D00:00:00.000000000 2021.02.12D12:00:00.000000000) 0b `maxAsk`maxBid!((max;`ask);(max;`bid)) More complex example collecting the avg price across multiple processes in the gateway g q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;.z.p;(`sum`count)!2#`ask;{flip x})\" sumAsk | 1.288549e+09 countAsk| 28738958 q)g\".dataaccess.getdata`tablename`starttime`endtime`aggregations`postprocessing!(`quote;2021.02.12D0;.z.p;(`sum`count)!2#`ask;{select avgprice: sumAsk%countAsk from x})\" avgprice -------- 44.83632","title":"Postprocessing"},{"location":"dataaccess/#sublist","text":"Use the `sublist key to return the first n rows of a table, for example we get the first 2 rows of the table. q)getdata `tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;00:00+2021.02.17D10;.z.d+18:00;\"sym\";(`max`min)!((`ask`bid);(`ask`bid));enlist(`desc;`maxAsk);2) sym | maxAsk maxBid minAsk minBid ----| --------------------------- AAPL| 171.23 170.36 56.35 55.32 GOOG| 101.09 99.96 45.57 44.47 q).dataaccess.buildquery `tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;00:00+2021.02.17D10;.z.d+18:00;\"sym\";(`max`min)!((`ask`bid);(`ask`bid));enlist(`desc;`maxAsk);2) ? `quote ,(within;`time;2021.02.17D10:00:00.000000000 2021.03.03D18:00:00.000000000) (,`sym)!,`sym `maxAsk`maxBid`minAsk`minBid!((max;`ask);(max;`bid);(min;`ask);(min;`bid)) q)getdata `tablename`starttime`endtime`freeformby`aggregations`ordering`sublist!(`quote;00:00+2021.02.17D10;.z.d+18:00;\"sym\";(`max`min)!((`ask`bid);(`ask`bid));enlist(`desc;`maxAsk);3) sym | maxAsk maxBid minAsk minBid ----| --------------------------- INTC| 68.51 67.56 44.59 43.63 IBM | 48.53 47.61 37.1 36.11 HPQ | 46.09 45.05 29.97 29.03","title":"Sublist"},{"location":"gettingstarted/","text":"Getting Started kdb+ is very customisable. Customisations are contained in q scripts (.q files), which define functions and variables which modify the behaviour of a process. Every q process can load a single q script, or a directory containing q scripts and/or q data files. Hooks are provided to enable the programmer to apply a custom function to each entry point of the process (.z.p*), to be invoked on the timer (.z.ts) or when a variable in the top level namespace is amended (.z.vs). By default none of these hooks are implemented. We provide a codebase and a single main script, torq.q. torq.q is essentially a wrapper for bespoke functionality which can load other scripts/directories, or can be sourced from other scripts. Whenever possible, torq.q should be invoked directly and used to load other scripts as required. torq.q will: ensure the environment is set up correctly; define some common utility functions (such as logging); execute process management tasks, such as discovering the name and type of the process, and re-directing output to log files; load configuration; load the shared code based; set up the message handlers; load any required bespoke scripts. The behavior of torq.q is modified by both command line parameters and configuration. We have tried to keep as much as possible in configuration files, but if the parameter either has a global effect on the process or if it is required to be known before the configuration is read, then it is a command line parameter. Installing TorQ A guide on how to install TorQ using installation script here . Using torq.q torq.q can be invoked directly from the command line and be set to source a specified file or directory. torq.q requires the 5 environment variables to be set (see section envvar). If using a unix environment, this can be done with the setenv.sh script. To start a process in the foreground without having to modify any other files (e.g. process.csv) you need to specify the type and name of the process as parameters. An example is below. $ . setenv.sh $ q torq.q -debug -proctype testproc -procname test1 To specify the parent process type, do: $ q torq.q -debug -parentproctype testparentproc -proctype testproc -procname test1 To load a file, do: $ q torq.q -load myfile.q -debug -proctype testproc -procname test1 It can also be sourced from another script. If this is the case, some of the variables can be overridden, and the usage information can be modified or extended. Any variable that has a definition like below can be overridden from the loading script. myvar:@[value;`myvar;1 2 3] The available command line parameters are: Cmd Line Param Description -procname x -proctype y The process name and process type -parentproctype x The parent process type. Specifying will load in any additional code or configuration that is associated with another process type -procfile x The name of the file to get the process information from -load x [y..z] The files or database directory to load -loaddir x [y..z] Load all .q, .k files in specified directories -localtime Sets processes running in local time rather than GMT for log messages, timer calls etc. The change is backwards compatible; without -localtime flag the process will print logs etc. in GMT but can also have a different .z.P -trap Any errors encountered during initialization when loading external files will be caught and logged, processing will continue -stop Stop loading the file if an error is encountered but do not exit -noredirect Do not redirect std out/std err to a file (useful for debugging) -noredirectalias Do not create an alias for the log files (aliases drop any suffix e.g. timestamp suffix) -noconfig Do not load configuration -nopi Reset the definition of .z.pi to the initial value (useful for debugging) -debug Equivalent to [-nopi -noredirect] -usage Print usage info and exit -onelog Writes all messages to stdout log file, note non-trapped errors will still be written to stderr log file -test x Use for unit testing. Pass the location of tests directory -dataaccess path/to/csv Initialise the Dataaccess API in the process with table properties In addition any process variable in a namespace (.*.*) can be overridden from the command line. Any value supplied on the command line will take priority over any other predefined value (.e.g. in a configuration or wrapper). Variable names should be supplied with full qualification e.g. -.servers.HOPENTIMEOUT 5000. Using torq.sh torq.sh is a script that runs processes in torq with added functionality, one key enhancement is all the process configuration is now in one place. The default process file is located in $KDBCONFIG/process.csv. This script is only available on Linux. It requires environment variables to be set, similar to torq.q. A usage statement for the script can be seen by running the following in a unix environment: ./torq.sh . Environment Variables Five environment variables are required: Environment Variable Description KDBCONFIG The base configuration directory KDBCODE The base code directory KDBLOGS Where standard out/error and usage logs are written KDBHTML Contains HTML files KDBLIB Contains supporting library files torq.q will check for these and exit if they are not set. If torq.q is being sourced from another script, the required environment variables can be extended by setting .proc.envvars before loading torq.q. Process Identification At the crux of AquaQ TorQ is how processes identify themselves. This is defined by two required variables - .proc.proctype and .proc.procname which are the type and name of the process respectively. An optional variable parentproctype allows an inital codebase and configuration to be loaded. The two required values determine the code base and configuration loaded, and how they are connected to by other processes. If both of the required variables are not defined, TorQ will attempt to use the port number a process was started on to determine the code base and configuration loaded. The most important of these is the proctype. It is up to the user to define at what level to specify a process type. For example, in a production environment it would be valid to specify processes of type \u201chdb\u201d (historic database) and \u201crdb\u201d (real time database). It would also be valid to segregate a little more granularly based on approximate functionality, for example \u201chdbEMEA\u201d and \u201chdbAmericas\u201d. In this example it may be sensible to set the parentproctype as \"hdb\" and putting all shared code in the \"hdb\" configuration to be loaded first with the region-specific configuration being loaded after. The actual functionality of a process can be defined more specifically, but this will be discussed later. The procname value is used solely for identification purposes. A process can determine its type and name in a number of ways: From the process file in the default location of $KDBCONFIG/process.csv; From the process file defined using the command line parameter -procfile; From the port number it is started on, by referring to the process file for further process details; Using the command line parameters -proctype and -procname; By defining .proc.proctype and .proc.procname in a script which loads torq.q. For options 4 and 5, both parameters must be defined using that method or neither will be used (the values will be read from the process file). For option 3, TorQ will check the process file for any entries where the port matches the port number it has been started on, and deduce it\u2019s proctype and procname based on this port number and the corresponding hostname entry. The process file has format as below. aquaq$ cat config/process.csv host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd aquaq,9997,rdb,rdb_europe_1,appconfig/passwords/accesslist.txt,1,1,3,,${KDBCODE}/processes/rdb.q,1,,q aquaq,9998,hdb,hdb_europe_1,appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,,q aquaq,9999,hdb,hdb_europe_2,appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,,q The process will read the file and try to identify itself based on the host and port it is started on. The host can either be the value returned by .z.h, or the ip address of the server. If the process can not automatically identify itself it will exit, unless proctype and procname were both passed in as command line parameters. If both of these parameters are passed in then default configuration settings will be used. The parameters following procname set the following: Parameter Description U Authentication requiring a usr:pwd file localtime Sets process running in local time rather than GMT g Garbage collection immediate (1) or deferred (0) T Timeout in seconds for client queries, 0 for no timeout w Workspace MB limit load Files or database directory to load startwithall Determine if process is started when all is specified extras Specify any additional parameters qcmd Allows different versions of q to be used or different command line options - rlwap, numactl Where U/g/T/w are standard q command line arguments and localtime and load are TorQ command line parameters. Running processes using torq.sh torq.sh is able to start or stop processes seperately, in a batch or all at once. Before a process is started/stopped the script will check that the process is not already running before attempting to start/stop a process, a time of when this is executed is printed to screen. $ ./torq.sh start rdb1 hdb1 tickerplant1 15:42:00 | Starting rdb1... 15:42:00 | hdb1 already running 15:42:00 | Starting tickerplant1... $ ./torq.sh stop all 15:46:19 | Shutting down hdb1... 15:46:19 | Shutting down hdb2... A status summary table of all the processes can be printed to screen, the summary provides information on the time the process was checked, process name, status and the port number and PID of that process. $ ./torq.sh summary TIME | PROCESS | STATUS | PORT | PID 11:33:59 | discovery1 | up | 41001 | 14426 11:33:59 | tickerplant1 | down | It is possible to view the underlying start code for all processes. This is useful if another available command line parameter was required for start up. $ ./torq.sh print discovery1 Start line for discovery1: nohup q deploy/torq.q -procname discovery1 -stackid 41000 -proctype discovery -U appconfig/passwords/accesslist.txt -localtime 1 -g 0 -load deploy/code/processes/discovery.q -procfile deploy/appconfig/process.csv </dev/null > deploy/logs/torqdiscovery1.txt 2>&1 & The debug command line parameter can be appended to the start line straight from torq.sh to start a process in debug mode. Note it is only possible to start one process at a time in debug mode. $ ./torq.sh debug tickerplant1 If a process name not present in the process.csv is used, the input process name will return as an invalid input. To see a list of all the processes in the process.csv see below. $ ./torq.sh procs A different process file can be used with this script from the command line. The argument following the csv flag needs to be a full path to the process.csv. $ ./torq.sh start all -csv ${KDBAPPCONFIG}/process.csv To add/override the default values in the g, T, w, or extras column the extras flag can be used in this script. $ ./torq.sh start rdb1 -extras -T 60 -w 4000 $ ./torq.sh start sort1 -extras -s -3 Using the Code Profiler with torq.sh KDB 4.0 includes an experimental built-in call-stack snapshot primitive that allows building a sampling profiler. The profiler uses the new function .Q.prf0 . Requirements and documentation of the new code profiler by kx can be found here . Assuming a process is running, you can run the code below as an example in the command line. Note that this top function currently only allows a single process as an argument and multiple processes is not currently supported. $ ./torq.sh top rdb1 This uses the top.q script given by kx (description found here ) which will show an automatically updated display of functions most heavily contributing to the running time. The display has the following fields: Field Description self the percentage of time spent in the function itself total percentage of time spent in the function including all descendants name the name of the function file the file path where the function is located Logging By default, each process will redirect output to a standard out log and a standard error log, and create aliases for them. These will be rolled at midnight on a daily basis. They are all written to the $KDBLOGS directory. The log files created are: Log File Description out_[procname]_[date].log Timestamped out log err_[procname]_[date].log Timestamped error log out_[procname].log Alias to current log log err_[procname].log Alias to current error log The date suffix can be overridden by modifying the .proc.logtimestamp function and sourcing torq.q from another script. This could, for example, change the suffixing to a full timestamp. In the case where -onelog is flagged TorQ will attempt to redirect all output to the out log file, unfortunately this is not perfect. TorQ uses \\1 and \\2 to redirect stderr and stdout, onelog only overrides handled errors to the \\1 redirect. This is because there are issuses with redirecting both to the same file, (the ordering of messages will be incorrect) the issue is with KDB+ rather than with TorQ. Because of this errors that are raised by KDB+ and unhandled are still directed to the err log file because \\1 and \\2 cannot be redirected to the same file. Configuration Loading Default Configuration Loading Default process configuration is contained in q scripts, and stored in the $KDBCONFIG /settings directory. Each process tries to load all the configuration it can find and will attempt to load three configuration files in the below order:- default.q: default configuration loaded by all processes. In a standard installation this should contain the superset of customisable configuration, including comments. [parentproctype].q: configuration for a specific parent process type (only if parentproctype specified). [proctype].q: configuration for a specific process type. [procname].q: configuration for a specific named process. The only one which should always be present is default.q. Each of the other scripts can contain a subset of the configuration variables, which will override anything loaded previously. Application Configuration Loading Application specific configuration can be stored in a user defined directory and made visible to TorQ by setting the $KDBAPPCONFIG environment variable. If $KDBAPPCONFIG is set, then TorQ will search the $KDBAPPCONFIG/settings directory and load all configuration it can find. Application configuration will be loaded after all default configuration in the following order:- default.q: Application default configuration loaded by all processes. [[parentproctype]]{}.q : Application specific configuration for a specific parent process type (only if parentproctype specified). [[proctype]]{}.q: Application specific configuration for a specific process type. [[procname]]{}.q: Appliction specific configuration for a specific named process. All loaded configuration will override anything loaded previously. None of the above scripts are required to be present and can contain a subset of the default configuration variables from the default configuration directory. All configuration is loaded before code. Application Dependency TorQ will automatically check application version and dependency information. TorQ will check the $KDBAPPCONFIG directory for a dependency.csv file. This file should contain information in the format: app version dependency app0 1.0.0 app1 1.1.1;app2 2.1.0 TorQ will also search the $KDBCONFIG directory for the TorQ dependency.csv file. If any of the dependency versions exceed application versions, TorQ will exit and log the error. If no dependency files are supplied, TorQ will run as normal. However, if only an application dependency file is supplied, TorQ will exit and log the error. Each version number can be up to 5 digits in length, separated by '.' and the current kdb+ version will be automatically added with the format major.minor.yyyy.mm.dd Code Loading Code is loaded from the $KDBCODE directory. There is also a common codebase, a codebase for each process type, and a code base for each process name, contained in the following directories and loaded in this order: $KDBCODE/common: shared codebase loaded by all processes; $KDBCODE/[parentproctype]: code for a specific parent process type (only if parentproctype specified); $KDBCODE/[proctype]: code for a specific process type; $KDBCODE/[procname]: code for a specific process name; For any directory loaded, the load order can be specified by adding order.txt to the directory. order.txt dictates the order that files in the directory are loaded. If a file is not in order.txt, it will still be loaded but after all the files listed in order.txt have been loaded. In addition to loading code form $KDBCODE, application specific code can be saved in a user defined directory with the same structure as above, and made visible to TorQ by setting the $KDBAPPCODE environment variable. If this environment variable is set, TorQ will load codebase in the following order. $KDBCODE/common: shared codebase loaded by all processes; $KDBAPPCODE/common: application specific code shared by all processes; $KDBCODE/[parentproctype]: code for a specific parent process type (only if parentproctype specified); $KDBAPPCODE/[parentproctype]: application specific code for a specific parent process type (only if parentproctype specified); $KDBCODE/[proctype]: code for a specific process type; $KDBAPPCODE/[proctype]: application specific code for a specific process type; $KDBCODE/[procname]: code for a specific process name; $KDBAPPCODE/[procname]: application specific code for a specific process name; Additional directories can be loaded using the -loaddir command line parameter. Initialization Errors Initialization errors can be handled in different ways. The default action is any initialization error causes the process to exit. This is to enable fail-fast type conditions, where it is better for a process to fail entirely and immediately than to start up in an indeterminate state. This can be overridden with the -trap or -stop command line parameters. With -trap, the process will catch the error, log it, and continue. This is useful if, for example, the error is encountered loading a file of stored procedures which may not be invoked and can be reloaded later. With -stop the process will halt at the point of the error but will not exit. Both -stop and -trap are useful for debugging.","title":"Getting Started"},{"location":"gettingstarted/#getting-started","text":"kdb+ is very customisable. Customisations are contained in q scripts (.q files), which define functions and variables which modify the behaviour of a process. Every q process can load a single q script, or a directory containing q scripts and/or q data files. Hooks are provided to enable the programmer to apply a custom function to each entry point of the process (.z.p*), to be invoked on the timer (.z.ts) or when a variable in the top level namespace is amended (.z.vs). By default none of these hooks are implemented. We provide a codebase and a single main script, torq.q. torq.q is essentially a wrapper for bespoke functionality which can load other scripts/directories, or can be sourced from other scripts. Whenever possible, torq.q should be invoked directly and used to load other scripts as required. torq.q will: ensure the environment is set up correctly; define some common utility functions (such as logging); execute process management tasks, such as discovering the name and type of the process, and re-directing output to log files; load configuration; load the shared code based; set up the message handlers; load any required bespoke scripts. The behavior of torq.q is modified by both command line parameters and configuration. We have tried to keep as much as possible in configuration files, but if the parameter either has a global effect on the process or if it is required to be known before the configuration is read, then it is a command line parameter.","title":"Getting Started"},{"location":"gettingstarted/#installing-torq","text":"A guide on how to install TorQ using installation script here .","title":"Installing TorQ"},{"location":"gettingstarted/#using-torqq","text":"torq.q can be invoked directly from the command line and be set to source a specified file or directory. torq.q requires the 5 environment variables to be set (see section envvar). If using a unix environment, this can be done with the setenv.sh script. To start a process in the foreground without having to modify any other files (e.g. process.csv) you need to specify the type and name of the process as parameters. An example is below. $ . setenv.sh $ q torq.q -debug -proctype testproc -procname test1 To specify the parent process type, do: $ q torq.q -debug -parentproctype testparentproc -proctype testproc -procname test1 To load a file, do: $ q torq.q -load myfile.q -debug -proctype testproc -procname test1 It can also be sourced from another script. If this is the case, some of the variables can be overridden, and the usage information can be modified or extended. Any variable that has a definition like below can be overridden from the loading script. myvar:@[value;`myvar;1 2 3] The available command line parameters are: Cmd Line Param Description -procname x -proctype y The process name and process type -parentproctype x The parent process type. Specifying will load in any additional code or configuration that is associated with another process type -procfile x The name of the file to get the process information from -load x [y..z] The files or database directory to load -loaddir x [y..z] Load all .q, .k files in specified directories -localtime Sets processes running in local time rather than GMT for log messages, timer calls etc. The change is backwards compatible; without -localtime flag the process will print logs etc. in GMT but can also have a different .z.P -trap Any errors encountered during initialization when loading external files will be caught and logged, processing will continue -stop Stop loading the file if an error is encountered but do not exit -noredirect Do not redirect std out/std err to a file (useful for debugging) -noredirectalias Do not create an alias for the log files (aliases drop any suffix e.g. timestamp suffix) -noconfig Do not load configuration -nopi Reset the definition of .z.pi to the initial value (useful for debugging) -debug Equivalent to [-nopi -noredirect] -usage Print usage info and exit -onelog Writes all messages to stdout log file, note non-trapped errors will still be written to stderr log file -test x Use for unit testing. Pass the location of tests directory -dataaccess path/to/csv Initialise the Dataaccess API in the process with table properties In addition any process variable in a namespace (.*.*) can be overridden from the command line. Any value supplied on the command line will take priority over any other predefined value (.e.g. in a configuration or wrapper). Variable names should be supplied with full qualification e.g. -.servers.HOPENTIMEOUT 5000.","title":"Using torq.q"},{"location":"gettingstarted/#using-torqsh","text":"torq.sh is a script that runs processes in torq with added functionality, one key enhancement is all the process configuration is now in one place. The default process file is located in $KDBCONFIG/process.csv. This script is only available on Linux. It requires environment variables to be set, similar to torq.q. A usage statement for the script can be seen by running the following in a unix environment: ./torq.sh .","title":"Using torq.sh"},{"location":"gettingstarted/#environment-variables","text":"Five environment variables are required: Environment Variable Description KDBCONFIG The base configuration directory KDBCODE The base code directory KDBLOGS Where standard out/error and usage logs are written KDBHTML Contains HTML files KDBLIB Contains supporting library files torq.q will check for these and exit if they are not set. If torq.q is being sourced from another script, the required environment variables can be extended by setting .proc.envvars before loading torq.q.","title":"Environment Variables"},{"location":"gettingstarted/#process-identification","text":"At the crux of AquaQ TorQ is how processes identify themselves. This is defined by two required variables - .proc.proctype and .proc.procname which are the type and name of the process respectively. An optional variable parentproctype allows an inital codebase and configuration to be loaded. The two required values determine the code base and configuration loaded, and how they are connected to by other processes. If both of the required variables are not defined, TorQ will attempt to use the port number a process was started on to determine the code base and configuration loaded. The most important of these is the proctype. It is up to the user to define at what level to specify a process type. For example, in a production environment it would be valid to specify processes of type \u201chdb\u201d (historic database) and \u201crdb\u201d (real time database). It would also be valid to segregate a little more granularly based on approximate functionality, for example \u201chdbEMEA\u201d and \u201chdbAmericas\u201d. In this example it may be sensible to set the parentproctype as \"hdb\" and putting all shared code in the \"hdb\" configuration to be loaded first with the region-specific configuration being loaded after. The actual functionality of a process can be defined more specifically, but this will be discussed later. The procname value is used solely for identification purposes. A process can determine its type and name in a number of ways: From the process file in the default location of $KDBCONFIG/process.csv; From the process file defined using the command line parameter -procfile; From the port number it is started on, by referring to the process file for further process details; Using the command line parameters -proctype and -procname; By defining .proc.proctype and .proc.procname in a script which loads torq.q. For options 4 and 5, both parameters must be defined using that method or neither will be used (the values will be read from the process file). For option 3, TorQ will check the process file for any entries where the port matches the port number it has been started on, and deduce it\u2019s proctype and procname based on this port number and the corresponding hostname entry. The process file has format as below. aquaq$ cat config/process.csv host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd aquaq,9997,rdb,rdb_europe_1,appconfig/passwords/accesslist.txt,1,1,3,,${KDBCODE}/processes/rdb.q,1,,q aquaq,9998,hdb,hdb_europe_1,appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,,q aquaq,9999,hdb,hdb_europe_2,appconfig/passwords/accesslist.txt,1,1,60,4000,${KDBHDB},1,,q The process will read the file and try to identify itself based on the host and port it is started on. The host can either be the value returned by .z.h, or the ip address of the server. If the process can not automatically identify itself it will exit, unless proctype and procname were both passed in as command line parameters. If both of these parameters are passed in then default configuration settings will be used. The parameters following procname set the following: Parameter Description U Authentication requiring a usr:pwd file localtime Sets process running in local time rather than GMT g Garbage collection immediate (1) or deferred (0) T Timeout in seconds for client queries, 0 for no timeout w Workspace MB limit load Files or database directory to load startwithall Determine if process is started when all is specified extras Specify any additional parameters qcmd Allows different versions of q to be used or different command line options - rlwap, numactl Where U/g/T/w are standard q command line arguments and localtime and load are TorQ command line parameters.","title":"Process Identification"},{"location":"gettingstarted/#running-processes-using-torqsh","text":"torq.sh is able to start or stop processes seperately, in a batch or all at once. Before a process is started/stopped the script will check that the process is not already running before attempting to start/stop a process, a time of when this is executed is printed to screen. $ ./torq.sh start rdb1 hdb1 tickerplant1 15:42:00 | Starting rdb1... 15:42:00 | hdb1 already running 15:42:00 | Starting tickerplant1... $ ./torq.sh stop all 15:46:19 | Shutting down hdb1... 15:46:19 | Shutting down hdb2... A status summary table of all the processes can be printed to screen, the summary provides information on the time the process was checked, process name, status and the port number and PID of that process. $ ./torq.sh summary TIME | PROCESS | STATUS | PORT | PID 11:33:59 | discovery1 | up | 41001 | 14426 11:33:59 | tickerplant1 | down | It is possible to view the underlying start code for all processes. This is useful if another available command line parameter was required for start up. $ ./torq.sh print discovery1 Start line for discovery1: nohup q deploy/torq.q -procname discovery1 -stackid 41000 -proctype discovery -U appconfig/passwords/accesslist.txt -localtime 1 -g 0 -load deploy/code/processes/discovery.q -procfile deploy/appconfig/process.csv </dev/null > deploy/logs/torqdiscovery1.txt 2>&1 & The debug command line parameter can be appended to the start line straight from torq.sh to start a process in debug mode. Note it is only possible to start one process at a time in debug mode. $ ./torq.sh debug tickerplant1 If a process name not present in the process.csv is used, the input process name will return as an invalid input. To see a list of all the processes in the process.csv see below. $ ./torq.sh procs A different process file can be used with this script from the command line. The argument following the csv flag needs to be a full path to the process.csv. $ ./torq.sh start all -csv ${KDBAPPCONFIG}/process.csv To add/override the default values in the g, T, w, or extras column the extras flag can be used in this script. $ ./torq.sh start rdb1 -extras -T 60 -w 4000 $ ./torq.sh start sort1 -extras -s -3","title":"Running processes using torq.sh"},{"location":"gettingstarted/#using-the-code-profiler-with-torqsh","text":"KDB 4.0 includes an experimental built-in call-stack snapshot primitive that allows building a sampling profiler. The profiler uses the new function .Q.prf0 . Requirements and documentation of the new code profiler by kx can be found here . Assuming a process is running, you can run the code below as an example in the command line. Note that this top function currently only allows a single process as an argument and multiple processes is not currently supported. $ ./torq.sh top rdb1 This uses the top.q script given by kx (description found here ) which will show an automatically updated display of functions most heavily contributing to the running time. The display has the following fields: Field Description self the percentage of time spent in the function itself total percentage of time spent in the function including all descendants name the name of the function file the file path where the function is located","title":"Using the Code Profiler with torq.sh"},{"location":"gettingstarted/#logging","text":"By default, each process will redirect output to a standard out log and a standard error log, and create aliases for them. These will be rolled at midnight on a daily basis. They are all written to the $KDBLOGS directory. The log files created are: Log File Description out_[procname]_[date].log Timestamped out log err_[procname]_[date].log Timestamped error log out_[procname].log Alias to current log log err_[procname].log Alias to current error log The date suffix can be overridden by modifying the .proc.logtimestamp function and sourcing torq.q from another script. This could, for example, change the suffixing to a full timestamp. In the case where -onelog is flagged TorQ will attempt to redirect all output to the out log file, unfortunately this is not perfect. TorQ uses \\1 and \\2 to redirect stderr and stdout, onelog only overrides handled errors to the \\1 redirect. This is because there are issuses with redirecting both to the same file, (the ordering of messages will be incorrect) the issue is with KDB+ rather than with TorQ. Because of this errors that are raised by KDB+ and unhandled are still directed to the err log file because \\1 and \\2 cannot be redirected to the same file.","title":"Logging"},{"location":"gettingstarted/#configuration-loading","text":"","title":"Configuration Loading"},{"location":"gettingstarted/#default-configuration-loading","text":"Default process configuration is contained in q scripts, and stored in the $KDBCONFIG /settings directory. Each process tries to load all the configuration it can find and will attempt to load three configuration files in the below order:- default.q: default configuration loaded by all processes. In a standard installation this should contain the superset of customisable configuration, including comments. [parentproctype].q: configuration for a specific parent process type (only if parentproctype specified). [proctype].q: configuration for a specific process type. [procname].q: configuration for a specific named process. The only one which should always be present is default.q. Each of the other scripts can contain a subset of the configuration variables, which will override anything loaded previously.","title":"Default Configuration Loading"},{"location":"gettingstarted/#application-configuration-loading","text":"Application specific configuration can be stored in a user defined directory and made visible to TorQ by setting the $KDBAPPCONFIG environment variable. If $KDBAPPCONFIG is set, then TorQ will search the $KDBAPPCONFIG/settings directory and load all configuration it can find. Application configuration will be loaded after all default configuration in the following order:- default.q: Application default configuration loaded by all processes. [[parentproctype]]{}.q : Application specific configuration for a specific parent process type (only if parentproctype specified). [[proctype]]{}.q: Application specific configuration for a specific process type. [[procname]]{}.q: Appliction specific configuration for a specific named process. All loaded configuration will override anything loaded previously. None of the above scripts are required to be present and can contain a subset of the default configuration variables from the default configuration directory. All configuration is loaded before code.","title":"Application Configuration Loading"},{"location":"gettingstarted/#application-dependency","text":"TorQ will automatically check application version and dependency information. TorQ will check the $KDBAPPCONFIG directory for a dependency.csv file. This file should contain information in the format: app version dependency app0 1.0.0 app1 1.1.1;app2 2.1.0 TorQ will also search the $KDBCONFIG directory for the TorQ dependency.csv file. If any of the dependency versions exceed application versions, TorQ will exit and log the error. If no dependency files are supplied, TorQ will run as normal. However, if only an application dependency file is supplied, TorQ will exit and log the error. Each version number can be up to 5 digits in length, separated by '.' and the current kdb+ version will be automatically added with the format major.minor.yyyy.mm.dd","title":"Application Dependency"},{"location":"gettingstarted/#code-loading","text":"Code is loaded from the $KDBCODE directory. There is also a common codebase, a codebase for each process type, and a code base for each process name, contained in the following directories and loaded in this order: $KDBCODE/common: shared codebase loaded by all processes; $KDBCODE/[parentproctype]: code for a specific parent process type (only if parentproctype specified); $KDBCODE/[proctype]: code for a specific process type; $KDBCODE/[procname]: code for a specific process name; For any directory loaded, the load order can be specified by adding order.txt to the directory. order.txt dictates the order that files in the directory are loaded. If a file is not in order.txt, it will still be loaded but after all the files listed in order.txt have been loaded. In addition to loading code form $KDBCODE, application specific code can be saved in a user defined directory with the same structure as above, and made visible to TorQ by setting the $KDBAPPCODE environment variable. If this environment variable is set, TorQ will load codebase in the following order. $KDBCODE/common: shared codebase loaded by all processes; $KDBAPPCODE/common: application specific code shared by all processes; $KDBCODE/[parentproctype]: code for a specific parent process type (only if parentproctype specified); $KDBAPPCODE/[parentproctype]: application specific code for a specific parent process type (only if parentproctype specified); $KDBCODE/[proctype]: code for a specific process type; $KDBAPPCODE/[proctype]: application specific code for a specific process type; $KDBCODE/[procname]: code for a specific process name; $KDBAPPCODE/[procname]: application specific code for a specific process name; Additional directories can be loaded using the -loaddir command line parameter.","title":"Code Loading"},{"location":"gettingstarted/#initialization-errors","text":"Initialization errors can be handled in different ways. The default action is any initialization error causes the process to exit. This is to enable fail-fast type conditions, where it is better for a process to fail entirely and immediately than to start up in an indeterminate state. This can be overridden with the -trap or -stop command line parameters. With -trap, the process will catch the error, log it, and continue. This is useful if, for example, the error is encountered loading a file of stored procedures which may not be invoked and can be reloaded later. With -stop the process will halt at the point of the error but will not exit. Both -stop and -trap are useful for debugging.","title":"Initialization Errors"},{"location":"handlers/","text":"Message Handlers There is a separate code directory containing message handler customizations. This is found at $KDBCODE/handlers. Much of the code is derived from Simon Garland\u2019s contributions to code.kx . Every external interaction with a process goes through a message handler, and these can be modified to, for example, log or restrict access. Passing through a bespoke function defined in a message handler will add extra processing time and therefore latency to the message. All the customizations we have provided aim to minimise additional latency, but if a bespoke process is latency sensitive then some or all of the customizations could be switched off. We would argue though that generally it is better to switch on all the message handler functions which provide diagnostic information, as for most non-latency sensitive processes (HDBs, Gateways, some RDBs etc.) the extra information upon failure is worth the cost. The message handlers can be globally switched off by setting .proc.loadhandlers to 0b in the configuration file. Script NS Diag Function Modifies logusage.q .usage Y Log all client interaction to an ascii log file and/or in-memory table. Messages can be logged before and after they are processed. Timer calls are also logged. Exclusion function list can be applied to .z.ps to disable logging of asynchronous real time updates pw, po, pg, ps, pc, ws, ph, pp, pi, exit, timer controlaccess.q .access N Restrict access for set of users/user groups to a list of functions, and from a defined set of servers pw, pg, ps, ws, ph, pp, pi trackclients.q .clients Y Track client process details including then number of requests and cumulative data size returned po, pg, ps, ws, pc trackservers.q .servers Y Discover and track server processes including name, type and attribute information. This also contains the core of the code which can be used in conjunction with the discovery service. pc, timer zpsignore.q .zpsignore N Override async message handler based on certain message patterns ps writeaccess.q .readonly N Restrict client write access to prevent any modification to data in place. Also disables all HTTP access. pg, ps, ws, ph, pp ldap.q .ldap N Restrict client access to process using ldap authentication. pw Each customization can be turned on or off individually from the configuration file(s). Each script can be extensively customised using the configuration file. Example customization for logusage.q, taken from $KDBCONFIG/settings/default.q is below. Please see default.q for the remaining configuration of the other message handler files. /- Configuration used by the usage functions - logging of client interaction \\d .usage enabled:1b /- whether the usage logging is enabled logtodisk:1b /- whether to log to disk or not logtomemory:1b /- write query logs to memory ignore:1b /- check the ignore list for functions to ignore ignorelist:(`upd;\"upd\") /- the list of functions to ignore in async calls flushtime:1D00 /- default value for how long to persist the /- in-memory logs. Set to 0D for no flushing suppressalias:0b /- whether to suppress the log file alias creation logtimestamp:{[].z.d} /- function to generate the log file timestamp suffix LEVEL:3 /- log level. 0=none;1=errors;2=errors+complete /- queries;3=errors+before a query+after logroll:1b /- Whether or not to roll the log file /- automatically (on a daily schedule) dotz.q Stores all the default values for the message handlers and can be used to revert back to the default if necessary. logusage.q logusage.q is probably the most important of the scripts from a diagnostic perspective. It is a modified version of the logusage.q script on code.kx. In its most verbose mode it will log information to an in-memory table (.usage.usage) and an on-disk ASCII file, both before and after every client interaction and function executed on the timer. These choices were made because: logging to memory enables easy interrogation of client interaction; logging to disk allows persistence if the process fails or locks up. ASCII text files allow interrogation using OS tools such as vi, grep or tail; logging before a query ensures any query that adversely effects the process is definitely captured, as well as capturing some state information before the query execution; logging after a query captures the time taken, result set size and resulting state; logging timer calls ensures a full history of what the process is actually doing. Also, timer call performance degradation over time is a common source of problems in kdb+ systems. The following fields are logged in .usage.usage: Field Description time Time the row was added to the table id ID of the query. Normally before and complete rows will be consecutive but it might not be the case if the incoming call invokes further external communication timer Execution time. Null for rows with status=b (before) zcmd .z handler the query arrived through status Query status. One of b, c or e (before, complete, error) a Address of sender. .dotz.ipa can be used to convert from the integer format to a hostname u Username of sender w Handle of sender cmd Command sent mem Memory statistics sz Size of result. Null for rows with status of b or e error Error message controlaccess.q controlaccess.q is used to restrict client access to the process. It is modified version of controlaccess.q from code.kx. The script allows control of several aspects: the host/ip address of the servers which are allowed to access the process; definition of three user groups (default, poweruser and superuser) and the actions each group is allowed to do; the group(s) each user is a member of, and any additional actions an individual user is allowed/disallowed outside of the group permissions; the maximum size of the result set returned to a client. The access restrictions are loaded from csv files. The permissions files are stored in $KDBCONFIG/permissions. File Description *_hosts.csv Contains hostname and ip address (patterns) for servers which are allowed or disallowed access. If a server is not found in the list, it is disallowed *_users.csv Contains individual users and the user groups they are are a member of *_functions.csv Contains individual functions and whether each user group is allowed to execute them. ; separated user list enables functions to be allowed by individual users The permissions files are loaded using a similar hierarchical approach as for the configuration and code loading. Three files can be provided- default_.csv, [proctype]_.csv, and [procname]_.csv. All of the files will be loaded, but permissions for the same entity (hostpattern, user, or function) defined in [procname]_.csv will override those in [proctype]_.csv which will in turn override [procname]_.csv. When a client makes a query which is refused by the permissioning layer, an error will be raised and logged in .usuage.usage if it is enabled. trackclients.q trackclients.q is used to track client interaction. It is a slightly modified version of trackclients.q from code.kx, and extends the functionality to handle interaction with the discovery service. Whenever a client opens a connection to the q process, it will be registered in the .clients.clients table. Various details are logged, but from a diagnostic perspective the most important information are the client details, the number of queries it has run, the last time it ran a query, the number of failed queries and the cumulative size of results returned to it. trackservers.q trackservers.q is used to register and maintain handles to external servers. It is a heavily modified version of trackservers.q from code.kx. It is explained more in section connectionmanagement. zpsignore.q zpsignore.q is used to check incoming async calls for certain patterns and to bypass all further message handler checks for messages matching the pattern. This is useful for handling update messages published to a process from a data source. writeaccess.q writeaccess.q is used to restrict client write access to data within a process. The script uses the reval function, released in KDB+ 3.3, to prevent client queries from modifying any data in place. At present only queries in the form of strings are passed through the reval function. Additonally the script disables any form of HTTP access. If using versions of KDB+ prior to 3.3, this feature must be disabled. An attempt to use this feature on previous KDB+ versions will result in an error and the relevant process exiting. permissions.q permissions.q is used to control client access to a server process. It allows: Access control via username/password access, either in combination with the -u/U process flags or in place of them. Definition of user groups, which control variable access. Definition of user roles, which allow control over function execution. Deeper control over table subsetting through the use of \u201cvirtual tables\u201d, using enforced where clauses. Access restriction in TorQ can be enabled on all processes, each of which can then load the default.q in $KDBCONFIG/permissions/, which adds users, groups and roles allowing standard operation of TorQ. The admin user and role by default can access all functions, and each of the system processes has access only to the required system functions. Permissions are enabled or disabled on a per-process basis through setting .pm.enabled as 1b or 0b at process load (set to 0b by default). A permissioned process can safely interact with a non-permissioned process while still controlling access to itself. The access schema consists of 7 control tables: Name Descriptions user Username, locality, encryption type and password hash usergroup User and their group. userrole User and role. functiongroup Functions and their group function Function names, the roles which can access them, and a lambda checking the parameters those roles can use. access Variable names, the groups which can access them, and the read or write access level. virtualtable Virtual table name, main table name, and the where clause it enforces on access to that table. In addition to groupinfo and roleinfo tables, which contain the group/role name and a string describing each group and role. A user can belong to multiple groups, and have multiple roles. In particular the schema supports group hierarchy, where a user group can be listed as a user in the group table, and inherit all the permissions from another other group, effectively inheriting the second group itself. Note that a group cannot have the same name as an individual user. A user belonging to a group listed in the access table will have the specified level of access (read or write) to that group\u2019s variables, e.g. Table Group Level quote headtrader write trade juniortrader read Here, users in headtrader will have write access to the quote table, while juniortrader group has read access to the trade table. If headtraders have been set to inherit the juniortrader group, they will also have read access to trade. Note that read access is distinct from write access. Headtraders in this circumstance do not have implicit read access to the quote table. This control is for direct name access only. Selects, execs and updates are controlled via the function table, as below. The permissions script can be set to have permissive mode enabled with permissivemode:1b (disabled by default). When enabled at script loading, this bypasses access checks on variables which are not listed in the access table, effectively auto-allowlisting any variables not listed in the access table for all users, which may be useful in partly restricted development environments. Function access is controlled through non-hierarchical roles. A user attempting to run a named function will have their access checked against the function table through their role, for example, trying to run a function timedata[syms;bkttype], which selects from a table by a time bucket type bkttype on xbar: Function Role Param. Check timedata quant {1b} timedata normal user {x[`bkttype] in `hh} select quant {1b} The parameter check in the third column must be a lambda accepting a dictionary of parameters and their values, which can then return a boolean if some parameter condition is met. Here, any normal user must have their bucket type as an hour. If they try anything else, the function is not permitted. This could be extended to restriction to certain syms as well, in this example, the quant can run this function with any parameters. Anything passed to the param. check function returns 1b. A quant having general select access is listed as having 1b in the param. check. Further restriction of data can be achieved with virtual tables, via which users can be restricted to having a certain subset of data from a main table available. To avoid the need to replicate a potentially large subset of a table into a separately-controlled variable, this is done through pointing to the table under a different name via a where clause, e.g. Virtual Table Table Where Clause trade_lse trade ,(in;`src;\u201cL\u201d) quote_new quote ,(>;`time;(-;`.z.p;01:00)) When a select from trade_lse is performed, a select on trade is modified to contain the where clause above. Access to virtual tables can be controlled identically to access to real tables through the access table. If the process is given the flag \u201c-public 1\u201d, it will run in public access mode. This allows a user to log in without a password and be given the publicuser role and membership of the public group, which can be configured as any other group or role. The permissions control has a default size restriction of 2GB, set (as bytes) on .pm.maxsize. This is a global restriction and is not affected by user permissions. Adding to the groups and roles is handled by the functions: adduser[`user;`locality;`hash type; md5\"password\"] removeuser[`user] addgroup[`groupname; \"description\"] removegroup[`groupname] addrole[`rolename; \"description\"] removerole[`rolename] addtogroup[`user;`groupname] removefromgroup[`user; `groupname] assignrole[`user; `rolename] unassignrole[`user; `rolename] addfunction[`function; `functiongroup] removefunction[`function; `functiongroup] grantaccess[`variable; `groupname; `level] revokeaccess[`variable; `groupname; `level] grantfunction[`function; `rolename; {paramCheckFn}] revokefunction[`function; `rolename] createvirtualtable[`vtablename; `table; ,(whereclause)] removevirtualtable[`vtablename] cloneuser[`user;`newuser;\"password\"] which are further explained in the script API. Permission control operates identically on the gateway. A user connected to the gateway must have access to the gateway, and their roles must have access to the .gw.syncexec or .gw.asyncexec functions. Usage Example To connect to a permissioned RDB in the TorQ system, a group and role for the user must be established. If the RDB contains the tables trade, quote, and depth, and the process contains the functions getdata[syms, bkttype,bktsize] and hloc[table], restricted access would be configured like so: .pm.adduser[`adam;`local;`md5;md5\"pass\"] .pm.adduser[`bob;`local;`md5;md5\"pass\"] .pm.addtogroup[`adam;`fulluser] .pm.addtogroup[`bob;`partuser] .pm.addtogroup[`fulluser;`partuser] .pm.grantaccess[`quote;`fulluser;`read] .pm.grantaccess[`trade;`partuser;`read] .pm.createvirtualtable[`quotenew;`quote;enlist(>;`time;(-;`.z.p;01:00))] .pm.grantaccess[`quotenew;`partuser;`read] .pm.assignrole[`adam;`toplevel] .pm.assignrole[`bob;`lowlevel] .pm.grantfunction[`getdata;`toplevel;{1b}] .pm.grantfunction[`getdata;`lowlevel;{x[`syms] in `GOOG}] .pm.grantfunction[`hloc;`toplevel;{1b}] .pm.grantfunction[`hloc;`lowlevel;{x[`table] in `trade}] This provides a system in which Bob can access only the trade table, while Adam has access to the trade table and quote table (through inheritance from Bob\u2019s group). Through a virtual table, if Bob runs \u201cselect from quotenew\u201d, he is able to get a table of the last hour of quotes. When the system is started in normal mode, there is no IPC access to the depth table, however if the system was started in permissive mode, in this case any user who could log in could access depth. Adam can run the getdata function however he wants, and Bob can only run it against sym GOOG. Similarly Adam can run hloc against any table, but Bob can only look at trade with it. Additionally, any system calls would need to be actively permissioned in the same way, after defining a systemuser role (or expanding the default role in TorQ). The superuser is given global function access by assigning them .pm.ALL in the function table, for example a tickerplant pushing to the RDB would need to have a user and role defined: .pm.adduser[`ticker;`local;`md5;md5\"plant\"] .pm.assignrole[`ticker;`tp] And then grant that role access to the .u.upd function: .pm.grantfunction[`.u.upd;`tp;{1b}] Although the .u.upd function updates to a table, there is no need to grant direct access to that table. Gateway Example The gateway user will have superuser role by default. The execution of a function passed through the gateway is checked against the user who sent the call. This should not be modified. Within the gateway itself, access to target processes can be controlled via the function table. For example, if Adam in the previous example was allowed to access only the RDB with .gw.syncexec, you could use: .pm.grantfunction[`.gw.syncexec;`toplevel;{x[`1] in `rdb}] Since .gw.syncexec is a projection, the arguments supplied are checked in order, with dictionary keys `0`1`2... etc. This could be further extended to restrict access to queries with the .pm.allowed[user;query] function, which checks permissions of the current user as listed on the gateway permission tables: .pm.grantfunction[`.gw.syncexec;`toplevel; {.pm.allowed[.z.u;x[`0]] and x[`1] in `rdb}] ldap.q Authentication with an ldap server is managed with ldap.q. It allows: A user to authenticate against an ldap server; Caching of user attempts to allow reauthentication without server if within checktime period; Users to be blocked if too many failed authentication attempts are made. Default parameters in the ldap namespace are set in {TORQHOME}/config/settings/default.q. parameter description enabled Whether ldap authentication is enabled debug Whether logging message are written to console server Host for ldap server. port Port number for ldap server. version Ldap version number. blocktime Time that must elapse before a blocked user can attempt to authenticate. If set to 0Np then the user is permanently blocked until an admin unblocks them. checklimit Login attempts before user is blocked. checktime Period of time that allows user to reauthenticate without confirming with ldap server. buildDNsuf Suffix for building distinguished name. buildDN Function to build distiniguished name. To get started the following will need altered from their default values: enabled, port, server, buildDNsuf. The value buildDNsuf is required to build a users bind_dn from the supplied username and is called by the function buildDN. An example definition is: .ldap.buildDNsuf:\"ou=users,dc=website,dc=com\"; Authentication is handled by .ldap.authenticate which is wrapped by .ldap.login, which is in turn wrapped by .z.pw when ldap authentication is enabled. When invoked .ldap.login retrieves the users latest authentication attempt from the cache, if it exists, and performs several checks before authenticating the user. To authenticate the function first checks whether the user has been blocked by reaching the checklimit and blocktime has not passed, immediately returning false if this is the case. If the user has previously successfully authenticated within the period defined by checktime and is using the same credentials authentication will be permitted. For all other cases an authentication attempt will be made against the ldap server. Example authentication attempt: .ldap.login[`user;pass] 0b To manually unblock a user the function .ldap.unblock must be passed their userame as a symbol. The function checks the cache to see whether a user is blocked and will reset the blocked status if necessary. An example usage of this function is: .ldap.unblock[`user] Diagnostic Reporting The message handler modifications provide a wealth of diagnostic information including: the timings and memory usage for every query run on a process; failed queries; clients trying to do things they are not permissioned for; the clients which are querying often and/or regularly extracting large datasets; the number of clients currently connected; timer calls and how long they take. Although not currently implemented, it would be straightforward to use this information to implement reports on the behaviour of each process and the overall health of the system. Similarly it would be straightforward to set up periodic publication to a central repository to have a single point for system diagnostic statistics.","title":"Handlers"},{"location":"handlers/#message-handlers","text":"There is a separate code directory containing message handler customizations. This is found at $KDBCODE/handlers. Much of the code is derived from Simon Garland\u2019s contributions to code.kx . Every external interaction with a process goes through a message handler, and these can be modified to, for example, log or restrict access. Passing through a bespoke function defined in a message handler will add extra processing time and therefore latency to the message. All the customizations we have provided aim to minimise additional latency, but if a bespoke process is latency sensitive then some or all of the customizations could be switched off. We would argue though that generally it is better to switch on all the message handler functions which provide diagnostic information, as for most non-latency sensitive processes (HDBs, Gateways, some RDBs etc.) the extra information upon failure is worth the cost. The message handlers can be globally switched off by setting .proc.loadhandlers to 0b in the configuration file. Script NS Diag Function Modifies logusage.q .usage Y Log all client interaction to an ascii log file and/or in-memory table. Messages can be logged before and after they are processed. Timer calls are also logged. Exclusion function list can be applied to .z.ps to disable logging of asynchronous real time updates pw, po, pg, ps, pc, ws, ph, pp, pi, exit, timer controlaccess.q .access N Restrict access for set of users/user groups to a list of functions, and from a defined set of servers pw, pg, ps, ws, ph, pp, pi trackclients.q .clients Y Track client process details including then number of requests and cumulative data size returned po, pg, ps, ws, pc trackservers.q .servers Y Discover and track server processes including name, type and attribute information. This also contains the core of the code which can be used in conjunction with the discovery service. pc, timer zpsignore.q .zpsignore N Override async message handler based on certain message patterns ps writeaccess.q .readonly N Restrict client write access to prevent any modification to data in place. Also disables all HTTP access. pg, ps, ws, ph, pp ldap.q .ldap N Restrict client access to process using ldap authentication. pw Each customization can be turned on or off individually from the configuration file(s). Each script can be extensively customised using the configuration file. Example customization for logusage.q, taken from $KDBCONFIG/settings/default.q is below. Please see default.q for the remaining configuration of the other message handler files. /- Configuration used by the usage functions - logging of client interaction \\d .usage enabled:1b /- whether the usage logging is enabled logtodisk:1b /- whether to log to disk or not logtomemory:1b /- write query logs to memory ignore:1b /- check the ignore list for functions to ignore ignorelist:(`upd;\"upd\") /- the list of functions to ignore in async calls flushtime:1D00 /- default value for how long to persist the /- in-memory logs. Set to 0D for no flushing suppressalias:0b /- whether to suppress the log file alias creation logtimestamp:{[].z.d} /- function to generate the log file timestamp suffix LEVEL:3 /- log level. 0=none;1=errors;2=errors+complete /- queries;3=errors+before a query+after logroll:1b /- Whether or not to roll the log file /- automatically (on a daily schedule)","title":"Message Handlers"},{"location":"handlers/#dotzq","text":"Stores all the default values for the message handlers and can be used to revert back to the default if necessary.","title":"dotz.q"},{"location":"handlers/#logusageq","text":"logusage.q is probably the most important of the scripts from a diagnostic perspective. It is a modified version of the logusage.q script on code.kx. In its most verbose mode it will log information to an in-memory table (.usage.usage) and an on-disk ASCII file, both before and after every client interaction and function executed on the timer. These choices were made because: logging to memory enables easy interrogation of client interaction; logging to disk allows persistence if the process fails or locks up. ASCII text files allow interrogation using OS tools such as vi, grep or tail; logging before a query ensures any query that adversely effects the process is definitely captured, as well as capturing some state information before the query execution; logging after a query captures the time taken, result set size and resulting state; logging timer calls ensures a full history of what the process is actually doing. Also, timer call performance degradation over time is a common source of problems in kdb+ systems. The following fields are logged in .usage.usage: Field Description time Time the row was added to the table id ID of the query. Normally before and complete rows will be consecutive but it might not be the case if the incoming call invokes further external communication timer Execution time. Null for rows with status=b (before) zcmd .z handler the query arrived through status Query status. One of b, c or e (before, complete, error) a Address of sender. .dotz.ipa can be used to convert from the integer format to a hostname u Username of sender w Handle of sender cmd Command sent mem Memory statistics sz Size of result. Null for rows with status of b or e error Error message","title":"logusage.q"},{"location":"handlers/#controlaccessq","text":"controlaccess.q is used to restrict client access to the process. It is modified version of controlaccess.q from code.kx. The script allows control of several aspects: the host/ip address of the servers which are allowed to access the process; definition of three user groups (default, poweruser and superuser) and the actions each group is allowed to do; the group(s) each user is a member of, and any additional actions an individual user is allowed/disallowed outside of the group permissions; the maximum size of the result set returned to a client. The access restrictions are loaded from csv files. The permissions files are stored in $KDBCONFIG/permissions. File Description *_hosts.csv Contains hostname and ip address (patterns) for servers which are allowed or disallowed access. If a server is not found in the list, it is disallowed *_users.csv Contains individual users and the user groups they are are a member of *_functions.csv Contains individual functions and whether each user group is allowed to execute them. ; separated user list enables functions to be allowed by individual users The permissions files are loaded using a similar hierarchical approach as for the configuration and code loading. Three files can be provided- default_.csv, [proctype]_.csv, and [procname]_.csv. All of the files will be loaded, but permissions for the same entity (hostpattern, user, or function) defined in [procname]_.csv will override those in [proctype]_.csv which will in turn override [procname]_.csv. When a client makes a query which is refused by the permissioning layer, an error will be raised and logged in .usuage.usage if it is enabled.","title":"controlaccess.q"},{"location":"handlers/#trackclientsq","text":"trackclients.q is used to track client interaction. It is a slightly modified version of trackclients.q from code.kx, and extends the functionality to handle interaction with the discovery service. Whenever a client opens a connection to the q process, it will be registered in the .clients.clients table. Various details are logged, but from a diagnostic perspective the most important information are the client details, the number of queries it has run, the last time it ran a query, the number of failed queries and the cumulative size of results returned to it.","title":"trackclients.q"},{"location":"handlers/#trackserversq","text":"trackservers.q is used to register and maintain handles to external servers. It is a heavily modified version of trackservers.q from code.kx. It is explained more in section connectionmanagement.","title":"trackservers.q"},{"location":"handlers/#zpsignoreq","text":"zpsignore.q is used to check incoming async calls for certain patterns and to bypass all further message handler checks for messages matching the pattern. This is useful for handling update messages published to a process from a data source.","title":"zpsignore.q"},{"location":"handlers/#writeaccessq","text":"writeaccess.q is used to restrict client write access to data within a process. The script uses the reval function, released in KDB+ 3.3, to prevent client queries from modifying any data in place. At present only queries in the form of strings are passed through the reval function. Additonally the script disables any form of HTTP access. If using versions of KDB+ prior to 3.3, this feature must be disabled. An attempt to use this feature on previous KDB+ versions will result in an error and the relevant process exiting.","title":"writeaccess.q"},{"location":"handlers/#permissionsq","text":"permissions.q is used to control client access to a server process. It allows: Access control via username/password access, either in combination with the -u/U process flags or in place of them. Definition of user groups, which control variable access. Definition of user roles, which allow control over function execution. Deeper control over table subsetting through the use of \u201cvirtual tables\u201d, using enforced where clauses. Access restriction in TorQ can be enabled on all processes, each of which can then load the default.q in $KDBCONFIG/permissions/, which adds users, groups and roles allowing standard operation of TorQ. The admin user and role by default can access all functions, and each of the system processes has access only to the required system functions. Permissions are enabled or disabled on a per-process basis through setting .pm.enabled as 1b or 0b at process load (set to 0b by default). A permissioned process can safely interact with a non-permissioned process while still controlling access to itself. The access schema consists of 7 control tables: Name Descriptions user Username, locality, encryption type and password hash usergroup User and their group. userrole User and role. functiongroup Functions and their group function Function names, the roles which can access them, and a lambda checking the parameters those roles can use. access Variable names, the groups which can access them, and the read or write access level. virtualtable Virtual table name, main table name, and the where clause it enforces on access to that table. In addition to groupinfo and roleinfo tables, which contain the group/role name and a string describing each group and role. A user can belong to multiple groups, and have multiple roles. In particular the schema supports group hierarchy, where a user group can be listed as a user in the group table, and inherit all the permissions from another other group, effectively inheriting the second group itself. Note that a group cannot have the same name as an individual user. A user belonging to a group listed in the access table will have the specified level of access (read or write) to that group\u2019s variables, e.g. Table Group Level quote headtrader write trade juniortrader read Here, users in headtrader will have write access to the quote table, while juniortrader group has read access to the trade table. If headtraders have been set to inherit the juniortrader group, they will also have read access to trade. Note that read access is distinct from write access. Headtraders in this circumstance do not have implicit read access to the quote table. This control is for direct name access only. Selects, execs and updates are controlled via the function table, as below. The permissions script can be set to have permissive mode enabled with permissivemode:1b (disabled by default). When enabled at script loading, this bypasses access checks on variables which are not listed in the access table, effectively auto-allowlisting any variables not listed in the access table for all users, which may be useful in partly restricted development environments. Function access is controlled through non-hierarchical roles. A user attempting to run a named function will have their access checked against the function table through their role, for example, trying to run a function timedata[syms;bkttype], which selects from a table by a time bucket type bkttype on xbar: Function Role Param. Check timedata quant {1b} timedata normal user {x[`bkttype] in `hh} select quant {1b} The parameter check in the third column must be a lambda accepting a dictionary of parameters and their values, which can then return a boolean if some parameter condition is met. Here, any normal user must have their bucket type as an hour. If they try anything else, the function is not permitted. This could be extended to restriction to certain syms as well, in this example, the quant can run this function with any parameters. Anything passed to the param. check function returns 1b. A quant having general select access is listed as having 1b in the param. check. Further restriction of data can be achieved with virtual tables, via which users can be restricted to having a certain subset of data from a main table available. To avoid the need to replicate a potentially large subset of a table into a separately-controlled variable, this is done through pointing to the table under a different name via a where clause, e.g. Virtual Table Table Where Clause trade_lse trade ,(in;`src;\u201cL\u201d) quote_new quote ,(>;`time;(-;`.z.p;01:00)) When a select from trade_lse is performed, a select on trade is modified to contain the where clause above. Access to virtual tables can be controlled identically to access to real tables through the access table. If the process is given the flag \u201c-public 1\u201d, it will run in public access mode. This allows a user to log in without a password and be given the publicuser role and membership of the public group, which can be configured as any other group or role. The permissions control has a default size restriction of 2GB, set (as bytes) on .pm.maxsize. This is a global restriction and is not affected by user permissions. Adding to the groups and roles is handled by the functions: adduser[`user;`locality;`hash type; md5\"password\"] removeuser[`user] addgroup[`groupname; \"description\"] removegroup[`groupname] addrole[`rolename; \"description\"] removerole[`rolename] addtogroup[`user;`groupname] removefromgroup[`user; `groupname] assignrole[`user; `rolename] unassignrole[`user; `rolename] addfunction[`function; `functiongroup] removefunction[`function; `functiongroup] grantaccess[`variable; `groupname; `level] revokeaccess[`variable; `groupname; `level] grantfunction[`function; `rolename; {paramCheckFn}] revokefunction[`function; `rolename] createvirtualtable[`vtablename; `table; ,(whereclause)] removevirtualtable[`vtablename] cloneuser[`user;`newuser;\"password\"] which are further explained in the script API. Permission control operates identically on the gateway. A user connected to the gateway must have access to the gateway, and their roles must have access to the .gw.syncexec or .gw.asyncexec functions.","title":"permissions.q"},{"location":"handlers/#usage-example","text":"To connect to a permissioned RDB in the TorQ system, a group and role for the user must be established. If the RDB contains the tables trade, quote, and depth, and the process contains the functions getdata[syms, bkttype,bktsize] and hloc[table], restricted access would be configured like so: .pm.adduser[`adam;`local;`md5;md5\"pass\"] .pm.adduser[`bob;`local;`md5;md5\"pass\"] .pm.addtogroup[`adam;`fulluser] .pm.addtogroup[`bob;`partuser] .pm.addtogroup[`fulluser;`partuser] .pm.grantaccess[`quote;`fulluser;`read] .pm.grantaccess[`trade;`partuser;`read] .pm.createvirtualtable[`quotenew;`quote;enlist(>;`time;(-;`.z.p;01:00))] .pm.grantaccess[`quotenew;`partuser;`read] .pm.assignrole[`adam;`toplevel] .pm.assignrole[`bob;`lowlevel] .pm.grantfunction[`getdata;`toplevel;{1b}] .pm.grantfunction[`getdata;`lowlevel;{x[`syms] in `GOOG}] .pm.grantfunction[`hloc;`toplevel;{1b}] .pm.grantfunction[`hloc;`lowlevel;{x[`table] in `trade}] This provides a system in which Bob can access only the trade table, while Adam has access to the trade table and quote table (through inheritance from Bob\u2019s group). Through a virtual table, if Bob runs \u201cselect from quotenew\u201d, he is able to get a table of the last hour of quotes. When the system is started in normal mode, there is no IPC access to the depth table, however if the system was started in permissive mode, in this case any user who could log in could access depth. Adam can run the getdata function however he wants, and Bob can only run it against sym GOOG. Similarly Adam can run hloc against any table, but Bob can only look at trade with it. Additionally, any system calls would need to be actively permissioned in the same way, after defining a systemuser role (or expanding the default role in TorQ). The superuser is given global function access by assigning them .pm.ALL in the function table, for example a tickerplant pushing to the RDB would need to have a user and role defined: .pm.adduser[`ticker;`local;`md5;md5\"plant\"] .pm.assignrole[`ticker;`tp] And then grant that role access to the .u.upd function: .pm.grantfunction[`.u.upd;`tp;{1b}] Although the .u.upd function updates to a table, there is no need to grant direct access to that table.","title":"Usage Example"},{"location":"handlers/#gateway-example","text":"The gateway user will have superuser role by default. The execution of a function passed through the gateway is checked against the user who sent the call. This should not be modified. Within the gateway itself, access to target processes can be controlled via the function table. For example, if Adam in the previous example was allowed to access only the RDB with .gw.syncexec, you could use: .pm.grantfunction[`.gw.syncexec;`toplevel;{x[`1] in `rdb}] Since .gw.syncexec is a projection, the arguments supplied are checked in order, with dictionary keys `0`1`2... etc. This could be further extended to restrict access to queries with the .pm.allowed[user;query] function, which checks permissions of the current user as listed on the gateway permission tables: .pm.grantfunction[`.gw.syncexec;`toplevel; {.pm.allowed[.z.u;x[`0]] and x[`1] in `rdb}]","title":"Gateway Example"},{"location":"handlers/#ldapq","text":"Authentication with an ldap server is managed with ldap.q. It allows: A user to authenticate against an ldap server; Caching of user attempts to allow reauthentication without server if within checktime period; Users to be blocked if too many failed authentication attempts are made. Default parameters in the ldap namespace are set in {TORQHOME}/config/settings/default.q. parameter description enabled Whether ldap authentication is enabled debug Whether logging message are written to console server Host for ldap server. port Port number for ldap server. version Ldap version number. blocktime Time that must elapse before a blocked user can attempt to authenticate. If set to 0Np then the user is permanently blocked until an admin unblocks them. checklimit Login attempts before user is blocked. checktime Period of time that allows user to reauthenticate without confirming with ldap server. buildDNsuf Suffix for building distinguished name. buildDN Function to build distiniguished name. To get started the following will need altered from their default values: enabled, port, server, buildDNsuf. The value buildDNsuf is required to build a users bind_dn from the supplied username and is called by the function buildDN. An example definition is: .ldap.buildDNsuf:\"ou=users,dc=website,dc=com\"; Authentication is handled by .ldap.authenticate which is wrapped by .ldap.login, which is in turn wrapped by .z.pw when ldap authentication is enabled. When invoked .ldap.login retrieves the users latest authentication attempt from the cache, if it exists, and performs several checks before authenticating the user. To authenticate the function first checks whether the user has been blocked by reaching the checklimit and blocktime has not passed, immediately returning false if this is the case. If the user has previously successfully authenticated within the period defined by checktime and is using the same credentials authentication will be permitted. For all other cases an authentication attempt will be made against the ldap server. Example authentication attempt: .ldap.login[`user;pass] 0b To manually unblock a user the function .ldap.unblock must be passed their userame as a symbol. The function checks the cache to see whether a user is blocked and will reset the blocked status if necessary. An example usage of this function is: .ldap.unblock[`user]","title":"ldap.q"},{"location":"handlers/#diagnostic-reporting","text":"The message handler modifications provide a wealth of diagnostic information including: the timings and memory usage for every query run on a process; failed queries; clients trying to do things they are not permissioned for; the clients which are querying often and/or regularly extracting large datasets; the number of clients currently connected; timer calls and how long they take. Although not currently implemented, it would be straightforward to use this information to implement reports on the behaviour of each process and the overall health of the system. Similarly it would be straightforward to set up periodic publication to a central repository to have a single point for system diagnostic statistics.","title":"Diagnostic Reporting"},{"location":"monit/","text":"Monitoring TorQ There are lots of standard tools available to monitor a TorQ stack. It can be easily integrated with incumbent monitoring systems. We have outlined how to use two of our preferred options below- Monit and Datadog. Some of their features overlap and some are complementary. Monit Monit is a small open source utility for monitoring and managing UNIX systems. Monit's ease of use makes it the perfect tool for tracking the status of TorQ processes. Installation Monit is included in most Unix distributions but can also be downloaded from here . This monit addition to TorQ allows the monit config files to be easily generated, based on the contents of the process.csv file. The basic monit directory which has been added to TorQ can be seen below: ${TORQHOME} |---monit |---bin | |---monit.sh |---templates |---monitalert.cfg |---monitrc |---monittemplate.txt It is important to mention that AquaQ will not offer support for monitalert.cfg and monitrc . Those two files have been added as an example on how monit can be configured to monitor your system and to offer an out-of-the-box configuration that you can use to test that monit works. If the monit installation contains an updated version of monitrc, this should be used instead. Features Monit is only available for UNIX and it comes with a bash script that you can use to generate the configuration and start the processes. More details on how you use this script can be found below. We have also included a standard monitrc which will: Set the check interval to 30 seconds Set the location of the monit.log file Set the location of monit.state fsile Define the mail alert basic configuration Define the e-mail format Set the interface port (11000) user and password Set the location of the *.cfg files The monitalert.cfg it is only an example on how you can configure your own alerts for monitoring your UNIX system. There are no TorQ specific examples in this file. The only file which will be updated with future TorQ releases is the monittemplate.txt which generates the monitconfig.cfg . An example is included below: check process tickerplant1 matching \"15000 -proctype tickerplant -procname tickerplant1\" start program = \"/bin/bash -c '/home/USER/torqprodsupp/torqdev/deploy/torq.sh start tickerplant1'\" with timeout 10 seconds stop program = \"/bin/bash -c '/home/USER/torqprodsupp/torqdev/deploy/torq.sh stop tickerplant1'\" every \"* * * * *\" mode active Usage Guide If you want to use monit to monitor your UNIX system and TorQ processes you must first generate the configuration files and then start monit . We will assume that you start with a fresh copy of TorQ. Install TorQ and the any optional customisations (e.g. the TorQ Finanace Starter Pack) Navigate to ${TORQHOME}/monit/bin/ Execute: bash monit.sh generate all - to generate all the config files bash monit.sh generate alert - to generate the alert configuration file bash monit.sh generate monitconfig - to generate the monitconfig.cfg bash monit.sh generate monitrc - to generate the monitrc file However, you can also use your own configuration files by either creating a new directory in monit called config and moving all the .cfg files and the monitrc file in there or by modifying the last line in the monitrc to point to the folder where the .cfg files can be found. Start monit by executing bash monit.sh start The start function also take a parameter (\"string\") whch can specify the location of the monitrc . Datadog Datadog is a monitoring service for cloud-scale applications, providing monitoring of servers, databases, tools, and services, through a SaaS-based data analytics platform. This documentation provides a guideline on how to integrate TorQ and Datadog on a Linux host. Note: The Datadog Agent used in this integration is version 7, and may not be backwards or forwards compatible with other versions. See the Datadog documentation for more detail. Installation Please Note: At this time the integration of Datadog with TorQ is supported on a unix host. We have not yet provided instructions or functionality for the use of Datadog with TorQ on other operating systems. The Datadog Agent is software that runs on your host, collecting events and metrics from the host and sending them to Datadog, where you can analyze your monitoring and performance data. We have provided the functionality to monitor system metrics such as CPU and memory usage, report process errors to Datadog, perform data checks and visualise the results on a custom dashboard. The datadog agent can be installed here . Note that there are multiple price plans for Datadog, and the functionality available to you is dependent on your plan. A 14-day free trial is available, on which all the monitoring provided within this integration will work. Once you've set up an account with datadog, more information on getting started can be found here . An easy one-step install is provided as shown in the video above. Code is provided within TorQ to enable the integration of Datadog with a TorQ stack on a unix host. The directory layout of the relevant code is as shown below: ${TORQHOME} |---datadog |---Example_TorQ_Monitoring_Dashboard.json |---monitors |---setupdatadog.sh ${TORQHOME} |---code |---common | |---datadog.q |---monitor |---datadogchecks.q The setupdatadog.sh script is provided as a quick-start install to set up datadog on your host. It edits the config files required by datadog: Datadog.yaml, which contains the port number to listen on, and enables the agent to receive metrics and events from processes. Process.yaml, which determines which processes datadog will retrieve CPU and memory stats for. This file is generated using the process.csv, and by default all processes will be monitored. If you wish to monitor only certain processes, a \"datadog\" column can be added to the process.csv, and a value of 1 or 0 added to each of the processes (1 meaning monitored, 0 meaning not monitored). Features Send Metrics and Events Metrics are values sent from the system to quickly indicate the state of a process or the system itself. We have provided the functionality to send metrics from TorQ processes to the datadog agent, via the .dg.sendmetric function. q) .dg.sendmetric[\"metric_name\";metric_value] The default configuration generated by setupdatadog.sh also automates the sending of process stats as metrics, using the process.yaml as a template for which processes to monitor. Events are sent in a similar way to Metrics, but indicate a noteworthy record of activity We can send an event from a TorQ process using the .dg.sendevent function: .dg.sendevent[event_title;event_text;tags;alert_type] This expects string arguments, and can accept any number of custom tags, and an alert type one of \"error\", \"warning\" or \"success\". Send Process Errors To Datadog We can utilise the sendevent functionality to track any TorQ process errors on datadog. TorQ provides a hook for extended logging, .lg.ext. We have provided the option to edit this to send errors and warnings to datadog as events: enablelogging:{[] .lg.ext:{[olddef;loglevel;proctype;proc;id;message;dict] olddef[loglevel;proctype;proc;id;message;dict]; if[loglevel in `ERR`WARN;.dg.sendevent[string proc;message;string proctype;]$[loglevel=`ERR;\"error\";\"warning\"]]}[@[value;`.lg.ext;{{[loglevel;proctype;proc;id;message;dict]}}]] } By default, this is not enabled, but should you wish to enable this and view process errors and warnings, you can change the value of .dg.enabled in $TORQHOME/config/settings/default.q . .dg.enabled:1b Automate checks using monitor process TorQ's monitor process lets us define a series of user-specific checks to be run at specified intervals on specified processes. We have provided an example check ( .dg.isok ) and ( .dg.sendresultmetric ). The .dg.isok function is a simple check to see if a process is available and can be queried. The monitor process will execute this function within the processes it has been configured to monitor, and a value of 1b is returned if the process is available. Within the monitor process, the .dg.sendresultmetric result handler then sends this result to datadog as a metric, which can be displayed on the UI dashboard. Other checks can be added to provide more specific metrics to monitor, such as table record counts, e.g: .dg.checkcount:{[table;day] count select from table where date=day} No default monitor configuration is provided, but if you wish to add a check such as the above to be run by the monitor process, the monitorconfig.csv can be edited as shown: family|metric|process|query|resultchecker|params|period|runtime datadog|okcheck|rdb1;sort1;gateway1;hdb1;hdb2;wdb1;feed1|.dg.isok|.dg.sendresultmetric|`varname`cond!(`datadogcheck;`true)|0D00:01|0D00:00:00.5 Usage Guide If you wish to incorporate Datadog into your system as a monitoring tool, you can follow the instructions in the video above or the written instructions as follows. This will configure datadog according to default settings, and will not enable error logging or any monitoring functions unless configured by the user. Install Datadog agent on your host according to instructions here Set up required configuration on host: (Optional) Change the port on which the agent listens, DOGSTATSD_PORT. This can be edited in the setenv.sh script in the $TORQHOME directory, and will default to port 8125. Execute . setenv.sh in the $TORHOME directory, to pick up any changes to DOGSTATSD_PORT Execute . setupdatadog.sh . This will edit the required datadog config files, datadog.yaml and process.yaml. Restart the datadog agent: sudo service datadog-agent restart . This ensures the updated config is used by the agent. The agent is now reporting process stats, and these can be visualised on the datadog UI. For more detailed metrics and events, enable the error logging using .dg.enabled:1b and configure the monitor process to run some checks. Using the Datadog UI You can visualise your metrics and events on a dashboard, and configure monitors and alerts. An example dashboard has been provided in $TORQHOME/datadog/Example_TorQ_Monitoring_Dashboard.json. This displays simple metrics such as process cpu %, process memory % and relative changes in these, as well as TorQ-specific process checks and a stream of any errors from TorQ processes. To import this into your datadog UI, go to the dashboards section here And click add new screenboard . In the settings section, select import dashboard json and copy and paste the json or browse your files to select the json dashboard you want to import. You will be prompted with whether or not you want to replace whatever is on the screenboard - click yes. Your dashboard will then load and display as below. The appearance of the dashboard will depend on whether you've enabled the extended error logging, and configured the monitor process. Alternatively, you can create your own custom dashboard according to the documentation here Monitors can be set up to track metrics and establish alerts based on critical changes. We've provided some example monitors, designed to notify if a process goes down, or system resources reach a limit. To import an example monitor, go to the \"Monitors\" section on your datadog account, click New Monitor , then Import Monitor , and copy and paste the JSON of the monitor you'd like to use into the text box provided, then click save. The monitor will then become available and start alerting. You can create custom monitors using the guidelines in the Datadog documentation here Monitoring Multiple Hosts Depending on your datadog pricing plan, the option to monitor multiple hosts is provided. To integrate Datadog and TorQ across all hosts, you will need to install the agent on each host, and follow the installation instructions as above. If your TorQ setup is operating across different hosts, you will need to establish which processes to monitor on which host by configuring a \"datadog\" column in the process.csv as discussed earlier.","title":"Monitoring"},{"location":"monit/#monitoring-torq","text":"There are lots of standard tools available to monitor a TorQ stack. It can be easily integrated with incumbent monitoring systems. We have outlined how to use two of our preferred options below- Monit and Datadog. Some of their features overlap and some are complementary.","title":"Monitoring TorQ"},{"location":"monit/#monit","text":"Monit is a small open source utility for monitoring and managing UNIX systems. Monit's ease of use makes it the perfect tool for tracking the status of TorQ processes.","title":"Monit"},{"location":"monit/#installation","text":"Monit is included in most Unix distributions but can also be downloaded from here . This monit addition to TorQ allows the monit config files to be easily generated, based on the contents of the process.csv file. The basic monit directory which has been added to TorQ can be seen below: ${TORQHOME} |---monit |---bin | |---monit.sh |---templates |---monitalert.cfg |---monitrc |---monittemplate.txt It is important to mention that AquaQ will not offer support for monitalert.cfg and monitrc . Those two files have been added as an example on how monit can be configured to monitor your system and to offer an out-of-the-box configuration that you can use to test that monit works. If the monit installation contains an updated version of monitrc, this should be used instead.","title":"Installation"},{"location":"monit/#features","text":"Monit is only available for UNIX and it comes with a bash script that you can use to generate the configuration and start the processes. More details on how you use this script can be found below. We have also included a standard monitrc which will: Set the check interval to 30 seconds Set the location of the monit.log file Set the location of monit.state fsile Define the mail alert basic configuration Define the e-mail format Set the interface port (11000) user and password Set the location of the *.cfg files The monitalert.cfg it is only an example on how you can configure your own alerts for monitoring your UNIX system. There are no TorQ specific examples in this file. The only file which will be updated with future TorQ releases is the monittemplate.txt which generates the monitconfig.cfg . An example is included below: check process tickerplant1 matching \"15000 -proctype tickerplant -procname tickerplant1\" start program = \"/bin/bash -c '/home/USER/torqprodsupp/torqdev/deploy/torq.sh start tickerplant1'\" with timeout 10 seconds stop program = \"/bin/bash -c '/home/USER/torqprodsupp/torqdev/deploy/torq.sh stop tickerplant1'\" every \"* * * * *\" mode active","title":"Features"},{"location":"monit/#usage-guide","text":"If you want to use monit to monitor your UNIX system and TorQ processes you must first generate the configuration files and then start monit . We will assume that you start with a fresh copy of TorQ. Install TorQ and the any optional customisations (e.g. the TorQ Finanace Starter Pack) Navigate to ${TORQHOME}/monit/bin/ Execute: bash monit.sh generate all - to generate all the config files bash monit.sh generate alert - to generate the alert configuration file bash monit.sh generate monitconfig - to generate the monitconfig.cfg bash monit.sh generate monitrc - to generate the monitrc file However, you can also use your own configuration files by either creating a new directory in monit called config and moving all the .cfg files and the monitrc file in there or by modifying the last line in the monitrc to point to the folder where the .cfg files can be found. Start monit by executing bash monit.sh start The start function also take a parameter (\"string\") whch can specify the location of the monitrc .","title":"Usage Guide"},{"location":"monit/#datadog","text":"Datadog is a monitoring service for cloud-scale applications, providing monitoring of servers, databases, tools, and services, through a SaaS-based data analytics platform. This documentation provides a guideline on how to integrate TorQ and Datadog on a Linux host. Note: The Datadog Agent used in this integration is version 7, and may not be backwards or forwards compatible with other versions. See the Datadog documentation for more detail.","title":"Datadog"},{"location":"monit/#installation_1","text":"Please Note: At this time the integration of Datadog with TorQ is supported on a unix host. We have not yet provided instructions or functionality for the use of Datadog with TorQ on other operating systems. The Datadog Agent is software that runs on your host, collecting events and metrics from the host and sending them to Datadog, where you can analyze your monitoring and performance data. We have provided the functionality to monitor system metrics such as CPU and memory usage, report process errors to Datadog, perform data checks and visualise the results on a custom dashboard. The datadog agent can be installed here . Note that there are multiple price plans for Datadog, and the functionality available to you is dependent on your plan. A 14-day free trial is available, on which all the monitoring provided within this integration will work. Once you've set up an account with datadog, more information on getting started can be found here . An easy one-step install is provided as shown in the video above. Code is provided within TorQ to enable the integration of Datadog with a TorQ stack on a unix host. The directory layout of the relevant code is as shown below: ${TORQHOME} |---datadog |---Example_TorQ_Monitoring_Dashboard.json |---monitors |---setupdatadog.sh ${TORQHOME} |---code |---common | |---datadog.q |---monitor |---datadogchecks.q The setupdatadog.sh script is provided as a quick-start install to set up datadog on your host. It edits the config files required by datadog: Datadog.yaml, which contains the port number to listen on, and enables the agent to receive metrics and events from processes. Process.yaml, which determines which processes datadog will retrieve CPU and memory stats for. This file is generated using the process.csv, and by default all processes will be monitored. If you wish to monitor only certain processes, a \"datadog\" column can be added to the process.csv, and a value of 1 or 0 added to each of the processes (1 meaning monitored, 0 meaning not monitored).","title":"Installation"},{"location":"monit/#features_1","text":"Send Metrics and Events Metrics are values sent from the system to quickly indicate the state of a process or the system itself. We have provided the functionality to send metrics from TorQ processes to the datadog agent, via the .dg.sendmetric function. q) .dg.sendmetric[\"metric_name\";metric_value] The default configuration generated by setupdatadog.sh also automates the sending of process stats as metrics, using the process.yaml as a template for which processes to monitor. Events are sent in a similar way to Metrics, but indicate a noteworthy record of activity We can send an event from a TorQ process using the .dg.sendevent function: .dg.sendevent[event_title;event_text;tags;alert_type] This expects string arguments, and can accept any number of custom tags, and an alert type one of \"error\", \"warning\" or \"success\". Send Process Errors To Datadog We can utilise the sendevent functionality to track any TorQ process errors on datadog. TorQ provides a hook for extended logging, .lg.ext. We have provided the option to edit this to send errors and warnings to datadog as events: enablelogging:{[] .lg.ext:{[olddef;loglevel;proctype;proc;id;message;dict] olddef[loglevel;proctype;proc;id;message;dict]; if[loglevel in `ERR`WARN;.dg.sendevent[string proc;message;string proctype;]$[loglevel=`ERR;\"error\";\"warning\"]]}[@[value;`.lg.ext;{{[loglevel;proctype;proc;id;message;dict]}}]] } By default, this is not enabled, but should you wish to enable this and view process errors and warnings, you can change the value of .dg.enabled in $TORQHOME/config/settings/default.q . .dg.enabled:1b Automate checks using monitor process TorQ's monitor process lets us define a series of user-specific checks to be run at specified intervals on specified processes. We have provided an example check ( .dg.isok ) and ( .dg.sendresultmetric ). The .dg.isok function is a simple check to see if a process is available and can be queried. The monitor process will execute this function within the processes it has been configured to monitor, and a value of 1b is returned if the process is available. Within the monitor process, the .dg.sendresultmetric result handler then sends this result to datadog as a metric, which can be displayed on the UI dashboard. Other checks can be added to provide more specific metrics to monitor, such as table record counts, e.g: .dg.checkcount:{[table;day] count select from table where date=day} No default monitor configuration is provided, but if you wish to add a check such as the above to be run by the monitor process, the monitorconfig.csv can be edited as shown: family|metric|process|query|resultchecker|params|period|runtime datadog|okcheck|rdb1;sort1;gateway1;hdb1;hdb2;wdb1;feed1|.dg.isok|.dg.sendresultmetric|`varname`cond!(`datadogcheck;`true)|0D00:01|0D00:00:00.5","title":"Features"},{"location":"monit/#usage-guide_1","text":"If you wish to incorporate Datadog into your system as a monitoring tool, you can follow the instructions in the video above or the written instructions as follows. This will configure datadog according to default settings, and will not enable error logging or any monitoring functions unless configured by the user. Install Datadog agent on your host according to instructions here Set up required configuration on host: (Optional) Change the port on which the agent listens, DOGSTATSD_PORT. This can be edited in the setenv.sh script in the $TORQHOME directory, and will default to port 8125. Execute . setenv.sh in the $TORHOME directory, to pick up any changes to DOGSTATSD_PORT Execute . setupdatadog.sh . This will edit the required datadog config files, datadog.yaml and process.yaml. Restart the datadog agent: sudo service datadog-agent restart . This ensures the updated config is used by the agent. The agent is now reporting process stats, and these can be visualised on the datadog UI. For more detailed metrics and events, enable the error logging using .dg.enabled:1b and configure the monitor process to run some checks.","title":"Usage Guide"},{"location":"monit/#using-the-datadog-ui","text":"You can visualise your metrics and events on a dashboard, and configure monitors and alerts. An example dashboard has been provided in $TORQHOME/datadog/Example_TorQ_Monitoring_Dashboard.json. This displays simple metrics such as process cpu %, process memory % and relative changes in these, as well as TorQ-specific process checks and a stream of any errors from TorQ processes. To import this into your datadog UI, go to the dashboards section here And click add new screenboard . In the settings section, select import dashboard json and copy and paste the json or browse your files to select the json dashboard you want to import. You will be prompted with whether or not you want to replace whatever is on the screenboard - click yes. Your dashboard will then load and display as below. The appearance of the dashboard will depend on whether you've enabled the extended error logging, and configured the monitor process. Alternatively, you can create your own custom dashboard according to the documentation here Monitors can be set up to track metrics and establish alerts based on critical changes. We've provided some example monitors, designed to notify if a process goes down, or system resources reach a limit. To import an example monitor, go to the \"Monitors\" section on your datadog account, click New Monitor , then Import Monitor , and copy and paste the JSON of the monitor you'd like to use into the text box provided, then click save. The monitor will then become available and start alerting. You can create custom monitors using the guidelines in the Datadog documentation here","title":"Using the Datadog UI"},{"location":"monit/#monitoring-multiple-hosts","text":"Depending on your datadog pricing plan, the option to monitor multiple hosts is provided. To integrate Datadog and TorQ across all hosts, you will need to install the agent on each host, and follow the installation instructions as above. If your TorQ setup is operating across different hosts, you will need to establish which processes to monitor on which host by configuring a \"datadog\" column in the process.csv as discussed earlier.","title":"Monitoring Multiple Hosts"},{"location":"unittesting/","text":"Unit Testing in TorQ Introduction Unit testing is a software validation methodology where individual modules or 'units' of source code are tested to determine whether they work as intended. TorQ's unit testing framework has been designed to be user-friendly and includes lots of useful features such as debugging and integration testing. We shall see that, using this framework, it is straightforward to test both small units of code as well as larger pieces of application functionality. Test Basics My First Test Tests are written in the style of Kx's k4unit , where tests are written in CSV files and loaded into a Q process along with the testing code and are run line by line, with the results being stored in a table. In order to write and run a basic test, in a new folder create a CSV file with some basic tests in it, like the following: action,ms,bytes,lang,code,repeat,minver,comment comment,0,0,,this will be ignored,1,,\"\" before,0,0,q,aa:22,1,,\"This sets a variable before the tests begin\" true,0,0,q,2=sum 1 1,1,,\"A basic arithmetic check\" fail,0,0,q,2=`aa,1,,\"This ought to fail\" after,0,0,q,bb:33,1,,\"This code executes after the tests have run\" Then use the torq.q script to start up a TorQ process with the test flag pointing to the directory where the CSV is written and a debug flag so that the process outputs its logs into the prompt and stays alive: $ q ${TORQHOME}/torq.q -proctype test -procname test1 -test /path/to/my/tests -debug This will automatically load in the test code and all CSVs in that directory into a test TorQ process and run the tests in those files, then output the results to the screen. Simple RDB Test A similar scenario is when the test code is loaded into a 'production' TorQ process, ie. an RDB or a tickerplant, in order to test the functioning of that process. A simple RDB example can be seen here: action,ms,bytes,lang,code,repeat,minver,comment true,0,0,q,`segmentedtickerplant~.rdb.tickerplanttypes,1,,\"Check the TP type\" true,0,0,q,`~.rdb.subscribeto,1,,\"Check RDB is subscribed to null symbol (all tables)\" true,0,0,q,0~count heartbeat,1,,\"Check heartheat table is empty\" run,0,0,q,.rdb.upd[`heartbeat;(.z.p;`test;`rdb1;1;1i;`testhost;1i)],1,,\"Call RDB UPD function\" true,0,0,q,1~count heartbeat,1,,\"Check that a row has been added to the heartbeat table\" These tests simply check that certain process variables have been set properly and test the UPD function, and these are triggered similarly to the previous set, except that an RDB process is used to load in the tests rather than a 'blank' test process. Here, the -proctype is an RDB, which loads in the code in the ${KDBCODE}/rdb/ folder as well as any common code, so that the process logic can be tested. The procname can be anything you want: $ q ${TORQHOME}/torq.q -proctype rdb -procname rdb1 -test /path/to/my/tests -debug Integration Testing: WDB Subscription Tests Once the basic concepts from before have been understood it is possible to generate much more complex testing scenarios. Often it will be necessary to test interactions between different processes, in which case you will need more than just the one testing process. In this example test we are examining the interaction between a Segmented Tickerplant (STP) and three WDBs, one of which is subscribed to everything, one to a subset of syms and the other to a subset of tables. In order to facilitate this, we need to make some additions to our test directory. As well as the CSVs containing the tests themselves we now add three more files: process.csv , run.sh and settings.q , and we shall examine these in more detail. process.csv An easy way to spin up multiple custom processes is to pass a custom version of process.csv into the TorQ start script. Our file will contain a discovery process, three WDBs and an STP and can be seen here: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+100,discovery,discovery1,${TORQHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/discovery.q,1,,q localhost,{KDBBASEPORT}+101,wdb,wdball,${TORQHOME}/appconfig/passwords/accesslist.txt,1,1,,,${KDBCODE}/processes/wdb.q,1,-.wdb.tickerplanttypes segmentedtickerplant,q localhost,{KDBBASEPORT}+102,wdb,wdbsymfilt,${TORQHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/wdb.q,1,-.wdb.tickerplanttypes segmentedtickerplant -.wdb.subsyms GOOG,q localhost,{KDBBASEPORT}+103,wdb,wdbtabfilt,${TORQHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/wdb.q,1,-.wdb.tickerplanttypes segmentedtickerplant -.wdb.subtabs quote,q localhost,{KDBBASEPORT}+104,segmentedtickerplant,stp1,${TORQHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQHOME}/database.q,q As we can see here, three WDBs are being started up but with different parameters in the 'extras' column, meaning they will have different subscription behaviour. run.sh Since we need to run multiple shell commands to bring up all the processes and run the tests, it makes sense to move them to a file: #!/bin/bash # Path to test directory testpath=${KDBTESTS}/demo # Start procs ${TORQHOME}/torq.sh start all -csv ${testpath}/process.csv # Start test proc /usr/bin/rlwrap q ${TORQHOME}/torq.q \\ -proctype test -procname test1 \\ -test ${testpath} \\ -load ${KDBTESTS}/helperfunctions.q ${testpath}/settings.q \\ -procfile ${testpath}/process.csv -debug # Shut down procs ${TORQHOME}/torq.sh stop all -csv ${testpath}/process.csv The first piece of code in this script simply sets a variable with the path for the test directory. Then the discovery, STP and WDB processes are started, with the custom CSV flag pointing to the file we just created above. If only a subset of the processes in that file are needed then they can be called by name instead of using 'all'. Next, our test process is started up with some extra flags. The -load flag allows other q files to be loaded, in this case we are loading a file of helper functions defined in the KDBTESTS directory and the settings.q file which will be explored in greater depth shortly. There is also a -procfile flag in use which, again, points to our custom process CSV, and this more easily allows the use of TorQ connection management in tests. Finally, once all the tests have been run and the test process exited, all the other processes are brought down. The tests can now be run from the command line with a simple /path/to/tests/run.sh . settings.q This file, while not strictly speaking necessary, is a very handy place to store variables and functions that will be used in the tests rather than having to declare them in the CSV itself. In this example we are storing TorQ connection parameters and test updates for the trade and quote tables: // IPC connection parameters .servers.CONNECTIONS:`wdb`segmentedtickerplant`tickerplant; .servers.USERPASS:`admin:admin; // Test updates testtrade:((5#`GOOG),5?`4;10?100.0;10?100i;10#0b;10?.Q.A;10?.Q.A;10#`buy); testquote:(10?`4;(5?50.0),50+5?50.0;10?100.0;10?100i;10?100i;10?.Q.A;10?.Q.A;10#`3); WDB Tests The following CSV shows a fairly straightforward use of this setup: action,ms,bytes,lang,code,repeat,minver,comment before,0,0,q,.servers.startup[],1,,\"Start TorQ connection management\" before,0,0,q,.proc.sys \"sleep 2\",1,,\"Wait for proc to start\" before,0,0,q,stpHandle:gethandle[`stp1],1,,\"Open STP handle\" before,0,0,q,wdbHandles:`all`sym`tab!gethandle each `wdball`wdbsymfilt`wdbtabfilt,1,,\"Open WDB handles\" before,0,0,q,t1:wdbHandles[`all`sym] @\\: \"count trade\",1,,\"Get initial trade table counts\" before,0,0,q,q1:(value wdbHandles) @\\: \"count quote\",1,,\"Get initial quote table counts\" run,0,0,q,\"stpHandle @/: `.u.upd ,/: ((`trade;testtrade);(`quote;testquote))\",1,,\"Send trade and quote updates to STP\" run,0,0,q,.proc.sys \"sleep 2\",1,,\"Wait for updates to publish\" true,0,0,q,(t1+10 5)~wdbHandles[`all`sym] @\\: \"count trade\",1,,\"Check trade update was correctly published\" true,0,0,q,(q1+10 0 10)~(value wdbHandles) @\\: \"count quote\",1,,\"Check quote update was correctly published\" The first thing that happens is that TorQ connection management is set up, then handles are opened to the STP and WDBs and some test updates are sent to the STP. Finally, the test process grabs the latest table counts from the WDBs and checks they have updated correctly. Note that functions and variables not defined in the test are brought in from the settings and helper function files. Our test directory now looks like this: wdbtests |---- process.csv |---- run.sh |---- settings.q |---- test.csv Adding functionality to the run script In order to make our tests more dynamic and useful there are a few things we can do. In the KDBTESTS directory there is a flagparse.sh script which contains some basic code for parsing command line flags. If we add this line to the top of our run script: source $KDBTESTS/flagparse.sh We can now pass flags to our run script, and these are as follows: Flag Description -d Starts the test process in debug mode -s Starts the test process in debug and stop mode (thrown out to q prompt on error or test failure) -w Writes test results to CSVs on disk -q Starts the test process in quiet mode -r timestamp Passes a timestamp into the tests for on-disk versioning The other addition to be made is a -testresults flag to the TorQ start line in the run script. This passes in a folder where the test process will store its logs and test results in a date-partitioned folder structure. Our run script now looks like the following: #!/bin/bash # Handle command-line arguments source $KDBTESTS/flagparse.sh # Path to test directory testpath=${KDBTESTS}/demo # Start procs ${TORQHOME}/torq.sh start discovery1 stp1 wdball wdbsymfilt wdbtabfilt -csv ${testpath}/process.csv # Start test proc /usr/bin/rlwrap q ${TORQHOME}/torq.q \\ -proctype test -procname test1 \\ -test ${testpath} \\ -load ${KDBTESTS}/helperfunctions.q ${testpath}/settings.q \\ -testresults ${KDBTESTS}/demo/results/ \\ -runtime $run \\ -procfile ${testpath}/process.csv \\ $debug $stop $write $quiet # Shut down procs ${TORQHOME}/torq.sh stop discovery1 stp1 wdball wdbsymfilt wdbtabfilt -csv ${testpath}/process.csv Near the end we have four variables which are optional flags that will be added by the flag parser script. We now have plenty of options when it comes to running the script: # Run in debug mode ./run.sh -d # Run in debug and stop mode ./run.sh -s # Run in quiet mode, pass in a runtime and write results to disk ./run.sh -r 2020.10.12D16:34:44.261143000 -wq Running Multiple Test Sets When testing a large piece of functionality it is very likely that there will a large number of tests to run. It is advised in such a case that the tests are split up into different technical groupings, each with their own folder with test CSVs, process and settings files and run scripts. This is essentially creating an isolated environment for each group of tests. The issue though is that it becomes cumbersome to run all tests, and so a script has been created which executes all the run scripts in each of the directories, saves their results to disk and displays them at the end of the run. The runall.q script lives in the KDBTESTS directory and it takes two arguments, -rundir and -resdir , which are the top level directory where your test sub-folders live, and the folder where the test logs and results are kept respectively. This script runs each of the run.sh scripts in each of the sub-folders in the -rundir folder in write and quiet mode, passing in the current timestamp so that all will be saved to disk in an easily-versioned manner. An example run would look like the following: q runall.q -rundir /path/to/top/test/folder/ -resdir /path/to/results/folder/ Debugging Tests In the scenario where there are a large number of tests spread across several folders, you can use the run-all mechanic to test them all. However, say there is an error in one of the test files. Normally, this would be fairly difficult to track down and deal with, but the features of this framework make it much easier. At the end of the run, three things will be shown on the console, a results table, a failures table and dictionary of error logs and any errors in them. Any tests that fail will be displayed in the fails table and any code errors that occur will be logged to disk and displayed in the dictionary, and both of these sources contain the file the test was in, the line number it is on and the code itself. A demo folder has been prepared which contains a code error and a test failure. All the tests in the folder were run as follows: q runall.q -rundir demo -resdir demo/results When the tests finish running the results are displayed on the console. There are various failed tests and an entry appears in our error logs which can be expanded out: ... \"Logged errors:\" :demo/results/2020.11.11/logs/err_eod.log| () :demo/results/2020.11.11/logs/err_wdb.log| ,\"2020.11.11D16:37:21.716171000|aquaq-184|test|test1|ERR|KUexecerr|run error in file :/ho..\" ... q) raze value errors \"2020.11.11D16:37:21.716171000|aquaq-184|test|test1|ERR|KUexecerr|run error in file :/home/mpotter/kdbCode/segtp/deploy/tests/demo/wdb/test.csv on line 8 - stHandle. Code: 'stHandle @/: `.u.upd ,/: ((`trade;testtrade);(`quote;testquote))'\" The script has read the file demo/results/2020.11.11/logs/err_wdb.log and from the message within we can see that there is a code error in a 'run' command in the file /home/mpotter/kdbCode/segtp/deploy/tests/demo/wdb/test.csv on line 8, and the offending piece of code is also displayed. This may be enough information for us to solve the issue, but if not we can dig deeper. We can see that the error is coming from the WDB tests, and so we run those in debug mode in the following way: ./demo/wdb/run.sh -d This runs the tests and displays the error message we saw earlier before outputting our test results and failures and leaving us in the q session. By examining the error message and being able to access the q session we can see that the variable stHandle should in fact be stpHandle and this typo is causing the error. Once this is fixed, it can be run again and the error doesn't appear any more. There is still one test failing, however: q) select action,code,csvline from KUerr action code csvline ----------------------------------------------------------------- true (t1+1 5)~t2:wdbHandles[`all`sym] @\\: \"count trade\" 10 The best way to debug this would be to be able to exit the tests as this line is being run and be able to examine it from there. We can do this by invoking the stop mode in our test script: ./demo/wdb/run.sh -s This throws us out to the q prompt at this point in the tests with the following error: 'failed to load /home/mpotter/kdbCode/segtp/deploy/tests/runtests.q : true test failure in file :/home/mpotter/kdbCode/segtp/deploy/tests/demo/wdb/test.csv on line 10 We can get the code which failed and run it here to see what it returns. From doing some quick debugging we can see that one of the items being added to t1 is wrong, it should be ten rather than 1. Once this is fixed we can run the WDB tests again and we see that there are now no errors and all the tests pass! We can then run all of our tests again as at the start and no new test failures come up and the latest error logs are empty. Notes on Best Practice Here are some recommendations on making test development more straightforward: Use TorQ connection management when dealing with multiple processes When just unit testing one process in isolation, run the tests from an instance of that process When performing integration tests that examine the interaction of multiple processes, run the tests from a 'blank' process where possible, as this ensures that test code and process code don't get in each other's way Put any variable or function declarations in a settings file so as not to clutter the test code If there are a large number of tests, split into folders of related tests Try to keep test output isolated from the rest of the application, ie. any logs or HDB data should be output to a separate testing location and cleared afterwards if appropriate","title":"Testing"},{"location":"unittesting/#unit-testing-in-torq","text":"","title":"Unit Testing in TorQ"},{"location":"unittesting/#introduction","text":"Unit testing is a software validation methodology where individual modules or 'units' of source code are tested to determine whether they work as intended. TorQ's unit testing framework has been designed to be user-friendly and includes lots of useful features such as debugging and integration testing. We shall see that, using this framework, it is straightforward to test both small units of code as well as larger pieces of application functionality.","title":"Introduction"},{"location":"unittesting/#test-basics","text":"My First Test Tests are written in the style of Kx's k4unit , where tests are written in CSV files and loaded into a Q process along with the testing code and are run line by line, with the results being stored in a table. In order to write and run a basic test, in a new folder create a CSV file with some basic tests in it, like the following: action,ms,bytes,lang,code,repeat,minver,comment comment,0,0,,this will be ignored,1,,\"\" before,0,0,q,aa:22,1,,\"This sets a variable before the tests begin\" true,0,0,q,2=sum 1 1,1,,\"A basic arithmetic check\" fail,0,0,q,2=`aa,1,,\"This ought to fail\" after,0,0,q,bb:33,1,,\"This code executes after the tests have run\" Then use the torq.q script to start up a TorQ process with the test flag pointing to the directory where the CSV is written and a debug flag so that the process outputs its logs into the prompt and stays alive: $ q ${TORQHOME}/torq.q -proctype test -procname test1 -test /path/to/my/tests -debug This will automatically load in the test code and all CSVs in that directory into a test TorQ process and run the tests in those files, then output the results to the screen. Simple RDB Test A similar scenario is when the test code is loaded into a 'production' TorQ process, ie. an RDB or a tickerplant, in order to test the functioning of that process. A simple RDB example can be seen here: action,ms,bytes,lang,code,repeat,minver,comment true,0,0,q,`segmentedtickerplant~.rdb.tickerplanttypes,1,,\"Check the TP type\" true,0,0,q,`~.rdb.subscribeto,1,,\"Check RDB is subscribed to null symbol (all tables)\" true,0,0,q,0~count heartbeat,1,,\"Check heartheat table is empty\" run,0,0,q,.rdb.upd[`heartbeat;(.z.p;`test;`rdb1;1;1i;`testhost;1i)],1,,\"Call RDB UPD function\" true,0,0,q,1~count heartbeat,1,,\"Check that a row has been added to the heartbeat table\" These tests simply check that certain process variables have been set properly and test the UPD function, and these are triggered similarly to the previous set, except that an RDB process is used to load in the tests rather than a 'blank' test process. Here, the -proctype is an RDB, which loads in the code in the ${KDBCODE}/rdb/ folder as well as any common code, so that the process logic can be tested. The procname can be anything you want: $ q ${TORQHOME}/torq.q -proctype rdb -procname rdb1 -test /path/to/my/tests -debug","title":"Test Basics"},{"location":"unittesting/#integration-testing-wdb-subscription-tests","text":"Once the basic concepts from before have been understood it is possible to generate much more complex testing scenarios. Often it will be necessary to test interactions between different processes, in which case you will need more than just the one testing process. In this example test we are examining the interaction between a Segmented Tickerplant (STP) and three WDBs, one of which is subscribed to everything, one to a subset of syms and the other to a subset of tables. In order to facilitate this, we need to make some additions to our test directory. As well as the CSVs containing the tests themselves we now add three more files: process.csv , run.sh and settings.q , and we shall examine these in more detail. process.csv An easy way to spin up multiple custom processes is to pass a custom version of process.csv into the TorQ start script. Our file will contain a discovery process, three WDBs and an STP and can be seen here: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+100,discovery,discovery1,${TORQHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/discovery.q,1,,q localhost,{KDBBASEPORT}+101,wdb,wdball,${TORQHOME}/appconfig/passwords/accesslist.txt,1,1,,,${KDBCODE}/processes/wdb.q,1,-.wdb.tickerplanttypes segmentedtickerplant,q localhost,{KDBBASEPORT}+102,wdb,wdbsymfilt,${TORQHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/wdb.q,1,-.wdb.tickerplanttypes segmentedtickerplant -.wdb.subsyms GOOG,q localhost,{KDBBASEPORT}+103,wdb,wdbtabfilt,${TORQHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/wdb.q,1,-.wdb.tickerplanttypes segmentedtickerplant -.wdb.subtabs quote,q localhost,{KDBBASEPORT}+104,segmentedtickerplant,stp1,${TORQHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQHOME}/database.q,q As we can see here, three WDBs are being started up but with different parameters in the 'extras' column, meaning they will have different subscription behaviour. run.sh Since we need to run multiple shell commands to bring up all the processes and run the tests, it makes sense to move them to a file: #!/bin/bash # Path to test directory testpath=${KDBTESTS}/demo # Start procs ${TORQHOME}/torq.sh start all -csv ${testpath}/process.csv # Start test proc /usr/bin/rlwrap q ${TORQHOME}/torq.q \\ -proctype test -procname test1 \\ -test ${testpath} \\ -load ${KDBTESTS}/helperfunctions.q ${testpath}/settings.q \\ -procfile ${testpath}/process.csv -debug # Shut down procs ${TORQHOME}/torq.sh stop all -csv ${testpath}/process.csv The first piece of code in this script simply sets a variable with the path for the test directory. Then the discovery, STP and WDB processes are started, with the custom CSV flag pointing to the file we just created above. If only a subset of the processes in that file are needed then they can be called by name instead of using 'all'. Next, our test process is started up with some extra flags. The -load flag allows other q files to be loaded, in this case we are loading a file of helper functions defined in the KDBTESTS directory and the settings.q file which will be explored in greater depth shortly. There is also a -procfile flag in use which, again, points to our custom process CSV, and this more easily allows the use of TorQ connection management in tests. Finally, once all the tests have been run and the test process exited, all the other processes are brought down. The tests can now be run from the command line with a simple /path/to/tests/run.sh . settings.q This file, while not strictly speaking necessary, is a very handy place to store variables and functions that will be used in the tests rather than having to declare them in the CSV itself. In this example we are storing TorQ connection parameters and test updates for the trade and quote tables: // IPC connection parameters .servers.CONNECTIONS:`wdb`segmentedtickerplant`tickerplant; .servers.USERPASS:`admin:admin; // Test updates testtrade:((5#`GOOG),5?`4;10?100.0;10?100i;10#0b;10?.Q.A;10?.Q.A;10#`buy); testquote:(10?`4;(5?50.0),50+5?50.0;10?100.0;10?100i;10?100i;10?.Q.A;10?.Q.A;10#`3); WDB Tests The following CSV shows a fairly straightforward use of this setup: action,ms,bytes,lang,code,repeat,minver,comment before,0,0,q,.servers.startup[],1,,\"Start TorQ connection management\" before,0,0,q,.proc.sys \"sleep 2\",1,,\"Wait for proc to start\" before,0,0,q,stpHandle:gethandle[`stp1],1,,\"Open STP handle\" before,0,0,q,wdbHandles:`all`sym`tab!gethandle each `wdball`wdbsymfilt`wdbtabfilt,1,,\"Open WDB handles\" before,0,0,q,t1:wdbHandles[`all`sym] @\\: \"count trade\",1,,\"Get initial trade table counts\" before,0,0,q,q1:(value wdbHandles) @\\: \"count quote\",1,,\"Get initial quote table counts\" run,0,0,q,\"stpHandle @/: `.u.upd ,/: ((`trade;testtrade);(`quote;testquote))\",1,,\"Send trade and quote updates to STP\" run,0,0,q,.proc.sys \"sleep 2\",1,,\"Wait for updates to publish\" true,0,0,q,(t1+10 5)~wdbHandles[`all`sym] @\\: \"count trade\",1,,\"Check trade update was correctly published\" true,0,0,q,(q1+10 0 10)~(value wdbHandles) @\\: \"count quote\",1,,\"Check quote update was correctly published\" The first thing that happens is that TorQ connection management is set up, then handles are opened to the STP and WDBs and some test updates are sent to the STP. Finally, the test process grabs the latest table counts from the WDBs and checks they have updated correctly. Note that functions and variables not defined in the test are brought in from the settings and helper function files. Our test directory now looks like this: wdbtests |---- process.csv |---- run.sh |---- settings.q |---- test.csv","title":"Integration Testing: WDB Subscription Tests"},{"location":"unittesting/#adding-functionality-to-the-run-script","text":"In order to make our tests more dynamic and useful there are a few things we can do. In the KDBTESTS directory there is a flagparse.sh script which contains some basic code for parsing command line flags. If we add this line to the top of our run script: source $KDBTESTS/flagparse.sh We can now pass flags to our run script, and these are as follows: Flag Description -d Starts the test process in debug mode -s Starts the test process in debug and stop mode (thrown out to q prompt on error or test failure) -w Writes test results to CSVs on disk -q Starts the test process in quiet mode -r timestamp Passes a timestamp into the tests for on-disk versioning The other addition to be made is a -testresults flag to the TorQ start line in the run script. This passes in a folder where the test process will store its logs and test results in a date-partitioned folder structure. Our run script now looks like the following: #!/bin/bash # Handle command-line arguments source $KDBTESTS/flagparse.sh # Path to test directory testpath=${KDBTESTS}/demo # Start procs ${TORQHOME}/torq.sh start discovery1 stp1 wdball wdbsymfilt wdbtabfilt -csv ${testpath}/process.csv # Start test proc /usr/bin/rlwrap q ${TORQHOME}/torq.q \\ -proctype test -procname test1 \\ -test ${testpath} \\ -load ${KDBTESTS}/helperfunctions.q ${testpath}/settings.q \\ -testresults ${KDBTESTS}/demo/results/ \\ -runtime $run \\ -procfile ${testpath}/process.csv \\ $debug $stop $write $quiet # Shut down procs ${TORQHOME}/torq.sh stop discovery1 stp1 wdball wdbsymfilt wdbtabfilt -csv ${testpath}/process.csv Near the end we have four variables which are optional flags that will be added by the flag parser script. We now have plenty of options when it comes to running the script: # Run in debug mode ./run.sh -d # Run in debug and stop mode ./run.sh -s # Run in quiet mode, pass in a runtime and write results to disk ./run.sh -r 2020.10.12D16:34:44.261143000 -wq","title":"Adding functionality to the run script"},{"location":"unittesting/#running-multiple-test-sets","text":"When testing a large piece of functionality it is very likely that there will a large number of tests to run. It is advised in such a case that the tests are split up into different technical groupings, each with their own folder with test CSVs, process and settings files and run scripts. This is essentially creating an isolated environment for each group of tests. The issue though is that it becomes cumbersome to run all tests, and so a script has been created which executes all the run scripts in each of the directories, saves their results to disk and displays them at the end of the run. The runall.q script lives in the KDBTESTS directory and it takes two arguments, -rundir and -resdir , which are the top level directory where your test sub-folders live, and the folder where the test logs and results are kept respectively. This script runs each of the run.sh scripts in each of the sub-folders in the -rundir folder in write and quiet mode, passing in the current timestamp so that all will be saved to disk in an easily-versioned manner. An example run would look like the following: q runall.q -rundir /path/to/top/test/folder/ -resdir /path/to/results/folder/","title":"Running Multiple Test Sets"},{"location":"unittesting/#debugging-tests","text":"In the scenario where there are a large number of tests spread across several folders, you can use the run-all mechanic to test them all. However, say there is an error in one of the test files. Normally, this would be fairly difficult to track down and deal with, but the features of this framework make it much easier. At the end of the run, three things will be shown on the console, a results table, a failures table and dictionary of error logs and any errors in them. Any tests that fail will be displayed in the fails table and any code errors that occur will be logged to disk and displayed in the dictionary, and both of these sources contain the file the test was in, the line number it is on and the code itself. A demo folder has been prepared which contains a code error and a test failure. All the tests in the folder were run as follows: q runall.q -rundir demo -resdir demo/results When the tests finish running the results are displayed on the console. There are various failed tests and an entry appears in our error logs which can be expanded out: ... \"Logged errors:\" :demo/results/2020.11.11/logs/err_eod.log| () :demo/results/2020.11.11/logs/err_wdb.log| ,\"2020.11.11D16:37:21.716171000|aquaq-184|test|test1|ERR|KUexecerr|run error in file :/ho..\" ... q) raze value errors \"2020.11.11D16:37:21.716171000|aquaq-184|test|test1|ERR|KUexecerr|run error in file :/home/mpotter/kdbCode/segtp/deploy/tests/demo/wdb/test.csv on line 8 - stHandle. Code: 'stHandle @/: `.u.upd ,/: ((`trade;testtrade);(`quote;testquote))'\" The script has read the file demo/results/2020.11.11/logs/err_wdb.log and from the message within we can see that there is a code error in a 'run' command in the file /home/mpotter/kdbCode/segtp/deploy/tests/demo/wdb/test.csv on line 8, and the offending piece of code is also displayed. This may be enough information for us to solve the issue, but if not we can dig deeper. We can see that the error is coming from the WDB tests, and so we run those in debug mode in the following way: ./demo/wdb/run.sh -d This runs the tests and displays the error message we saw earlier before outputting our test results and failures and leaving us in the q session. By examining the error message and being able to access the q session we can see that the variable stHandle should in fact be stpHandle and this typo is causing the error. Once this is fixed, it can be run again and the error doesn't appear any more. There is still one test failing, however: q) select action,code,csvline from KUerr action code csvline ----------------------------------------------------------------- true (t1+1 5)~t2:wdbHandles[`all`sym] @\\: \"count trade\" 10 The best way to debug this would be to be able to exit the tests as this line is being run and be able to examine it from there. We can do this by invoking the stop mode in our test script: ./demo/wdb/run.sh -s This throws us out to the q prompt at this point in the tests with the following error: 'failed to load /home/mpotter/kdbCode/segtp/deploy/tests/runtests.q : true test failure in file :/home/mpotter/kdbCode/segtp/deploy/tests/demo/wdb/test.csv on line 10 We can get the code which failed and run it here to see what it returns. From doing some quick debugging we can see that one of the items being added to t1 is wrong, it should be ten rather than 1. Once this is fixed we can run the WDB tests again and we see that there are now no errors and all the tests pass! We can then run all of our tests again as at the start and no new test failures come up and the latest error logs are empty.","title":"Debugging Tests"},{"location":"unittesting/#notes-on-best-practice","text":"Here are some recommendations on making test development more straightforward: Use TorQ connection management when dealing with multiple processes When just unit testing one process in isolation, run the tests from an instance of that process When performing integration tests that examine the interaction of multiple processes, run the tests from a 'blank' process where possible, as this ensures that test code and process code don't get in each other's way Put any variable or function declarations in a settings file so as not to clutter the test code If there are a large number of tests, split into folders of related tests Try to keep test output isolated from the rest of the application, ie. any logs or HDB data should be output to a separate testing location and cleared afterwards if appropriate","title":"Notes on Best Practice"},{"location":"utilities/","text":"Utilities We have provided several utility scripts, which either implement developer aids or standard operations which are useful across processes. api.q This provides a mechanism for documenting and publishing function/variable/table or view definitions within the kdb+ process. It provides a search facility both by name and definition (in the case of functions). There is also a function for returning the approximate memory usage of each variable in the process in descending order. Definitions are added using the .api.add function. A variable can be marked as public or private, and given a description, parameter list and return type. The search functions will return all the values found which match the pattern irrespective of them having a pre-defined definition. Whether a value is public or private is defined in the definitions table. If not found then by default all values are private, except those which live in the .q or top level namespace. .api.f is used to find a function, variable, table or view based on a case-insensitive pattern search. If a symbol parameter is supplied, a wildcard search of *[suppliedvalue]* is done. If a string is supplied, the value is used as is, meaning other non-wildcard regex pattern matching can be done. q).api.f`max name | vartype namespace public descrip .. --------------------| -----------------------------------------------.. maxs | function .q 1 \"\" .. mmax | function .q 1 \"\" .. .clients.MAXIDLE | variable .clients 0 \"\" .. .access.MAXSIZE | variable .access 0 \"\" .. .cache.maxsize | variable .cache 1 \"The maximum size in.. .cache.maxindividual| variable .cache 1 \"The maximum size in.. max | primitive 1 \"\" .. q).api.f\"max*\" name| vartype namespace public descrip params return ----| ------------------------------------------------ maxs| function .q 1 \"\" \"\" \"\" max | primitive 1 \"\" \"\" \"\" .api.torqnamespaces is a variable which returns a symbol list of torq namespaces. .api.exportconfig uses the table returned by .api.f` to give a table of the current values and descriptions of variables within the inputted namespace. This can be used to quickly see what configurable variables are currently set to. .api.exportallconfig is .api.exportconfig evaluated with all the available torqnamespaces and returns the same format as .api.exportconfig. .api.p is the same as .api.f, but only returns public functions. .api.u is as .api.p, but only includes user defined values i.e. it excludes q primitives and values found in the .q, .Q, .h and .o namespaces. .api.find is a more general version of .api.f which can be used to do case sensitive searches. .api.s is used to search function definitions for specific values. q).api.s\"*max*\" function definition .. ---------------------------------------------------------------------.. .Q.w \"k){`used`heap`peak`wmax`mmap`mphy`syms`symw!(.\\\".. .clients.cleanup \"{if[count w0:exec w from`.clients.clients where .. .access.validsize \"{[x;y;z] $[superuser .z.u;x;MAXSIZE>s:-22!x;x;'\\.. .servers.getservers \"{[nameortype;lookups;req;autoopen;onlyone]\\n r:$.. .cache.add \"{[function;id;status]\\n \\n res:value function;\\n.. .api.m is used to return the approximate memory usage of variables and views in the process, retrieved using -22!. Views will be re-evaluated if required. Use .api.mem[0b] if you do not want to evaluate and return views. q).api.m[] variable size sizeMB -------------------------------- .tz.t 1587359 2 .help.TXT 15409 0 .api.detail 10678 0 .proc.usage 3610 0 .proc.configusage 1029 0 .. .api.whereami[lambda] can be used to retrieve the name of a function given its definition. This can be useful in debugging. q)g:{x+y} q)f:{20 + g[x;10]} q)f[10] 40 q)f[`a] {x+y} `type + `a 10 q)).api.whereami[.z.s] `..g apidetails.q This file in both the common and the handler directories is used to add to the api using the functions defined in api.q timer.q kdb+ provides a single timer function, .z.ts which is triggered with the frequency specified by -t. We have provided an extension to allow multiple functions to be added to the timer and fired when required. The basic concept is that timer functions are registered in a table, with .z.ts periodically checking the table and running whichever functions are required. This is not a suitable mechanism where very high frequency timers are required (e.g. sub 500ms). There are two ways a function can be added to a timer- either as a repeating timer, or to fire at a specific time. When a repeating timer is specified, there are three options as to how the timer can be rescheduled. Assuming that a timer function with period P is scheduled to fire at time T0, actually fires at time T1 and finishes at time T2, then mode 0 will reschedule for T0+P; mode 1 will reschedule for T1+P; mode 2 will reschedule for T2+P. Both mode 0 and mode 1 have the potential for causing the timer to back up if the finish time T2 is after the next schedule time. See .api.p\u201c.timer.*\u201dfor more details. async.q kdb+ processes can communicate with each using either synchronous or asynchronous calls. Synchronous calls expect a response and so the server must process the request when it is received to generate the result and return it to the waiting client. Asynchronous calls do not expect a response so allow for greater flexibility. The effect of synchronous calls can be replicated with asynchronous calls in one of two ways (further details in section gateway): deferred synchronous: the client sends an async request, then blocks on the handle waiting for the result. This allows the server more flexibility as to how and when the query is processed; asynchronous postback: the client sends an async request which is wrapped in a function to be posted back to the client when the result is ready. This allows the server flexibility as to how and when the query is processed, and allows the client to continue processing while the server is generating the result. The code for both of these can get a little tricky, largely due to the amount of error trapping required. We have provided two functions to allow these methods to be used more easily. .async.deferred takes a list of handles and a query, and will return a two item list of (success;results). q).async.deferred[3 5;({system\"sleep 1\";system\"p\"};())] 1 1 9995 9996 q).async.deferred[3 5;({x+y};1;2)] 1 1 3 3 q).async.deferred[3 5;({x+y};1;`a)] 0 0 \"error: server fail:type\" \"error: server fail:type\" q).async.deferred[3 5 87;({system\"sleep 1\";system\"p\"};())] 1 1 0 9995i 9996i \"error: comm fail: failed to send query\" .async.postback takes a list of handles, a query, and the name or lambda of the postback function to return the result to. It will immediately return a success vector, and the results will be posted back to the client when ready. q).async.postback[3 5;({system\"sleep 1\";system\"p\"};());`showresult] 11b q) q)9995i 9996i q).async.postback[3 5;({x+y};1;2);`showresult] 11b q)3 3 q).async.postback[3 5;({x+y};1;`a);`showresult] 11b q)\"error: server fail:type\" \"error: server fail:type\" q).async.postback[3 5;({x+y};1;`a);showresult] 11b q)\"error: server fail:type\" \"error: server fail:type\" q).async.postback[3 5 87;({x+y};1;2);showresult] 110b q)3 3 For more details, see .api.p\u201c.async.*\u201d. cache.q cache.q provides a mechanism for storing function results in a cache and returning them from the cache if they are available and non stale. This can greatly boost performance for frequently run queries. The result set cache resides in memory and as such takes up space. It is up to the programmer to determine which functions are suitable for caching. Likely candidates are those where some or all of the following conditions hold: the function is run multiple times with the same parameters (perhaps different clients all want the same result set); the result set changes infrequently or the clients can accept slightly out-of-date values; the result set is not too large and/or is relatively expensive to produce. For example, it does not make sense to cache raw data extracts. The cache has a maximum size and a minimum size for any individual result set, both of which are defined in the configuration file. Size checks are done with -22! which will give an approximation (but underestimate) of the result set size. In the worst case the estimate could be half the size of the actual size. If a new result set is to be cached, the size is checked. Assuming it does not exceed the maximum individual size then it is placed in the cache. If the new cache size would exceed the maximum allowed space, other result sets are evicted from the cache. The current eviction policy is to remove the least recently accessed result sets until the required space is freed. The cache performance is tracked in a table. Cache adds, hits, fails, reruns and evictions are monitored. The main function to use the cache is .cache.execute[function; staletime]. If the function has been executed within the last staletime, then the result is returned from the cache. Otherwise the function is executed and placed in the cache. The function is run and the result placed in the cache: q)\\t r:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01] 2023 q)r 3 The second time round, the result set is returned immediately from the cache as we are within the staletime value: q)\\t r1:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01] 0 q)r1 3 If the time since the last execution is greater than the required stale time, the function is re-run, the cached result is updated, and the result returned: q)\\t r2:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:00] 2008 q)r2 3 The cache performance is tracked: q).cache.getperf[] time id status function ------------------------------------------------------------------ 2013.11.06D12:41:53.103508000 2 add {system\"sleep 2\"; x+y} 1 2 2013.11.06D12:42:01.647731000 2 hit {system\"sleep 2\"; x+y} 1 2 2013.11.06D12:42:53.930404000 2 rerun {system\"sleep 2\"; x+y} 1 2 See .api.p.cache.*for more details. email.q A library file is provided to allow TorQ processes to send emails using an SMTP server. This is a wrapper around the standard libcurl library. The library file is currently available for Windows (32 bit), Linux (32 and 64 bit) and OSX (32 and 64 bit). The associated q script contains two main methods for creating a connection and sending emails. The email library requires a modification to the path to find the required libs - see the top of email.q for details. The main connection method .email.connect takes a single dictionary parameter and returns 0i for success and -1i for failure. Parameter Req Type Description url Y symbol URL of mail server e.g. smtp://mail.example.com user Y symbol Username of user to login as password Y symbol Password for user usessl N boolean Connect using SSL/TLS, defaults to false from N symbol Email from field, defaults to torq@aquaq.co.uk debug N integer Debug level. 0=no output, 1=normal output, 2=verbose output. Default is 1 An example is: q).email.connect[`url`user`password`from`usessl`debug!(`$\"smtp://mail.example.com:80\";`$\"torquser@aquaq.co.uk\";`hello;`$\"torquser@aquaq.co.uk\";0b;1i)] 02 Jan 2015 11:45:19 emailConnect: url is set to smtp://mail.example.com:80 02 Jan 2015 11:45:19 emailConnect: user is set to torquser@aquaq.co.uk 02 Jan 2015 11:45:19 emailConnect: password is set 02 Jan 2015 11:45:19 emailConnect: from is set torquser@aquaq.co.uk 02 Jan 2015 11:45:19 emailConnect: trying to connect 02 Jan 2015 11:45:19 emailConnect: connected, socket is 5 0i The email sending function .email.send takes a single dictionary parameter containing the details of the email to send. A connection must be established before an email can be sent. The send function returns an integer of the email length on success, or -1 on failure. Parameter Req Type Description to Y symbol (list) addresses to send to subject Y char list email subject body Y list of char lists email body cc N symbol (list) cc list bodyType N symbol type of email body. Can be `text or `html. Default is `text debug N integer Debug level. 0=no output, 1=normal output,2=verbose output. Default is 1 An example is: q).email.send[`to`subject`body`debug!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\");1i)] 02 Jan 2015 12:39:29 sending email with subject: test email 02 Jan 2015 12:39:29 email size in bytes is 16682 02 Jan 2015 12:39:30 emailSend: email sent 16682i Note that if emails are sent infrequently the library must re-establish the connection to the mail server (this will be done automatically after the initial connection). In some circumstances it may be better to batch emails together to send, or to offload email sending to separate processes as communication with the SMTP server can take a little time. Two additional functions are available, .email.connectdefault and .email.senddefault. These are as above but will use the default configuration defined within the configuration files as the relevant parameters passed to the methods. In addition, .email.senddefault will automatically establish a connection. q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))] 2015.01.02D12:43:34.646336000|aquaq||discovery1|INF|email|sending email 2015.01.02D12:43:35.743887000|aquaq||discovery1|INF|email|connection to mail server successful 2015.01.02D12:43:37.250427000|aquaq|discovery1|INF|email|email sent 16673i q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\"))] 2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email 2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent 16675i q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\");`\"$/home/ashortt/example.txt\")] 2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email 2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent 47338i .email.test will attempt to establish a connection to the default configured email server and send a test email to the specified address. debug should be set to 2i (verbose) to extract the full information. q).email.debug:2i q).email.test `$\"test@aquaq.co.uk\" ... A further function .email.sendviaservice can be used to send an email using the default mail server on a separate specified process and can be used to allow latency sensitive processes to offload this piece of functionality. The function takes two parameters a process and a dictionary which should follow the same format as .email.send. The function uses the .async.postback Utility to send the email by calling .email.servicesend on the specified process. The postback function immediately returns a success boolean indicating that the the async request has been sent and when the function has been run on the server the results are posted back to the client function email.servicecallback which logs the email status. q).email.sendviaservice[`emailservice;`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))] 1b q)2019.01.04D12:02:57.641940000|gateway|gateway1|INF|email|Email sent successfully Additionally functions are available within the email library. See .api.p.email.*for more details. Emails with SSL certificates from Windows If you wish to send emails via an account which requires authentication from Windows (e.g. Hotmail, Gmail) then you have to do a few extra steps as usessl must be true and Windows does not usually find the correct certificate. The steps are: download this and save it to your PC set CURLOPT_CAINFO=c:/path/to/cabundle_file/ca-bundle.crt More information is available here and here timezone.q A slightly customised version of the timezone conversion functionality from code.kx. It loads a table of timezone information from $KDBCONFIG. See .api.p.tz.*for more details. compress.q compress.q applies compression to any kdb+ database, handles all partition types including date, month, year, int, and can deal with top level splayed tables. It will also decompress files as required. Once the compression/decompression is complete, summary statistics are returned, with detailed statistics for each compressed or decompressed file held in a table. The utility is driven by the configuration specified within a csv file. Default parameters can be given, and these can be used to compress all files within the database. However, the compress.q utility also provides the flexibility to compress different tables with different compression parameters, and different columns within tables using different parameters. A function is provided which will return a table showing each file in the database to be compressed, and how, before the compression is performed. Compression is performed using the -19! operator, which takes 3 parameters; the compression algorithm to use (0 - none, 1 - kdb+ IPC, 2 - gzip), the compression blocksize as a power of 2 (between 12 and 19), and the level of compression to apply (from 0 - 9, applicable only for gzip). (For further information on -19! and the parameters used, see code.kx.com.) The compressionconfig.csv file should have the following format: table,minage,column,calgo,cblocksize,clevel default,20,default,2,17,6 trades,20,default,1,17,0 quotes,20,asize,2,17,7 quotes,20,bsize,2,17,7 This file can be placed in the config folder, or a path to the file given at run time. The compression utility compresses all tables and columns present in the HDB but not specified in the driver file according the default parameters. In effect, to compress an entire HDB using the same compression parameters, a single row with name default would suffice. To specify that a particular table should be compressed in a certain different manner, it should be listed in the table. If default is given as the column for this table, then all of the columns of that table will be compressed accordingly. To specify the compression parameters for particular columns, these should be listed individually. For example, the file above will compress trades tables 20 days old or more with an algorithm of 1, and a blocksize of 17. The asize and bsize columns of any quotes tables older than 20 days old will be compressed using algorithm 2, blocksize 17 and level 7. All other files present will be compressed according to the default, using an algorithm 2, blocksize 17 and compression level 6. To leave files uncompressed, you must specify them explicitly in the table with a calgo of 0. If the file is already compressed, note that an algorithm of 0 will decompress the file. This utility should be used with caution. Before running the compression it is recommended to run the function .cmp.showcomp, which takes three parameters - the path to the database, the path to the csv file, and the maximum age of the files to be compressed: .cmp.showcomp[`:/full/path/to/HDB;.cmp.inputcsv;maxage] /- for using the csv file in the config folder .cmp.showcomp[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage] /- to specify a file This function produces a table of the files to be compressed, the parameters with which they will be compressed, and the current size of the file. Note that the current size column is calculated using hcount; on a file which is already compressed this returns the uncompressed length, i.e. this cannot be used as a signal as to whether the file is compressed already. fullpath column table partition age calgo cblocksize clevel compressage currentsize ------------------------------------------------------------------------------------- :/home/hdb/2013.11.05/depth/asize1 asize1 depth 2013.11.05 146 0 17 8 1 787960 :/home/hdb/2013.11.05/depth/asize2 asize2 depth 2013.11.05 146 0 17 8 1 787960 :/home/hdb/2013.11.05/depth/asize3 asize3 depth 2013.11.05 146 0 17 8 1 787960 :/home/hdb/2013.11.05/depth/ask1 ask1 depth 2013.11.05 146 0 17 8 1 1575904 .... To then run the compression function, use .cmp.compressmaxage with the same parameters as .cmp.showcomp (hdb path, csv path, maximum age of files): .cmp.compressmaxage[`:/full/path/to/HDB;.cmp.inputcsv;maxage] /- for using the csv file in the config folder .cmp.compressmaxage[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage] /- to specify a file To run compression on all files in the database disregarding the maximum age of the files (i.e. from minage as specified in the configuration file to infinitely old), then use: .cmp.docompression[`:/full/path/to/HDB;.cmp.inputcsv] /- for using the csv file in the config folder .cmp.docompression[`:/full/path/to/HDB;`:/full/path/to/csvfile] /- to specify a file Logs are produced for each file which is compressed or decompressed. Once the utility is complete, the statistics of the compression are also logged. This includes the memory savings in MB from compression, the additional memory usage in MB for decompression, the total compression ratio, and the total decompression ratio: |comp1|INF|compression|Memory savings from compression: 34.48MB. Total compression ratio: 2.51. |comp1|INF|compression|Additional memory used from de-compression: 0.00MB. Total de-compression ratio: . |comp1|INF|compression|Check .cmp.statstab for info on each file. A table with the compressed and decompressed length for each individual file, in descending order of compression ratio, is also produced. This can be found in .cmp.statstab: file algo compressedLength uncompressedLength compressionratio ----------------------------------------------------------------------------------- :/hdb/2014.03.05/depth/asize1 2 89057 772600 8.675343 :/hdb/2014.01.06/depth/asize1 2 114930 995532 8.662073 :/hdb/2014.03.05/depth/bsize1 2 89210 772600 8.660464 :/hdb/2014.03.12/depth/bsize1 2 84416 730928 8.658643 :/hdb/2014.01.06/depth/bsize1 2 115067 995532 8.651759 ..... A note for windows users - windows supports compression only with a compression blocksize of 16 or more. dataloader.q This script contains some utility functions to assist in loading data from delimited files (e.g. comma separated, tab delimited). It is a more generic version of the data loader example on code.kx . The supplied functions allow data to be read in configurable size chunks and written out to the database. When all the data is written, the on-disk data is re-sorted and the attributes are applied. The main function is .loader.loadalldata which takes two parameters- a dictionary of loading parameters and a directory containing the files to read. The dictionary should/can have the following fields: Parameter Req Type Description headers Y symbol list Names of the header columns in the file types Y char list Data types to read from the file separator Y char[list] Delimiting character. Enlist it if first line of file is header data tablename Y symbol Name of table to write data to dbdir Y symbol Directory to write data to symdir N symbol Directory to enumerate against partitiontype N symbol Partitioning to use. Must be one of `date`month`year`int. Default is `date partitioncol N symbol Column to use to extract partition information.Default is `time dataprocessfunc N function Diadic function to process data after it has been read in. First argument is load parameters dictionary, second argument is data which has been read in. Default is {[x;y] y} chunksize N int Data size in bytes to read in one chunk. Default is 100 MB compression N int list Compression parameters to use e.g. 17 2 6. Default is empty list for no compression gc N boolean Whether to run garbage collection at appropriate points. Default is 0b (false) filepattern N char[list] Pattern used to only load certain files e.g. \" .csv\",(\" .csv\",\"*.txt\") Example usage: .loader.loadallfiles[`headers`types`separator`tablename`dbdir!(`sym`time`price`volume;\"SP FI\";\",\";`trade;`:hdb); `:TDC/toload] .loader.loadallfiles[`headers`types`separator`tablename`dbdir`dataprocessfunc`chunksize`partitiontype`partitioncol`compression`gc`filepattern!(`sym`time`price`volume;\"SP FI\";enlist\",\";`tradesummary;`:hdb;{[p;t] select sum size, max price by date:time.date from t};`int$500*2 xexp 20;`month;`date;16 1 0;1b;(\"*.csv\";\"*.txt\")); `:TDC/toload] subscriptions.q The subscription utilities allow multiple subscriptions to different data sources to be managed and maintained. Automatic resubscriptions in the event of failure are possible, along as specifying whether the process will get the schema and replay the log file from the remote source (e.g. in the case of tickerplant subscriptions). .sub.getsubscriptionhandles is used to get a table of processes to subscribe to. It takes a process type and process name, where () or a null symbol can be used for all: .sub.getsubscriptionhandles[`tickerplant;();()!()] / all processes of type tickerplant .sub.getsubscriptionhandles[`;`rdb1;()!()] / all processes called 'rdb1' .sub.getsubscriptionhandles[`;`;()!()] / all processes .sub.getsubscriptionhandles[();();()!()] / nothing .sub.subscribe is used to subscribe to a process for the supplied list of tables and instruments. For example, to subscribe to instruments A, B and C for the quote table from all tickerplants: .sub.subscribe[`trthquote;`A`B;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;();()!()] The subscription method uses backtick for \u201call\u201d (which is the same as kdb+tick). To subscribe to all tables, all instruments, from all tickerplants: .sub.subscribe[`;`;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;();()!()] See .api.p\u201c.sub.*\u201d for more details. pubsub.q This file defines the .ps namespace, which contains various functions for subscribing and publishing to processes. These functions have traditionally been wrappers for some of the functions found in u.q , but the advent of the Segmented Tickerplant has brought about a more fully-featured pub/sub library, which is now leveraged by the .ps functions. This library is part of the 'common' code, so to load it in by default .proc.loadcommoncode must be set to true. The following three functions are primarily associated with the pub/sub library: .ps.initialise - this is a wrapper for .stpps.init and it sets up several data structures in the .stpps namespace using the tables loaded into memory. This is automatically called on process start up. .ps.subscribe - this wraps .u.sub and it receives subscription requests from other processes and populates subscription tables in memory .ps.publish - this is a wrapper for .stpps.pub and it publishes data to subscribers using the information given on subscription For example: // Subscribe to all tables and symbols handletosubscriber(`.ps.subscribe;`;`) // Subscribe to all tables and subset of symbols handletosubscriber(`.ps.subscribe;`;`AAPL`GOOG) // Subscribe to Microsoft quotes only handletosubscriber(`.ps.subscribe;`quote;enlist `MSFT) There are two new functions which have been added that wrap .u.sub with the goal of making it easier for non-kdb+ processes to subscribe using strings: .ps.subtable - accepts two strings, a table and a comma-separated list of instruments respectively .ps.subtablefiltered - accepts 3 strings representing a table, where clause and a list of columns For example: // Subscribe to Google and Apple trades handletoSTP(`.ps.subtable;\"trade\";\"GOOG\",\"AAPL\") // Subscribe to time, sym and bid price data for quotes where bid > 50 .ps.subtablefiltered[\"quote\";\"bid>50.0\";\"time,sym,bid\"] Subscribing to the STP works in a very similar fashion to the original tickerplant. From the subscriber's perspective the subscription logic is backwardly compatible: it opens a handle to the STP and calls .u.sub with a list of tables to subscribe to as its first argument and either a null symbol or a list of symbols as a sym filter. The STP also supports a keyed table of conditions (in q parse format) and a list of columns that should be published. Whilst complex bespoke subscription is possible in the STP it is generally not recommended. Complex subscription filtering should be off loaded to a chained STP. // Subscribe to everything handletoSTP(`.u.sub;`;`) // Subscribe GOOG and AAPL symbols in the trade table handletoSTP(`.u.sub;`trade;`GOOG`AAPL) // Subscribe to all tables but with custom conditions handletoSTP(`.u.sub;`;conditions) ... q) show conditions tabname| filts columns -------| ---------------------------------- trade | \"\" \"time,sym,price\" quote | \"bid>100,bid<200\" \"\" Here subscribing subject to the conditions table results in the subscriber only receiving quotes where the bid is between 100 and 200. Also only the time, sym and price columns of the trade table are published to the subscriber. Note that it is also possible to use the conditions table to subscribe to just one of the trade or quote tables. A conditions table may also be used to perform calculations on columns and define new ones as well: q) show conditions tabname| filts columns -------| ---------------------------------------------------- quote | \"bid>100,bid<200\" \"time,sym,bid,ask,mid:0.5*bid+ask\" q)quote time sym bid ask mid ------------------------------------------------------ 2020.12.09D15:29:23.183738000 INTC 58.6 59.4 59 2020.12.09D15:29:23.183738000 DOW 21.39 22.53 21.96 ... For more information on subscriptions, see the documentation on the segmented tickerplant process. Data striping Data striping is the technique of logically segmenting data between processes so that they are stored on different processes. The advantages of striping include performance and throughput improvements. Segmenting data on separate processes reduces latency when processing data. Total data throughput is increased as multiple processes can be accessed concurrently. In addition, the memory footprint per process is lower and processes can be split across multiple hosts. It is useful for load balancing across processes. The reduction in data access in each process cumulatively multiplies the data throughput by the number of processes. It also allows the process to complete its task faster and without interruption, thereby reducing latency. Example of data striping in TorQ A simple but effective way of data striping is to do it divide the data randomly across all processes. This will ensure an even distribution of data. However, querying (locating and retrieving) data can become complicated. A common method for data striping between processes is to use an instrument (sym) filter. For example, to stripe data across 2 RDB processes, data with symbols starting with A-M and N-L will be striped to RDB1 and RDB2 respectively. However, a major problem with this method is the uneven distribution of data (a lot of symbols tend to start with A for example). Data hash striping A way to get around this problem is to stripe the data using a hash value which allows for better distribution. The hash function will store the mappings for the symbols that it has already computed and for subsequent requests for those symbols, it looks them up. It is loaded into the segmented tickerplant to use as subscription requests. The hash function has to be highly performant as it will potentially be invoked ~100-1000times/s in a core (potential bottleneck) component. For this purpose, a fast, simple and non-cryptographic hash function is created to segment the data since the performance of the segmented tickerplant is of paramount importance. The hash map is created by summing the ASCII codes of each input string then dividing by the number of segements. A hash map based on the sym column is created like so: q)show sym:`$-100000?(thrl cross .Q.A),thrl:(.Q.A cross .Q.A cross .Q.A) `AWNM`MFLD`XWTL`OVIH`HAMI`OFDX`MWNC`BLJT`MVOR`TVAG`RGUU`MZCY`HLTW`RJVZ`MAYS`O.. q)sym!(sum each string sym)mod 4 AWNM| 3 MFLD| 3 XWTL| 3 OVIH| 2 HAMI| 3 OFDX| 1 MWNC| 1 BLJT| 0 MVOR| 0 .. Using a hash function ensures better distribution of symbols across databases: q)count each group sym!(sum each string sym)mod 4 3| 25077 2| 24920 1| 24877 0| 25126 An advantage of using data hash striping is such that a specific symbol will always be striped to the exact location based on the number of segments. Setting up data striping in TorQ 1) Example setup for data striping across ALL (4) RDB instances $KDBCONFIG/process.csv The process file should contain the RDB instances by specifying a different port and procname column. Set the load column to ${KDBCODE}/processes/rdb.q . NOTE The procname convention should always start with rdb1 Increment by 1 for each RDB instance In the following format: rdb{i} There is no need to create separate rdb{i}.q files $KDBCONFIG/process.csv should look something like this: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+1,discovery,discovery1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/discovery.q,1,,q localhost,{KDBBASEPORT}+2,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -tplogdir ${KDBTPLOG},q localhost,{KDBBASEPORT}+3,rdb,rdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+4,rdb,rdb2,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+5,rdb,rdb3,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+6,rdb,rdb4,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q $KDBCONFIG/rdbsub/rdbsub{i}.csv The rdbsub{i}.csv should be modified like this: tabname,filters,columns trade,.ds.stripe[sym;{i-1}], quote,.ds.stripe[sym;{i-1}], $KDBAPPCONFIG/settings/rdb.q Set .rdb.subfiltered: 1b (default is 0b) $KDBAPPCONFIG/settings/default.q Add .ds.numseg: {4}i 2) Example setup for data striping across SOME RDB instances 2 RDB instances unfiltered 2 RDB instances striped $KDBCONFIG/process.csv Add in -.rdb.subfiltered 1 (to enable striping) in the extras column for the striped RDB instances. NOTE It is -.rdb.subfiltered 1 and not -.rdb.subfiltered 1b The RDB instances must be grouped according to those being striped first i.e. rdb1 , rdb2 are striped and rdb3 , rdb4 are unfiltered $KDBCONFIG/process.csv should look something like this: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+1,discovery,discovery1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/discovery.q,1,,q localhost,{KDBBASEPORT}+2,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -tplogdir ${KDBTPLOG},q localhost,{KDBBASEPORT}+3,rdb,rdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,-.rdb.subfiltered 1,q localhost,{KDBBASEPORT}+4,rdb,rdb2,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,-.rdb.subfiltered 1,q localhost,{KDBBASEPORT}+5,rdb,rdb3,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+6,rdb,rdb4,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q $KDBAPPCONFIG/settings/rdb.q Ensure .rdb.subfiltered: 0b $KDBAPPCONFIG/settings/default.q Add .ds.numseg: {2}i 3) Data striping for general subscribers {subscriber}.q Add the conditions required (using the data hash striping function). ... // Subscribe to all tables but with custom conditions conditions:1!flip`tabname`filters`columns!(`trade`quote;(\".ds.stripe[sym;0]\";\".ds.stripe[sym;1]\");(\"\";\"\")) handletoSTP(`.u.sub;`;conditions) ... q)conditions tabname| filters columns -------| ---------------------------------- trade | \".ds.stripe[sym;0]\" \"\" quote | \".ds.stripe[sym;1]\" \"\" q)handletoSTP\".stpps.subrequestfiltered\" tbl handle filts columns --------------------------------------------------------- logmsg 21 () packets 21 () quote 21 (`.ds.stripe;`sym;1) quote_iex 21 () trade 21 (`.ds.stripe;`sym;0) trade_iex 21 () kafka.q kafka.q provides q language bindings for Apache Kafka, a 'distributed streaming platform', a real time messaging system with persistent storage in message logs. The core functionality of Kafka \u2013 pub/sub messaging with persisted logs, will be familiar to most readers as the functionality offered by the kdb+ tick tickerplant. The tickerplant log allows the real time database and other consumers to replay a day\u2019s events to recover state. An application architecture built around Kafka could dispense with a tickerplant component, and have RDBs and other real time clients query Kafka on startup for offsets, and play back the data they need. While not suitable for very low latency access to streaming data, it would carry some advantages for very high throughput applications, particularly those in the cloud: Kafka\u2019s distributed nature should allow it to scale more transparently than splitting tickerplants by instrument universe or message type Replaying from offsets is the same interface as live pub/sub and doesn\u2019t require filesystem access to the tickerplant log, so RDB\u2019s and other consumer could be on a different server By default, the Kafka bindings will be loaded into all TorQ processes running on l64 systems (the only platform currently supported). An example of usage is shown here (this assumes a local running instance of kafka - instructions for this are available on the kafkaq github repo): q).kafka.initconsumer[`localhost:9092;()] q).kafka.initproducer[`localhost:9092;()] q)kupd / print default definition for incoming data - ignore key, print message as ascii {[k;x] -1 `char$x;} q).kafka.subscribe[`test;0] / subscribe to topic test, partition 0 q)pub:{.kafka.publish[`test;0;`;`byte$x]} / define pub to publish text input to topic test on partition 0 with no key defined q)pub\"hello world\" q)hello world Limitations of the current implementation: Only l64 supported Single consumer thread subscribed to one topic at a time tplogutils.q tplogutils.q contains functions for recovering tickerplant log files. Under certain circumstances the tickerplant log file can become corrupt by having an invalid sequence of bytes written to it. A log file can be recovered using a simple recovery method. However, this will only recover messages up to the first invalid message. The recovery functions defined in tplogutils.q allow all valid messages to be recovered from the tickerplant log file. monitoringchecks.q monitoringchecks.q implements a set of standard, basic monitoring checks. They include checks to ensure: table sizes are increasing during live capture the HDB data saves down correctly the allocated memory of a process does not increase past a certain size the size of the symbol list in memory doesn\u2019t grow to big the process does not have too much on its pending subscriber queue These checks are intended to be run by the reporter process on a schedule, and any alerts emailed to an appropriate recipient list. heartbeat.q heartbeat.q implements heartbeating, and relies on both timer.q and pubsub.q. A table called heartbeat will be published periodically, allowing downstream processes to detect the availability of upstream components. The heartbeat table contains a heartbeat time and counter. The heartbeat script contains functions to handle and process heartbeats and manage upstream process failures. See .api.p.hb.*for details. rmvr.q This file contains a function which can be used to convert environment variable paths into a full path from the root directory. os.q A file with various q functions to perform system operations. This will detect your operating system and will perform the correct commands depending on what you are using. This is a modification of a script developed by Simon Garland. dbwriteutils.q This contains a set of utility functions for writing data to historic databases. Sorting and Attributes The sort utilities allow the sort order and attributes of tables to be globally defined. This helps to manage the code base when the data can potentially be written from multiple locations (e.g. written from the RDB, loaded from flat file, replayed from the tickerplant log). The configuration is defined in a csv which defaults to $KDBCONFG/sort.csv. The default setup is that every table is sorted by sym and time, with a p attribute on sym (this is the standard kdb+ tick configuration). aquaq$ tail config/sort.csv tabname,att,column,sort default,p,sym,1 default,,time,1 As an example, assume we have an optiontrade table which we want to be different from the standard set up. We would like the table to be sorted by optionticker and then time, with a p attribute on optionticker. We also have a column called underlyingticker which we can put an attribute on as it is derived from optionticker (so there is an element of de-normalisation present in the table). We also have an exchange field which we would like to put a g attribute on. All other tables we want to be sorted and parted in the standard way. The configuration file would look like this (sort order is derived from the order within the file combined with the sort flag being set to true): aquaq$ tail config/sort.csv tabname,att,column,sort default,p,sym,1 default,,time,1 optiontrade,p,optionticker,1 optiontrade,,exchtime,1 optiontrade,p,underlyingticker,0 optiontrade,g,exchange,0 To invoke the sort utilities, supply a list of (tablename; partitions) e.g. q).sort.sorttab(`trthtrade;`:hdb/2014.11.20/trthtrade`:hdb/2014.11.20/trthtrade) 2014.12.03D09:56:19.214006000|aquaq|test|INF|sort|sorting the trthtrade table 2014.12.03D09:56:19.214045000|aquaq|test|INF|sorttab|No sort parameters have been specified for : trthtrade. Using default parameters 2014.12.03D09:56:19.214057000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.19/trthtrade/ by these columns : sym, time 2014.12.03D09:56:19.219716000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.19/trthtrade/ 2014.12.03D09:56:19.220846000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.20/trthtrade/ by these columns : sym, time 2014.12.03D09:56:19.226008000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.20/trthtrade/ 2014.12.03D09:56:19.226636000|aquaq|test|INF|sort|finished sorting the trthtrade table A different sort configuration file can be loaded with .sort.getsortcsv[`:file] Garbage Collection The garbage collection utility prints some debug information before and after the garbage collection. q).gc.run[] 2014.12.03D10:22:51.688435000|aquaq|test|INF|garbagecollect|Starting garbage collect. mem stats: used=2 MB; heap=1984 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB 2014.12.03D10:22:53.920656000|aquaq|test|INF|garbagecollect|Garbage collection returned 1472MB. mem stats: used=2 MB; heap=512 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB Table Manipulation The table manipulation utilities allow table manipulation routines to be defined in a single place. This is useful when data can be written from mutliple different processes e.g. RDB, WDB, or tickerplant log replay. Instead of having to create a separate definition of customised manipulation in each process, it can be done in a single location and invokved in each process. help.q The standard help.q from code.kx provides help utilities in the console. This should be kept up to date with [ code.kx ]. q)help` adverb | adverbs/operators attributes| data attributes cmdline | command line parameters data | data types define | assign, define, control and debug dotz | .z locale contents errors | error messages save | save/load tables syscmd | system commands temporal | temporal - date & time casts verbs | verbs/functions html.q An HTML utility has been added to accompany the HTML5 front end for the Monitoring process. It includes functions to format dates, tables to csv to configure the HTML file to work on the correct process. It is accessible from the .html namespace. eodtime.q This script provides functionality for managing timezones. TorQ can be configured to timestamp data in a specific timezone, while also being configured to perform the end of day rollover in another timezone, at a configurable time. These options are handled by three settings: Setting Req Type Description .eodtime.rolltimeoffset Y timespan Offset from default midnight roll time .eodtime.rolltimezone Y symbol Time zone in which to rollover .eodtime.datatimezone Y symbol Time zone in which to timestamp data in TP The default configuration sets both timezones to GMT and has the rollover performed at midnight. A table containing the valid timezones is loaded into TorQ processes as .tz.t An example configuration where data is stamped in GMT, but the rollover occurs at 5PM New York time would be: .eodtime.rolltimeoffset:-0D07:00:00.000000000; // 5 PM i.e. 7 hours before midnight .eodtime.rolltimezone:`$\"America/New_YorK\"; // roll in NYC time .eodtime.datatimezone:`$\"GMT\"; // timestamp in GMT Note that the rolltimeoffset can be negative - this will cause the rollover to happen \"yesterday\", meaning that at the rolltime, the trading date will become the day after the calendar date. Where this is positive, the rollover occurs \"today\" and so the trading date will become the current calendar date. subscribercutoff.q This script is used to provide functionality for cutting off any slow subscribers on any TorQ processes. The script will periodically check (time between checks set in .subcut.checkfreq. Default is 1 minute) the byte size of the queue for all the handles on the process to see if they have exceeded a set cut-off point (set in the variable .subcut.maxsize) and will only cut-off the handle if it exceeds this limit a set number of times in a row (default is 3 and set in the .subcut.breachlimit variable). This gives clients a chance to tidy up their behavior and will avoid cutting off clients if they happened to have a spike just before the check was performed. The .subcut.state variable is used to keep track of the handles and the number of times they have exceeded the size limit in a row. To enable this functionality the .subcut.enabled flag must be set to true and the timer.q script must be loaded on the desired processes. By default the chained tickerplant is the only processes with the functionality enabled. datareplay.q The datareplay utility provides functionality for generating tickerplant function calls from historcial data which can be executed by subscriber functions. This can be used to test a known data-set against a subscriber for testing or debugging purposes. It can load this data from the current TorQ session, or from a remote hdb if given its connection handle. It can also chunk the data by time increments (as if the tickerplant was in batch mode), and can also generate calls to a custom timer function for the same time increments (defaults to .z.ts). The functions provided by this utility are made available in the .datareplay namespace. The utility is mainly used via the tabesToDataStreamFunction, which accepts a dictionary parameter with the following fields: Key Example Value Description Required Default tabs `trade`quote or `trade List of tables to include Yes N/A sts 2014.04.04D07:00:00.000 Start timestamp for data Yes N/A ets 2014.04.04D16:30:00.000 End of timestamp for data Yes N/A syms `AAPL`IBM List of symbols to include No All syms where ,(=;`src;,`L) Custom where clause in functional form No none timer 1b Generate timer function flag No 0b h 5i Handle to hdb process No 0i (self) interval 0D00:00:01.00 Time interval used to chunk data, bucketed by timestamp if no time interval set No None tc `data_time Name of time column to cut on No `time timerfunc .z.ts Timer function to use if `timer parameter is set No .z.ts When the timer flag is set, the utility will interleave timer function calls in the message column at intervals based on the interval parameter, or every 10 seconds if interval is not set. This is useful if testing requires a call to a function at a set time, to generate a VWAP every 10 minutes for example. The function the timer messages call is based on the timerfunc parameter, or .z.ts if this parameter is not set. If the interval is set the messages will be aggregated into chunks based on the interval value, if no interval is specified, the data will be bucketed by timestamp (one message chunk per distinct timestamp per table). If no connection handle is specified (h parameter), the utility will retrieve the data from the process the utility is running on, using handle 0. The where parameter allows for the use of a custom where clause when extracting data, which can be useful when the dataset is large and only certain data is required, for example if only data where src=`L is required. The where clause(s) are required to be in functional form, for example enlist (=;`src;,`L) or ((=;`src;enlist `L);(>;`size;100)) (note, that if only one custom where clause is included it is required to be enlisted). It is possible to get the functional form of a where clause by running parse on a mock select string like below: q)parse \"select from t where src=`L,size>100\" ? `t ,((=;`src;,`L);(>;`size;100)) 0b () The where clause is then the 3rd item returned in the parse tree. Examples: Extract all data between sts and ets from the trades table in the current process. q)input tabs| `trades sts | 2014.04.21D07:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:00:23.478000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. 2014.04.21D08:00:49.511000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. 2014.04.21D08:01:45.623000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. 2014.04.21D08:02:41.346000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. .. q)first .datareplay.tablesToDataStream input time| 2014.04.21D08:00:23.478000000 msg | (`upd;`trades;`sym`time`src`price`size!(`YHOO;2014.04.21D08:00:23.47800.. Extract all data between sts and ets from the trades table from a remote hdb handle=3i. q)input tabs| `trades sts | 2014.04.21D07:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 h | 3i q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:00:07.769000000 `upd `trades `sym`time`src`price`size!(`IBM;201.. 2014.04.21D08:00:13.250000000 `upd `trades `sym`time`src`price`size!(`NOK;201.. 2014.04.21D08:00:19.070000000 `upd `trades `sym`time`src`price`size!(`MSFT;20.. 2014.04.21D08:00:23.678000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. .. q)first .datareplay.tablesToDataStream input time| 2014.04.21D08:00:07.769000000 msg | (`upd;`trades;`sym`time`src`price`size!(`IBM;2014.04.21D08:00:07.769000.. Same as above but including quote table and with interval of 10 minutes: q)input tabs | `quotes`trades sts | 2014.04.21D07:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 h | 3i interval| 0D00:10:00.000000000 q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:09:47.600000000 `upd `trades +`sym`time`src`price`size!(`YHOO`A.. 2014.04.21D08:09:55.210000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize.. 2014.04.21D08:19:39.467000000 `upd `trades +`sym`time`src`price`size!(`CSCO`N.. 2014.04.21D08:19:49.068000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize.. .. q)first .datareplay.tablesToDataStream input time| 2014.04.21D08:09:47.600000000 msg | (`upd;`trades;+`sym`time`src`price`size!(`YHOO`AAPL`MSFT`NOK`DELL`YHOO`.. \u200b All messages from trades where src=`L bucketed in 10 minute intervals interleaved with calls to the function `vwap . q)input tabs | `trades h | 3i sts | 2014.04.21D08:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 where | ,(=;`src;,`L) timer | 1b timerfunc| `vwap interval | 0D00:10:00.000000000 q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:00:00.000000000 (`vwap;2014.04.21D08:00:00.000000000) .. 2014.04.21D08:09:46.258000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`.. 2014.04.21D08:10:00.000000000 (`vwap;2014.04.21D08:10:00.000000000) .. 2014.04.21D08:18:17.188000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`.. .. Modified u.q Starting in kdb+ v3.4, the new broadcast feature has some performance benefits. It works by serialising a message once before sending it asynchronously to a list of subscribers whereas the previous method would serialise it separately for each subscriber. To take advantage of this, we\u2019ve modified u.q. This can be turned off by setting .u.broadcast to false. It is enabled by default, but will only override default publishing if the kdb+ version being used is 3.4 or after. bglaunchutils.q The background launch utilities allow other processes to be programmatically launched, or terminated, from inside a TorQ process. The two main functions here are .sys.bglaunch and .sys.bgkill. .sys.bglaunch is the q function which takes a dictionary of input parameters which are then passed to a bash script. This bash script functions like torq.sh in how it starts processes. It is important to note that the background launch utilities are only supported on Linux as a result. q)input:`procname`proctype`localtime`p!(\"hdb1\";\"hdb\";\"0\";\"1234\"); The input parameter dictionary, as shown above, should contain symbols as keys and strings as values. Any standard or custom TorQ process can be launched using .sys.bglaunch, and as such the function can accept any desired command line parameters in the input dictionary. The minimum required are `procname and `proctype . In the case that only these two are used the other arguments will be given default values. Parameter Default Value U The password file used for the parent launching process, if none exists ${KDBAPPCONFIG}/passwords/accesslist.txt is used localtime The .proc.localtime value of the parent launching process p 0W - a random port will be chosen qcmd q The .sys.bgkill function is passed a single argument: the `procname of the process to be terminated, as a string. If the default value of \"0W\" is used for the port, a random port will be chosen on which to launch the process. In this case the process will neeed to register itself with discovery in order for other processes to be able to connect to it. This is a standard behaviour for TorQ processes on startup (for more information see Connection Management ). Full API The full public api can be found by running q).api.u` name | vartype namespace public descrip .. -----------------| --------------------------------------------------.. .proc.createlog | function .proc 1 \"Create the standard out.. .proc.rolllogauto| function .proc 1 \"Roll the standard out/e.. .proc.loadf | function .proc 1 \"Load the specified file.. .proc.loaddir | function .proc 1 \"Load all the .q and .k .. .lg.o | function .lg 1 \"Log to standard out\" .. .. Combined with the commented configuration file, this should give a good overview of the functionality available. A description of the individual namespaces is below- run .api.u namespace*to list the functions. Namespace Description .proc Process API .lg Standard out/error logging API .err Error throwing API .usage Usage logging API .access Permissions API .clients Client tracking API .servers Server tracking API .async Async communication API .timer Timer API .cache Caching API .tz Timezone conversions API .checks Monitoring API .cmp Compression API .ps Publish and Subscribe API .hb Heartbeating API .loader Data Loader API .sort Data sorting and attribute setting API .sub Subscription API .gc Garbage Collection API .tplog Tickerplant Log Replay API .api API management API API Table name vartype namespace descrip params return .proc.createlog function .proc Create the standard out and standard err log files. Redirect to them [string: log directory; string: name of the log file;mixed: timestamp suffix for the file (can be null); boolean: suppress the generation of an alias link] null .proc.rolllogauto function .proc Roll the standard out/err log files [] null .proc.loadf function .proc Load the specified file [string: filename] null .proc.loaddir function .proc Load all the .q and .k files in the specified directory. If order.txt is found in the directory, use the ordering found in that file [string: name of directory] null .proc.getattributes function .proc Called by external processes to retrieve the attributes (advertised functionality) of this process [] dictionary of attributes .proc.override function .proc Override configuration varibles with command line parameters. For example, if you set -.servers.HOPENTIMEOUT 5000 on the command line and call this function, then the command line value will be used [] null .proc.overrideconfig function .proc Override configuration varibles with values in supplied parameter dictionary. Generic version of .proc.override [dictionary: command line parameters. .proc.params should be used] null .lg.o function .lg Log to standard out [symbol: id of log message; string: message] null .lg.e function .lg Log to standard err [symbol: id of log message; string: message] null .lg.l function .lg Log to either standard error or standard out, depending on the log level [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function] null .lg.err function .lg Log to standard err [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function] null .lg.ext function .lg Extra function invoked in standard logging function .lg.l. Can be used to do more with the log message, e.g. publish externally [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters] null .err.ex function .err Log to standard err, exit [symbol: id of log message; string: message; int: exit code] null .err.usage function .err Throw a usage error and exit [] null .err.param function .err Check a dictionary for a set of required parameters. Print an error and exit if not all required are supplied [dict: parameters; symbol list: the required param values] null .err.env function .err Check if a list of required environment variables are set. If not, print an error and exit [symbol list: list of required environment variables] null .usage.rolllogauto function .usage Roll the .usage txt files [] null .usage.readlog function .usage Read and return a usage log file as a table [string: name of log file] null .usage.logtodisk variable .usage whether to log to disk .usage.logtomemory variable .usage whether to log to .usage.usage .usage.ignore variable .usage whether to check the ignore list for functions to ignore .usage.ignorelist variable .usage the list of functions to ignore .usage.logroll variable .usage whether to automatically roll the log file .usage.usage table .usage log of messages through the message handlers .clients.clients table .clients table containing client handles and session values .sub.getsubscriptionhandles function .sub Connect to a list of processes of a specified type [symbol: process type to match; symbol: process name to match; dictionary:attributes of process] table of process names, types and the handle connected on .sub.subscribe function .sub Subscribe to a table or list of tables and specified instruments [symbol (list):table names; symbol (list): instruments; boolean: whether to set the schema from the server; boolean: wether to replay the logfile; dictionary: procname,proctype,handle .pm.adduser function .pm Adds a user to be permissioned as well as setting their password and the method used to hash it. [symbol: the username; symbol: method used to authenticate; symbol: method used to hash the password; string: password, hashed using the proper method] null .pm.addgroup function .pm Add a group which will have access to certain tables and variables [symbol: the name of the group; string: a description of the group] null .pm.addrole function .pm Add a role which will have access to certain functions [symbol: the name of the role; string: a description of the role] null .pm.addtogroup function .pm Add a user to a group, giving them access to all of its variables [symbol: the name of the user to add; symbol: group the user is to be added to] null .pm.assignrole function .pm Assign a user a role, giving them access to all of its functions [symbol: the name of the user to add; symbol: role the user is to be assigned to] null .pm.grantaccess function .pm Give a group access to a variable [symbol: the name of the variable the group should get access to; symbol: group that is to be given this access; symbol: the type of access that should be given, eg. read, write] null .pm.grantfunction function .pm Give a role access to a function symbol: name of the function to be added; symbol: role that is to be given this access; TO CLARIFY null .pm.createvirtualtable function .pm Create a virtual table that a group might be able to access instead of the full table [symbol: new name of the table; symbol: name of the actual table t add; TO CLARIFY] null .pm.cloneuser function .pm Add a new user that is identical to another user [symbol: name of the new user; symbol: name of the user to be cloned; string: password of the new user] null .access.addsuperuser function .access Add a super user [symbol: user] null .access.addpoweruser function .access Add a power user [symbol: user] null .access.adddefaultuser function .access Add a default user [symbol: user] null .access.readpermissions function .access Read the permissions from a directory [string: directory containing the permissions files] null .access.USERS table .access Table of users and their types .servers.opencon function .servers open a connection to a process using the default timeout. If no user:pass supplied, the default one will be added if set [symbol: the host:port[:user:pass]] int: the process handle, null if the connection failed .servers.addh function .servers open a connection to a server, store the connection details [symbol: the host:port:user:pass connection symbol] int: the server handle .servers.addw function .servers add the connection details of a process behind the handle [int: server handle] null .servers.addnthawc function .servers add the details of a connection to the table [symbol: process name; symbol: process type; hpup: host:port:user:pass connection symbol; dict: attributes of the process; int: handle to the process;boolean: whether to check the handle is valid on insert int: the handle of the process .servers.getservers function .servers get a table of servers which match the given criteria [symbol: pick the server based on the name value or the type value. Can be either `procname`proctype; symbol(list): lookup values. ` for any; dict: requirements dictionary; boolean: whether to automatically open dead connections for the specified lookup values; boolean: if only one of each of the specified lookup values is required (means dead connections aren't opened if there is one available)] table: processes details and requirements matches .servers.gethandlebytype function .servers get a server handle for the supplied type [symbol: process type; symbol: selection criteria. One of `roundrobin`any`last] int: handle of server .servers.gethpbytype function .servers get a server hpup connection symbol for the supplied type [symbol: process type; symbol: selection criteria. One of `roundrobin`any`last] symbol: h:p:u:p connection symbol of server .servers.startup function .servers initialise all the connections. Must processes should call this during initialisation [] null .servers.refreshattributes function .servers refresh the attributes registered with the discovery service. Should be called whenever they change e.g. end of day for an HDB [] null .servers.SERVERS table .servers table containing server handles and session values .timer.repeat function .timer Add a repeating timer with default next schedule [timestamp: start time; timestamp: end time; timespan: period; mixedlist: (function and argument list); string: description string] null .timer.once function .timer Add a one-off timer to fire at a specific time [timestamp: execute time; mixedlist: (function and argument list); string: description string] null .timer.remove function .timer Delete a row from the timer schedule [int: timer id to delete] null .timer.removefunc function .timer Delete a specific function from the timer schedule [mixedlist: (function and argument list)] null .timer.rep function .timer Add a repeating timer - more flexibility than .timer.repeat [timestamp: execute time; mixedlist: (function and argument list); short: scheduling algorithm for next timer; string: description string; boolean: whether to check if this new function is already present on the schedule] null .timer.one function .timer Add a one-off timer to fire at a specific time - more flexibility than .timer.once [timestamp: execute time; mixedlist: (function and argument list); string: description string; boolean: whether to check if this new function is already present on the schedule] null .timer.timer table .timer The table containing the timer information .cache.execute function .cache Check the cache for a valid result set, return the results if found, execute the function, cache it and return if not [mixed: function or string to execute;timespan: maximum allowable age of cache item if found in cache] mixed: result of function .cache.getperf function .cache Return the performance statistics of the cache [] table: cache performance .cache.maxsize variable .cache The maximum size in MB of the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation. To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want .cache.maxindividual variable .cache The maximum size in MB of an individual item in the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation. To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want .tz.dg function .tz default from GMT. Convert a timestamp from GMT to the default timezone [timestamp (list): timestamps to convert] timestamp atom or list .tz.lg function .tz local from GMT. Convert a timestamp from GMT to the specified local timezone [symbol (list): timezone ids;timestamp (list): timestamps to convert] timestamp atom or list .tz.gd function .tz GMT from default. Convert a timestamp from the default timezone to GMT [timestamp (list): timestamps to convert] timestamp atom or list .tz.gl function .tz GMT from local. Convert a timestamp from the specified local timezone to GMT [symbol (list): timezone ids; timestamp (list): timestamps to convert] timestamp atom or list .tz.ttz function .tz Convert a timestamp from a specified timezone to a specified destination timezone [symbol (list): destination timezone ids; symbol (list): source timezone ids; timestamp (list): timestamps to convert] timestamp atom or list .tz.default variable .tz Default timezone .tz.t table .tz Table of timestamp information .email.connectdefault function .email connect to the default mail server specified in configuration [] .email.senddefault function .email connect to email server if not connected. Send email using default settings [dictionary of email parameters. Required dictionary keys are to (symbol (list) of email address to send to), subject (character list), body (list of character arrays). Optional parameters are cc (symbol(list) of addresses to cc), bodyType (can be `html, default is `text), attachment (symbol (list) of files to attach), image (symbol of image to append to bottom of email. `none is no image), debug (int flag for debug level of connection library. 0i=no info, 1i=normal. 2i=verbose)] size in bytes of sent email. -1 if failure .email.test function .email send a test email [symbol(list):email address to send test email to] size in bytes of sent email. -1 if failure .hb.addprocs function .hb Add a set of process types and names to the heartbeat table to actively monitor for heartbeats. Processes will be automatically added and monitored when the heartbeats are subscribed to, but this is to allow for the case where a process might already be dead and so can't be subscribed to [symbol(list): process types; symbol(list): process names] .hb.processwarning function .hb Callback invoked if any process goes into a warning state. Default implementation is to do nothing - modify as required [table: processes currently in warning state] .hb.processerror function .hb Callback invoked if any process goes into an error state. Default implementation is to do nothing - modify as required [table: processes currently in error state] .hb.storeheartbeat function .hb Store a heartbeat update. This function should be added to you update callback when a heartbeat is received [table: the heartbeat table data to store] .hb.warningperiod function .hb Return the warning period for a particular process type. Default is to return warningtolerance * publishinterval. Can be overridden as required [symbollist: the process types to return the warning period for] timespan list of warning period .hb.errorperiod function .hb Return the error period for a particular process type. Default is to return errortolerance * publishinterval. Can be overridden as required [symbollist: the process types to return the error period for] timespan list of error period .rdb.moveandclear function .rdb Move a variable (table) from one namespace to another, deleting its contents. Useful during the end-of-day roll down for tables you do not want to save to the HDB [symbol: the namespace to move the table from; symbol:the namespace to move the variable to; symbol: the name of the variable] null .api.f function .api Find a function/variable/table/view in the current process [string:search string] table of matching elements .api.p function .api Find a public function/variable/table/view in the current process [string:search string] table of matching public elements .api.u function .api Find a non-standard q public function/variable/table/view in the current process. This excludes the .q, .Q, .h, .o namespaces [string:search string] table of matching public elements .api.s function .api Search all function definitions for a specific string [string: search string] table of matching functions and definitions .api.find function .api Generic method for finding functions/variables/tables/views. f,p and u are based on this [string: search string; boolean (list): public flags to include; boolean: whether the search is context senstive table of matching elements .api.search function .api Generic method for searching all function definitions for a specific string. s is based on this [string: search string; boolean: whether the search is context senstive table of matching functions and definitions .api.add function .api Add a function to the api description table [symbol:the name of the function; boolean:whether it should be called externally; string:the description; dict or string:the parameters for the function;string: what the function returns] null .api.fullapi function .api Return the full function api table [] api table .api.m function .api Return the ordered approximate memory usage of each variable and view in the process. Views will be re-evaluated if required [] memory usage table .api.mem function .api Return the ordered approximate memory usage of each variable and view in the process. Views are only returned if view flag is set to true. Views will be re-evaluated if required [boolean:return views] memory usage table .api.whereami function .api Get the name of a supplied function definition. Can be used in the debugger e.g. .api.whereami[.z.s] function definition symbol: the name of the current function .ps.publish function .ps Publish a table of data [symbol: name of table; table: table of data] .ps.subscribe function .ps Subscribe to a table and list of instruments [symbol(list): table name. ` for all; symbol(list): symbols to subscribe to. ` for all] mixed type list of table names and schemas .ps.initialise function .ps Initialise the pubsub routines. Any tables that exist in the top level can be published [] .async.deferred function .async Use async messaging to simulate sync communication [int(list): handles to query; query] (boolean list:success status; result list) .async.postback function .async Send an async message to a process and the results will be posted back within the postback function call [int(list): handles to query; query; postback function] boolean list: successful send status .cmp.showcomp function .cmp Show which files will be compressed and how; driven from csv file [`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress] table of files to be compressed .cmp.compressmaxage function .cmp Run compression on files using parameters specified in configuration csv file, and specifying the maximum age of files to compress [`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress] .cmp.docompression function .cmp Run compression on files using parameters specified in configuration csv file [`:/path/to/database; `:/path/to/configcsv] .loader.loadallfiles function .loader Generic loader function to read a directory of files in chunks and write them out to disk [dictionary of load parameters. Should have keys of headers (symbol list), types (character list), separator (character), tablename (symbol), dbdir (symbol). Optional params of dataprocessfunc (diadic function), datecol (name of column to extract date from: symbol), chunksize (amount of data to read at once:int), compression (compression parameters to use e.g. 16 1 0:int list), gc (boolean flag of whether to run garbage collection:boolean); directory containing files to load (symbol)] .sort.sorttab function .sort Sort and set the attributes for a table and set of partitions based on a configuration file (default is $KDBCONFIG/sort.csv) [2 item list of (tablename e.g. `trade; partitions to sort and apply attributes to e.g. `:/hdb/2000.01.01/trade`:hdb/2000.01.02/trade)] .sort.getsortcsv function .sort Read in the sort csv from the specified location [symbol: the location of the file e.g. `:config/sort.csv] .gc.run function .gc Run garbage collection, print debug info before and after .mem.objsize function .mem Returns the calculated memory size in bytes used by an object. It may take a little bit of time for objects with lots of nested structures (e.g. lots of nested columns) [q object] size of the object in bytes .tplog.check function .tplog Checks if tickerplant log can be replayed. If it can or can replay the first X messages, then returns the log handle, else it will read log as byte stream and create a good log and then return the good log handle [logfile (symbol), handle to the log file to check; lastmsgtoreplay (long), the index of the last message to be replayed from log ] handle to log file, will be either the input log handle or handle to repaired log, depends on whether the log was corrupt grafana.q Grafana is an open source analytics platform, used to display time-series data from a web application. Currently it supports a variety of data sources including Graphite, InfluxDb & Prometheus with users including the likes of Paypal, Ebay, Intel and Booking.com. However, there is no in-built support for direct analysis of data from kdb+. Thus, using the SimpleJSON data source , we have engineered an adaptor to allow visualisation of kdb+ data. Requirements Grafana v5.2.2+ (Tested on Kdb v3.5+) Getting Started Download and set up Grafana. This is well explained on the Grafana website , where you have the option to either download the software locally or let Grafana host it for you. For the purpose of this document, we host the software locally. Pull down this repository with the adaptor already installed in code/common. In your newly installed Grafana folder (eg.grafana-5.2.2/) run the command: ./bin/grafana-server web . This will start your Grafana server. If you would like to alter the port which this is run on, this can be changed in: /grafana-5.2.2/conf/custom.ini , Where custom.ini should be a copy of defaults.ini. You can now open the Grafana server in your web browser where you will be greeted with a login page to fill in appropriately. Once logged in, navigate to the configurations->plugin section where you will find the simple JSON adaptor, install this. Upon installation of the JSON you can now set-up your datasource. Host your data on a port accesible to Grafana, eg. the RDB. In the \"add new datasource\" panel, enter the details for the port in which your data is hosted, making the type SimpleJSON. Run the test button on the bottom of your page, this should succeed and you are ready to go! Using the adaptor As the adaptor is part of the TorQ framework it will automatically be loaded into TorQ sessions. From this point onwards you can proceed to use Grafana as it is intended, with the only difference coming in the form of the queries. Use cases and further examples of the queries can be seen in our blogpost: The Grafana-KDB Adaptor . For information and examples of how to execute server side functions in queries, please read our followup blogpost on the subject: Grafana kdb+ Adaptor Update . Here you can see examples of graphs, tables, heatmaps and single statistics. The best explanation of the inputs allowed in the query section can be seen pictorially here: Upon opening the query box, in the metrics tab, the user will be provided with a populated drop down of all possible options. Server functions are not included in the dropdown, but can be called by entering the letter f followed by the value of .grafana.del (see below) before their function call. Due to the limitations of the JSON messages, it is not possible for our adaptor to distinguish between panels. Consequently, every possible option is returned for each panel, the user can reduce these choices by simply entering the first letter of their panel type, g for graph, t for table and o for other (heatmap or single stat). From here, you can follow the above diagram to specify your type of query. Limitations & Assumptions This adaptor has been built to allow visualisation of real-time and historical data. It is capable of handling static and timeseries data. In addition, the drop-down options have been formed such that only one query is possible per panel. If more than one query on a specfic panel is made it will throw an error. To get around this, we added the options of including all \"syms\" in queries so the options can be filtered out in the legend. Table queries should work for any table format supplied to the adaptor. However, time series data is limited by the requriment of a time column, in our adaptor we assume this column to be called time. This assumption can be modified to fit your data in the settings (config/settings/defualt.q) file which dictates the following lines at the start of the script: // user defined column name of time column timecol:@[value;`.grafana.timecol;`time]; // user defined column name of sym column sym:@[value;`.grafana.sym;`sym]; // user defined date range to find syms from timebackdate:@[value;`.grafana.timebackdate;2D]; // user defined number of ticks to return ticks:@[value;`.grafana.ticks;1000]; // user defined query argument deliminator del:@[value;`.grafana.del;\".\"]; .grafana.timecol represents the name of the time column and thus can be reassigned if your time column has a different name, eg. date. One more common modification could be changing the variable .grafana.sym which defines the name of the the sym column, which is normally referenced in financial data. However if the data is non-financial this could be tailored to represent another identifier such as name or postcode. This column is used to populate the drop down options in the query selector. .grafana.timebackdate is a user definable variable which dictates how far back into a hdb the adaptor will look to gather options for distinct syms to populate the dropdowns. It is important to note that this should be increased if all your required syms are not in the last 2 days. Optionally a user could hard code this list or implement their own search function to limit interrogation of the database. .grafana.ticks can be defined so that only n rows from the end of the table will be queried. This can be left as large as the user likes, but is included for managing large partitioned tables. One final important variable is .grafana.del , this dictates the delimeter between options in the drop down menus. This has significant repercussions if one of your columns includes full stops, eg. email adresses. As a result we have left this as definable so that the user can alter this to a non-disruptive value for their data eg./.","title":"Utilities"},{"location":"utilities/#utilities","text":"We have provided several utility scripts, which either implement developer aids or standard operations which are useful across processes.","title":"Utilities"},{"location":"utilities/#apiq","text":"This provides a mechanism for documenting and publishing function/variable/table or view definitions within the kdb+ process. It provides a search facility both by name and definition (in the case of functions). There is also a function for returning the approximate memory usage of each variable in the process in descending order. Definitions are added using the .api.add function. A variable can be marked as public or private, and given a description, parameter list and return type. The search functions will return all the values found which match the pattern irrespective of them having a pre-defined definition. Whether a value is public or private is defined in the definitions table. If not found then by default all values are private, except those which live in the .q or top level namespace. .api.f is used to find a function, variable, table or view based on a case-insensitive pattern search. If a symbol parameter is supplied, a wildcard search of *[suppliedvalue]* is done. If a string is supplied, the value is used as is, meaning other non-wildcard regex pattern matching can be done. q).api.f`max name | vartype namespace public descrip .. --------------------| -----------------------------------------------.. maxs | function .q 1 \"\" .. mmax | function .q 1 \"\" .. .clients.MAXIDLE | variable .clients 0 \"\" .. .access.MAXSIZE | variable .access 0 \"\" .. .cache.maxsize | variable .cache 1 \"The maximum size in.. .cache.maxindividual| variable .cache 1 \"The maximum size in.. max | primitive 1 \"\" .. q).api.f\"max*\" name| vartype namespace public descrip params return ----| ------------------------------------------------ maxs| function .q 1 \"\" \"\" \"\" max | primitive 1 \"\" \"\" \"\" .api.torqnamespaces is a variable which returns a symbol list of torq namespaces. .api.exportconfig uses the table returned by .api.f` to give a table of the current values and descriptions of variables within the inputted namespace. This can be used to quickly see what configurable variables are currently set to. .api.exportallconfig is .api.exportconfig evaluated with all the available torqnamespaces and returns the same format as .api.exportconfig. .api.p is the same as .api.f, but only returns public functions. .api.u is as .api.p, but only includes user defined values i.e. it excludes q primitives and values found in the .q, .Q, .h and .o namespaces. .api.find is a more general version of .api.f which can be used to do case sensitive searches. .api.s is used to search function definitions for specific values. q).api.s\"*max*\" function definition .. ---------------------------------------------------------------------.. .Q.w \"k){`used`heap`peak`wmax`mmap`mphy`syms`symw!(.\\\".. .clients.cleanup \"{if[count w0:exec w from`.clients.clients where .. .access.validsize \"{[x;y;z] $[superuser .z.u;x;MAXSIZE>s:-22!x;x;'\\.. .servers.getservers \"{[nameortype;lookups;req;autoopen;onlyone]\\n r:$.. .cache.add \"{[function;id;status]\\n \\n res:value function;\\n.. .api.m is used to return the approximate memory usage of variables and views in the process, retrieved using -22!. Views will be re-evaluated if required. Use .api.mem[0b] if you do not want to evaluate and return views. q).api.m[] variable size sizeMB -------------------------------- .tz.t 1587359 2 .help.TXT 15409 0 .api.detail 10678 0 .proc.usage 3610 0 .proc.configusage 1029 0 .. .api.whereami[lambda] can be used to retrieve the name of a function given its definition. This can be useful in debugging. q)g:{x+y} q)f:{20 + g[x;10]} q)f[10] 40 q)f[`a] {x+y} `type + `a 10 q)).api.whereami[.z.s] `..g","title":"api.q"},{"location":"utilities/#apidetailsq","text":"This file in both the common and the handler directories is used to add to the api using the functions defined in api.q","title":"apidetails.q"},{"location":"utilities/#timerq","text":"kdb+ provides a single timer function, .z.ts which is triggered with the frequency specified by -t. We have provided an extension to allow multiple functions to be added to the timer and fired when required. The basic concept is that timer functions are registered in a table, with .z.ts periodically checking the table and running whichever functions are required. This is not a suitable mechanism where very high frequency timers are required (e.g. sub 500ms). There are two ways a function can be added to a timer- either as a repeating timer, or to fire at a specific time. When a repeating timer is specified, there are three options as to how the timer can be rescheduled. Assuming that a timer function with period P is scheduled to fire at time T0, actually fires at time T1 and finishes at time T2, then mode 0 will reschedule for T0+P; mode 1 will reschedule for T1+P; mode 2 will reschedule for T2+P. Both mode 0 and mode 1 have the potential for causing the timer to back up if the finish time T2 is after the next schedule time. See .api.p\u201c.timer.*\u201dfor more details.","title":"timer.q"},{"location":"utilities/#asyncq","text":"kdb+ processes can communicate with each using either synchronous or asynchronous calls. Synchronous calls expect a response and so the server must process the request when it is received to generate the result and return it to the waiting client. Asynchronous calls do not expect a response so allow for greater flexibility. The effect of synchronous calls can be replicated with asynchronous calls in one of two ways (further details in section gateway): deferred synchronous: the client sends an async request, then blocks on the handle waiting for the result. This allows the server more flexibility as to how and when the query is processed; asynchronous postback: the client sends an async request which is wrapped in a function to be posted back to the client when the result is ready. This allows the server flexibility as to how and when the query is processed, and allows the client to continue processing while the server is generating the result. The code for both of these can get a little tricky, largely due to the amount of error trapping required. We have provided two functions to allow these methods to be used more easily. .async.deferred takes a list of handles and a query, and will return a two item list of (success;results). q).async.deferred[3 5;({system\"sleep 1\";system\"p\"};())] 1 1 9995 9996 q).async.deferred[3 5;({x+y};1;2)] 1 1 3 3 q).async.deferred[3 5;({x+y};1;`a)] 0 0 \"error: server fail:type\" \"error: server fail:type\" q).async.deferred[3 5 87;({system\"sleep 1\";system\"p\"};())] 1 1 0 9995i 9996i \"error: comm fail: failed to send query\" .async.postback takes a list of handles, a query, and the name or lambda of the postback function to return the result to. It will immediately return a success vector, and the results will be posted back to the client when ready. q).async.postback[3 5;({system\"sleep 1\";system\"p\"};());`showresult] 11b q) q)9995i 9996i q).async.postback[3 5;({x+y};1;2);`showresult] 11b q)3 3 q).async.postback[3 5;({x+y};1;`a);`showresult] 11b q)\"error: server fail:type\" \"error: server fail:type\" q).async.postback[3 5;({x+y};1;`a);showresult] 11b q)\"error: server fail:type\" \"error: server fail:type\" q).async.postback[3 5 87;({x+y};1;2);showresult] 110b q)3 3 For more details, see .api.p\u201c.async.*\u201d.","title":"async.q"},{"location":"utilities/#cacheq","text":"cache.q provides a mechanism for storing function results in a cache and returning them from the cache if they are available and non stale. This can greatly boost performance for frequently run queries. The result set cache resides in memory and as such takes up space. It is up to the programmer to determine which functions are suitable for caching. Likely candidates are those where some or all of the following conditions hold: the function is run multiple times with the same parameters (perhaps different clients all want the same result set); the result set changes infrequently or the clients can accept slightly out-of-date values; the result set is not too large and/or is relatively expensive to produce. For example, it does not make sense to cache raw data extracts. The cache has a maximum size and a minimum size for any individual result set, both of which are defined in the configuration file. Size checks are done with -22! which will give an approximation (but underestimate) of the result set size. In the worst case the estimate could be half the size of the actual size. If a new result set is to be cached, the size is checked. Assuming it does not exceed the maximum individual size then it is placed in the cache. If the new cache size would exceed the maximum allowed space, other result sets are evicted from the cache. The current eviction policy is to remove the least recently accessed result sets until the required space is freed. The cache performance is tracked in a table. Cache adds, hits, fails, reruns and evictions are monitored. The main function to use the cache is .cache.execute[function; staletime]. If the function has been executed within the last staletime, then the result is returned from the cache. Otherwise the function is executed and placed in the cache. The function is run and the result placed in the cache: q)\\t r:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01] 2023 q)r 3 The second time round, the result set is returned immediately from the cache as we are within the staletime value: q)\\t r1:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01] 0 q)r1 3 If the time since the last execution is greater than the required stale time, the function is re-run, the cached result is updated, and the result returned: q)\\t r2:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:00] 2008 q)r2 3 The cache performance is tracked: q).cache.getperf[] time id status function ------------------------------------------------------------------ 2013.11.06D12:41:53.103508000 2 add {system\"sleep 2\"; x+y} 1 2 2013.11.06D12:42:01.647731000 2 hit {system\"sleep 2\"; x+y} 1 2 2013.11.06D12:42:53.930404000 2 rerun {system\"sleep 2\"; x+y} 1 2 See .api.p.cache.*for more details.","title":"cache.q"},{"location":"utilities/#emailq","text":"A library file is provided to allow TorQ processes to send emails using an SMTP server. This is a wrapper around the standard libcurl library. The library file is currently available for Windows (32 bit), Linux (32 and 64 bit) and OSX (32 and 64 bit). The associated q script contains two main methods for creating a connection and sending emails. The email library requires a modification to the path to find the required libs - see the top of email.q for details. The main connection method .email.connect takes a single dictionary parameter and returns 0i for success and -1i for failure. Parameter Req Type Description url Y symbol URL of mail server e.g. smtp://mail.example.com user Y symbol Username of user to login as password Y symbol Password for user usessl N boolean Connect using SSL/TLS, defaults to false from N symbol Email from field, defaults to torq@aquaq.co.uk debug N integer Debug level. 0=no output, 1=normal output, 2=verbose output. Default is 1 An example is: q).email.connect[`url`user`password`from`usessl`debug!(`$\"smtp://mail.example.com:80\";`$\"torquser@aquaq.co.uk\";`hello;`$\"torquser@aquaq.co.uk\";0b;1i)] 02 Jan 2015 11:45:19 emailConnect: url is set to smtp://mail.example.com:80 02 Jan 2015 11:45:19 emailConnect: user is set to torquser@aquaq.co.uk 02 Jan 2015 11:45:19 emailConnect: password is set 02 Jan 2015 11:45:19 emailConnect: from is set torquser@aquaq.co.uk 02 Jan 2015 11:45:19 emailConnect: trying to connect 02 Jan 2015 11:45:19 emailConnect: connected, socket is 5 0i The email sending function .email.send takes a single dictionary parameter containing the details of the email to send. A connection must be established before an email can be sent. The send function returns an integer of the email length on success, or -1 on failure. Parameter Req Type Description to Y symbol (list) addresses to send to subject Y char list email subject body Y list of char lists email body cc N symbol (list) cc list bodyType N symbol type of email body. Can be `text or `html. Default is `text debug N integer Debug level. 0=no output, 1=normal output,2=verbose output. Default is 1 An example is: q).email.send[`to`subject`body`debug!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\");1i)] 02 Jan 2015 12:39:29 sending email with subject: test email 02 Jan 2015 12:39:29 email size in bytes is 16682 02 Jan 2015 12:39:30 emailSend: email sent 16682i Note that if emails are sent infrequently the library must re-establish the connection to the mail server (this will be done automatically after the initial connection). In some circumstances it may be better to batch emails together to send, or to offload email sending to separate processes as communication with the SMTP server can take a little time. Two additional functions are available, .email.connectdefault and .email.senddefault. These are as above but will use the default configuration defined within the configuration files as the relevant parameters passed to the methods. In addition, .email.senddefault will automatically establish a connection. q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))] 2015.01.02D12:43:34.646336000|aquaq||discovery1|INF|email|sending email 2015.01.02D12:43:35.743887000|aquaq||discovery1|INF|email|connection to mail server successful 2015.01.02D12:43:37.250427000|aquaq|discovery1|INF|email|email sent 16673i q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\"))] 2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email 2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent 16675i q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\");`\"$/home/ashortt/example.txt\")] 2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email 2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent 47338i .email.test will attempt to establish a connection to the default configured email server and send a test email to the specified address. debug should be set to 2i (verbose) to extract the full information. q).email.debug:2i q).email.test `$\"test@aquaq.co.uk\" ... A further function .email.sendviaservice can be used to send an email using the default mail server on a separate specified process and can be used to allow latency sensitive processes to offload this piece of functionality. The function takes two parameters a process and a dictionary which should follow the same format as .email.send. The function uses the .async.postback Utility to send the email by calling .email.servicesend on the specified process. The postback function immediately returns a success boolean indicating that the the async request has been sent and when the function has been run on the server the results are posted back to the client function email.servicecallback which logs the email status. q).email.sendviaservice[`emailservice;`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))] 1b q)2019.01.04D12:02:57.641940000|gateway|gateway1|INF|email|Email sent successfully Additionally functions are available within the email library. See .api.p.email.*for more details.","title":"email.q"},{"location":"utilities/#emails-with-ssl-certificates-from-windows","text":"If you wish to send emails via an account which requires authentication from Windows (e.g. Hotmail, Gmail) then you have to do a few extra steps as usessl must be true and Windows does not usually find the correct certificate. The steps are: download this and save it to your PC set CURLOPT_CAINFO=c:/path/to/cabundle_file/ca-bundle.crt More information is available here and here","title":"Emails with SSL certificates from Windows"},{"location":"utilities/#timezoneq","text":"A slightly customised version of the timezone conversion functionality from code.kx. It loads a table of timezone information from $KDBCONFIG. See .api.p.tz.*for more details.","title":"timezone.q"},{"location":"utilities/#compressq","text":"compress.q applies compression to any kdb+ database, handles all partition types including date, month, year, int, and can deal with top level splayed tables. It will also decompress files as required. Once the compression/decompression is complete, summary statistics are returned, with detailed statistics for each compressed or decompressed file held in a table. The utility is driven by the configuration specified within a csv file. Default parameters can be given, and these can be used to compress all files within the database. However, the compress.q utility also provides the flexibility to compress different tables with different compression parameters, and different columns within tables using different parameters. A function is provided which will return a table showing each file in the database to be compressed, and how, before the compression is performed. Compression is performed using the -19! operator, which takes 3 parameters; the compression algorithm to use (0 - none, 1 - kdb+ IPC, 2 - gzip), the compression blocksize as a power of 2 (between 12 and 19), and the level of compression to apply (from 0 - 9, applicable only for gzip). (For further information on -19! and the parameters used, see code.kx.com.) The compressionconfig.csv file should have the following format: table,minage,column,calgo,cblocksize,clevel default,20,default,2,17,6 trades,20,default,1,17,0 quotes,20,asize,2,17,7 quotes,20,bsize,2,17,7 This file can be placed in the config folder, or a path to the file given at run time. The compression utility compresses all tables and columns present in the HDB but not specified in the driver file according the default parameters. In effect, to compress an entire HDB using the same compression parameters, a single row with name default would suffice. To specify that a particular table should be compressed in a certain different manner, it should be listed in the table. If default is given as the column for this table, then all of the columns of that table will be compressed accordingly. To specify the compression parameters for particular columns, these should be listed individually. For example, the file above will compress trades tables 20 days old or more with an algorithm of 1, and a blocksize of 17. The asize and bsize columns of any quotes tables older than 20 days old will be compressed using algorithm 2, blocksize 17 and level 7. All other files present will be compressed according to the default, using an algorithm 2, blocksize 17 and compression level 6. To leave files uncompressed, you must specify them explicitly in the table with a calgo of 0. If the file is already compressed, note that an algorithm of 0 will decompress the file. This utility should be used with caution. Before running the compression it is recommended to run the function .cmp.showcomp, which takes three parameters - the path to the database, the path to the csv file, and the maximum age of the files to be compressed: .cmp.showcomp[`:/full/path/to/HDB;.cmp.inputcsv;maxage] /- for using the csv file in the config folder .cmp.showcomp[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage] /- to specify a file This function produces a table of the files to be compressed, the parameters with which they will be compressed, and the current size of the file. Note that the current size column is calculated using hcount; on a file which is already compressed this returns the uncompressed length, i.e. this cannot be used as a signal as to whether the file is compressed already. fullpath column table partition age calgo cblocksize clevel compressage currentsize ------------------------------------------------------------------------------------- :/home/hdb/2013.11.05/depth/asize1 asize1 depth 2013.11.05 146 0 17 8 1 787960 :/home/hdb/2013.11.05/depth/asize2 asize2 depth 2013.11.05 146 0 17 8 1 787960 :/home/hdb/2013.11.05/depth/asize3 asize3 depth 2013.11.05 146 0 17 8 1 787960 :/home/hdb/2013.11.05/depth/ask1 ask1 depth 2013.11.05 146 0 17 8 1 1575904 .... To then run the compression function, use .cmp.compressmaxage with the same parameters as .cmp.showcomp (hdb path, csv path, maximum age of files): .cmp.compressmaxage[`:/full/path/to/HDB;.cmp.inputcsv;maxage] /- for using the csv file in the config folder .cmp.compressmaxage[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage] /- to specify a file To run compression on all files in the database disregarding the maximum age of the files (i.e. from minage as specified in the configuration file to infinitely old), then use: .cmp.docompression[`:/full/path/to/HDB;.cmp.inputcsv] /- for using the csv file in the config folder .cmp.docompression[`:/full/path/to/HDB;`:/full/path/to/csvfile] /- to specify a file Logs are produced for each file which is compressed or decompressed. Once the utility is complete, the statistics of the compression are also logged. This includes the memory savings in MB from compression, the additional memory usage in MB for decompression, the total compression ratio, and the total decompression ratio: |comp1|INF|compression|Memory savings from compression: 34.48MB. Total compression ratio: 2.51. |comp1|INF|compression|Additional memory used from de-compression: 0.00MB. Total de-compression ratio: . |comp1|INF|compression|Check .cmp.statstab for info on each file. A table with the compressed and decompressed length for each individual file, in descending order of compression ratio, is also produced. This can be found in .cmp.statstab: file algo compressedLength uncompressedLength compressionratio ----------------------------------------------------------------------------------- :/hdb/2014.03.05/depth/asize1 2 89057 772600 8.675343 :/hdb/2014.01.06/depth/asize1 2 114930 995532 8.662073 :/hdb/2014.03.05/depth/bsize1 2 89210 772600 8.660464 :/hdb/2014.03.12/depth/bsize1 2 84416 730928 8.658643 :/hdb/2014.01.06/depth/bsize1 2 115067 995532 8.651759 ..... A note for windows users - windows supports compression only with a compression blocksize of 16 or more.","title":"compress.q"},{"location":"utilities/#dataloaderq","text":"This script contains some utility functions to assist in loading data from delimited files (e.g. comma separated, tab delimited). It is a more generic version of the data loader example on code.kx . The supplied functions allow data to be read in configurable size chunks and written out to the database. When all the data is written, the on-disk data is re-sorted and the attributes are applied. The main function is .loader.loadalldata which takes two parameters- a dictionary of loading parameters and a directory containing the files to read. The dictionary should/can have the following fields: Parameter Req Type Description headers Y symbol list Names of the header columns in the file types Y char list Data types to read from the file separator Y char[list] Delimiting character. Enlist it if first line of file is header data tablename Y symbol Name of table to write data to dbdir Y symbol Directory to write data to symdir N symbol Directory to enumerate against partitiontype N symbol Partitioning to use. Must be one of `date`month`year`int. Default is `date partitioncol N symbol Column to use to extract partition information.Default is `time dataprocessfunc N function Diadic function to process data after it has been read in. First argument is load parameters dictionary, second argument is data which has been read in. Default is {[x;y] y} chunksize N int Data size in bytes to read in one chunk. Default is 100 MB compression N int list Compression parameters to use e.g. 17 2 6. Default is empty list for no compression gc N boolean Whether to run garbage collection at appropriate points. Default is 0b (false) filepattern N char[list] Pattern used to only load certain files e.g. \" .csv\",(\" .csv\",\"*.txt\") Example usage: .loader.loadallfiles[`headers`types`separator`tablename`dbdir!(`sym`time`price`volume;\"SP FI\";\",\";`trade;`:hdb); `:TDC/toload] .loader.loadallfiles[`headers`types`separator`tablename`dbdir`dataprocessfunc`chunksize`partitiontype`partitioncol`compression`gc`filepattern!(`sym`time`price`volume;\"SP FI\";enlist\",\";`tradesummary;`:hdb;{[p;t] select sum size, max price by date:time.date from t};`int$500*2 xexp 20;`month;`date;16 1 0;1b;(\"*.csv\";\"*.txt\")); `:TDC/toload]","title":"dataloader.q"},{"location":"utilities/#subscriptionsq","text":"The subscription utilities allow multiple subscriptions to different data sources to be managed and maintained. Automatic resubscriptions in the event of failure are possible, along as specifying whether the process will get the schema and replay the log file from the remote source (e.g. in the case of tickerplant subscriptions). .sub.getsubscriptionhandles is used to get a table of processes to subscribe to. It takes a process type and process name, where () or a null symbol can be used for all: .sub.getsubscriptionhandles[`tickerplant;();()!()] / all processes of type tickerplant .sub.getsubscriptionhandles[`;`rdb1;()!()] / all processes called 'rdb1' .sub.getsubscriptionhandles[`;`;()!()] / all processes .sub.getsubscriptionhandles[();();()!()] / nothing .sub.subscribe is used to subscribe to a process for the supplied list of tables and instruments. For example, to subscribe to instruments A, B and C for the quote table from all tickerplants: .sub.subscribe[`trthquote;`A`B;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;();()!()] The subscription method uses backtick for \u201call\u201d (which is the same as kdb+tick). To subscribe to all tables, all instruments, from all tickerplants: .sub.subscribe[`;`;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;();()!()] See .api.p\u201c.sub.*\u201d for more details.","title":"subscriptions.q"},{"location":"utilities/#pubsubq","text":"This file defines the .ps namespace, which contains various functions for subscribing and publishing to processes. These functions have traditionally been wrappers for some of the functions found in u.q , but the advent of the Segmented Tickerplant has brought about a more fully-featured pub/sub library, which is now leveraged by the .ps functions. This library is part of the 'common' code, so to load it in by default .proc.loadcommoncode must be set to true. The following three functions are primarily associated with the pub/sub library: .ps.initialise - this is a wrapper for .stpps.init and it sets up several data structures in the .stpps namespace using the tables loaded into memory. This is automatically called on process start up. .ps.subscribe - this wraps .u.sub and it receives subscription requests from other processes and populates subscription tables in memory .ps.publish - this is a wrapper for .stpps.pub and it publishes data to subscribers using the information given on subscription For example: // Subscribe to all tables and symbols handletosubscriber(`.ps.subscribe;`;`) // Subscribe to all tables and subset of symbols handletosubscriber(`.ps.subscribe;`;`AAPL`GOOG) // Subscribe to Microsoft quotes only handletosubscriber(`.ps.subscribe;`quote;enlist `MSFT) There are two new functions which have been added that wrap .u.sub with the goal of making it easier for non-kdb+ processes to subscribe using strings: .ps.subtable - accepts two strings, a table and a comma-separated list of instruments respectively .ps.subtablefiltered - accepts 3 strings representing a table, where clause and a list of columns For example: // Subscribe to Google and Apple trades handletoSTP(`.ps.subtable;\"trade\";\"GOOG\",\"AAPL\") // Subscribe to time, sym and bid price data for quotes where bid > 50 .ps.subtablefiltered[\"quote\";\"bid>50.0\";\"time,sym,bid\"] Subscribing to the STP works in a very similar fashion to the original tickerplant. From the subscriber's perspective the subscription logic is backwardly compatible: it opens a handle to the STP and calls .u.sub with a list of tables to subscribe to as its first argument and either a null symbol or a list of symbols as a sym filter. The STP also supports a keyed table of conditions (in q parse format) and a list of columns that should be published. Whilst complex bespoke subscription is possible in the STP it is generally not recommended. Complex subscription filtering should be off loaded to a chained STP. // Subscribe to everything handletoSTP(`.u.sub;`;`) // Subscribe GOOG and AAPL symbols in the trade table handletoSTP(`.u.sub;`trade;`GOOG`AAPL) // Subscribe to all tables but with custom conditions handletoSTP(`.u.sub;`;conditions) ... q) show conditions tabname| filts columns -------| ---------------------------------- trade | \"\" \"time,sym,price\" quote | \"bid>100,bid<200\" \"\" Here subscribing subject to the conditions table results in the subscriber only receiving quotes where the bid is between 100 and 200. Also only the time, sym and price columns of the trade table are published to the subscriber. Note that it is also possible to use the conditions table to subscribe to just one of the trade or quote tables. A conditions table may also be used to perform calculations on columns and define new ones as well: q) show conditions tabname| filts columns -------| ---------------------------------------------------- quote | \"bid>100,bid<200\" \"time,sym,bid,ask,mid:0.5*bid+ask\" q)quote time sym bid ask mid ------------------------------------------------------ 2020.12.09D15:29:23.183738000 INTC 58.6 59.4 59 2020.12.09D15:29:23.183738000 DOW 21.39 22.53 21.96 ... For more information on subscriptions, see the documentation on the segmented tickerplant process.","title":"pubsub.q"},{"location":"utilities/#data-striping","text":"Data striping is the technique of logically segmenting data between processes so that they are stored on different processes. The advantages of striping include performance and throughput improvements. Segmenting data on separate processes reduces latency when processing data. Total data throughput is increased as multiple processes can be accessed concurrently. In addition, the memory footprint per process is lower and processes can be split across multiple hosts. It is useful for load balancing across processes. The reduction in data access in each process cumulatively multiplies the data throughput by the number of processes. It also allows the process to complete its task faster and without interruption, thereby reducing latency. Example of data striping in TorQ A simple but effective way of data striping is to do it divide the data randomly across all processes. This will ensure an even distribution of data. However, querying (locating and retrieving) data can become complicated. A common method for data striping between processes is to use an instrument (sym) filter. For example, to stripe data across 2 RDB processes, data with symbols starting with A-M and N-L will be striped to RDB1 and RDB2 respectively. However, a major problem with this method is the uneven distribution of data (a lot of symbols tend to start with A for example). Data hash striping A way to get around this problem is to stripe the data using a hash value which allows for better distribution. The hash function will store the mappings for the symbols that it has already computed and for subsequent requests for those symbols, it looks them up. It is loaded into the segmented tickerplant to use as subscription requests. The hash function has to be highly performant as it will potentially be invoked ~100-1000times/s in a core (potential bottleneck) component. For this purpose, a fast, simple and non-cryptographic hash function is created to segment the data since the performance of the segmented tickerplant is of paramount importance. The hash map is created by summing the ASCII codes of each input string then dividing by the number of segements. A hash map based on the sym column is created like so: q)show sym:`$-100000?(thrl cross .Q.A),thrl:(.Q.A cross .Q.A cross .Q.A) `AWNM`MFLD`XWTL`OVIH`HAMI`OFDX`MWNC`BLJT`MVOR`TVAG`RGUU`MZCY`HLTW`RJVZ`MAYS`O.. q)sym!(sum each string sym)mod 4 AWNM| 3 MFLD| 3 XWTL| 3 OVIH| 2 HAMI| 3 OFDX| 1 MWNC| 1 BLJT| 0 MVOR| 0 .. Using a hash function ensures better distribution of symbols across databases: q)count each group sym!(sum each string sym)mod 4 3| 25077 2| 24920 1| 24877 0| 25126 An advantage of using data hash striping is such that a specific symbol will always be striped to the exact location based on the number of segments.","title":"Data striping"},{"location":"utilities/#setting-up-data-striping-in-torq","text":"1) Example setup for data striping across ALL (4) RDB instances $KDBCONFIG/process.csv The process file should contain the RDB instances by specifying a different port and procname column. Set the load column to ${KDBCODE}/processes/rdb.q . NOTE The procname convention should always start with rdb1 Increment by 1 for each RDB instance In the following format: rdb{i} There is no need to create separate rdb{i}.q files $KDBCONFIG/process.csv should look something like this: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+1,discovery,discovery1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/discovery.q,1,,q localhost,{KDBBASEPORT}+2,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -tplogdir ${KDBTPLOG},q localhost,{KDBBASEPORT}+3,rdb,rdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+4,rdb,rdb2,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+5,rdb,rdb3,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+6,rdb,rdb4,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q $KDBCONFIG/rdbsub/rdbsub{i}.csv The rdbsub{i}.csv should be modified like this: tabname,filters,columns trade,.ds.stripe[sym;{i-1}], quote,.ds.stripe[sym;{i-1}], $KDBAPPCONFIG/settings/rdb.q Set .rdb.subfiltered: 1b (default is 0b) $KDBAPPCONFIG/settings/default.q Add .ds.numseg: {4}i 2) Example setup for data striping across SOME RDB instances 2 RDB instances unfiltered 2 RDB instances striped $KDBCONFIG/process.csv Add in -.rdb.subfiltered 1 (to enable striping) in the extras column for the striped RDB instances. NOTE It is -.rdb.subfiltered 1 and not -.rdb.subfiltered 1b The RDB instances must be grouped according to those being striped first i.e. rdb1 , rdb2 are striped and rdb3 , rdb4 are unfiltered $KDBCONFIG/process.csv should look something like this: host,port,proctype,procname,U,localtime,g,T,w,load,startwithall,extras,qcmd localhost,{KDBBASEPORT}+1,discovery,discovery1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/discovery.q,1,,q localhost,{KDBBASEPORT}+2,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -tplogdir ${KDBTPLOG},q localhost,{KDBBASEPORT}+3,rdb,rdb1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,-.rdb.subfiltered 1,q localhost,{KDBBASEPORT}+4,rdb,rdb2,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,-.rdb.subfiltered 1,q localhost,{KDBBASEPORT}+5,rdb,rdb3,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q localhost,{KDBBASEPORT}+6,rdb,rdb4,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,1,180,,${KDBCODE}/processes/rdb.q,1,,q $KDBAPPCONFIG/settings/rdb.q Ensure .rdb.subfiltered: 0b $KDBAPPCONFIG/settings/default.q Add .ds.numseg: {2}i 3) Data striping for general subscribers {subscriber}.q Add the conditions required (using the data hash striping function). ... // Subscribe to all tables but with custom conditions conditions:1!flip`tabname`filters`columns!(`trade`quote;(\".ds.stripe[sym;0]\";\".ds.stripe[sym;1]\");(\"\";\"\")) handletoSTP(`.u.sub;`;conditions) ... q)conditions tabname| filters columns -------| ---------------------------------- trade | \".ds.stripe[sym;0]\" \"\" quote | \".ds.stripe[sym;1]\" \"\" q)handletoSTP\".stpps.subrequestfiltered\" tbl handle filts columns --------------------------------------------------------- logmsg 21 () packets 21 () quote 21 (`.ds.stripe;`sym;1) quote_iex 21 () trade 21 (`.ds.stripe;`sym;0) trade_iex 21 ()","title":"Setting up data striping in TorQ"},{"location":"utilities/#kafkaq","text":"kafka.q provides q language bindings for Apache Kafka, a 'distributed streaming platform', a real time messaging system with persistent storage in message logs. The core functionality of Kafka \u2013 pub/sub messaging with persisted logs, will be familiar to most readers as the functionality offered by the kdb+ tick tickerplant. The tickerplant log allows the real time database and other consumers to replay a day\u2019s events to recover state. An application architecture built around Kafka could dispense with a tickerplant component, and have RDBs and other real time clients query Kafka on startup for offsets, and play back the data they need. While not suitable for very low latency access to streaming data, it would carry some advantages for very high throughput applications, particularly those in the cloud: Kafka\u2019s distributed nature should allow it to scale more transparently than splitting tickerplants by instrument universe or message type Replaying from offsets is the same interface as live pub/sub and doesn\u2019t require filesystem access to the tickerplant log, so RDB\u2019s and other consumer could be on a different server By default, the Kafka bindings will be loaded into all TorQ processes running on l64 systems (the only platform currently supported). An example of usage is shown here (this assumes a local running instance of kafka - instructions for this are available on the kafkaq github repo): q).kafka.initconsumer[`localhost:9092;()] q).kafka.initproducer[`localhost:9092;()] q)kupd / print default definition for incoming data - ignore key, print message as ascii {[k;x] -1 `char$x;} q).kafka.subscribe[`test;0] / subscribe to topic test, partition 0 q)pub:{.kafka.publish[`test;0;`;`byte$x]} / define pub to publish text input to topic test on partition 0 with no key defined q)pub\"hello world\" q)hello world Limitations of the current implementation: Only l64 supported Single consumer thread subscribed to one topic at a time","title":"kafka.q"},{"location":"utilities/#tplogutilsq","text":"tplogutils.q contains functions for recovering tickerplant log files. Under certain circumstances the tickerplant log file can become corrupt by having an invalid sequence of bytes written to it. A log file can be recovered using a simple recovery method. However, this will only recover messages up to the first invalid message. The recovery functions defined in tplogutils.q allow all valid messages to be recovered from the tickerplant log file.","title":"tplogutils.q"},{"location":"utilities/#monitoringchecksq","text":"monitoringchecks.q implements a set of standard, basic monitoring checks. They include checks to ensure: table sizes are increasing during live capture the HDB data saves down correctly the allocated memory of a process does not increase past a certain size the size of the symbol list in memory doesn\u2019t grow to big the process does not have too much on its pending subscriber queue These checks are intended to be run by the reporter process on a schedule, and any alerts emailed to an appropriate recipient list.","title":"monitoringchecks.q"},{"location":"utilities/#heartbeatq","text":"heartbeat.q implements heartbeating, and relies on both timer.q and pubsub.q. A table called heartbeat will be published periodically, allowing downstream processes to detect the availability of upstream components. The heartbeat table contains a heartbeat time and counter. The heartbeat script contains functions to handle and process heartbeats and manage upstream process failures. See .api.p.hb.*for details.","title":"heartbeat.q"},{"location":"utilities/#rmvrq","text":"This file contains a function which can be used to convert environment variable paths into a full path from the root directory.","title":"rmvr.q"},{"location":"utilities/#osq","text":"A file with various q functions to perform system operations. This will detect your operating system and will perform the correct commands depending on what you are using. This is a modification of a script developed by Simon Garland.","title":"os.q"},{"location":"utilities/#dbwriteutilsq","text":"This contains a set of utility functions for writing data to historic databases.","title":"dbwriteutils.q"},{"location":"utilities/#sorting-and-attributes","text":"The sort utilities allow the sort order and attributes of tables to be globally defined. This helps to manage the code base when the data can potentially be written from multiple locations (e.g. written from the RDB, loaded from flat file, replayed from the tickerplant log). The configuration is defined in a csv which defaults to $KDBCONFG/sort.csv. The default setup is that every table is sorted by sym and time, with a p attribute on sym (this is the standard kdb+ tick configuration). aquaq$ tail config/sort.csv tabname,att,column,sort default,p,sym,1 default,,time,1 As an example, assume we have an optiontrade table which we want to be different from the standard set up. We would like the table to be sorted by optionticker and then time, with a p attribute on optionticker. We also have a column called underlyingticker which we can put an attribute on as it is derived from optionticker (so there is an element of de-normalisation present in the table). We also have an exchange field which we would like to put a g attribute on. All other tables we want to be sorted and parted in the standard way. The configuration file would look like this (sort order is derived from the order within the file combined with the sort flag being set to true): aquaq$ tail config/sort.csv tabname,att,column,sort default,p,sym,1 default,,time,1 optiontrade,p,optionticker,1 optiontrade,,exchtime,1 optiontrade,p,underlyingticker,0 optiontrade,g,exchange,0 To invoke the sort utilities, supply a list of (tablename; partitions) e.g. q).sort.sorttab(`trthtrade;`:hdb/2014.11.20/trthtrade`:hdb/2014.11.20/trthtrade) 2014.12.03D09:56:19.214006000|aquaq|test|INF|sort|sorting the trthtrade table 2014.12.03D09:56:19.214045000|aquaq|test|INF|sorttab|No sort parameters have been specified for : trthtrade. Using default parameters 2014.12.03D09:56:19.214057000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.19/trthtrade/ by these columns : sym, time 2014.12.03D09:56:19.219716000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.19/trthtrade/ 2014.12.03D09:56:19.220846000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.20/trthtrade/ by these columns : sym, time 2014.12.03D09:56:19.226008000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.20/trthtrade/ 2014.12.03D09:56:19.226636000|aquaq|test|INF|sort|finished sorting the trthtrade table A different sort configuration file can be loaded with .sort.getsortcsv[`:file]","title":"Sorting and Attributes"},{"location":"utilities/#garbage-collection","text":"The garbage collection utility prints some debug information before and after the garbage collection. q).gc.run[] 2014.12.03D10:22:51.688435000|aquaq|test|INF|garbagecollect|Starting garbage collect. mem stats: used=2 MB; heap=1984 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB 2014.12.03D10:22:53.920656000|aquaq|test|INF|garbagecollect|Garbage collection returned 1472MB. mem stats: used=2 MB; heap=512 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB","title":"Garbage Collection"},{"location":"utilities/#table-manipulation","text":"The table manipulation utilities allow table manipulation routines to be defined in a single place. This is useful when data can be written from mutliple different processes e.g. RDB, WDB, or tickerplant log replay. Instead of having to create a separate definition of customised manipulation in each process, it can be done in a single location and invokved in each process.","title":"Table Manipulation"},{"location":"utilities/#helpq","text":"The standard help.q from code.kx provides help utilities in the console. This should be kept up to date with [ code.kx ]. q)help` adverb | adverbs/operators attributes| data attributes cmdline | command line parameters data | data types define | assign, define, control and debug dotz | .z locale contents errors | error messages save | save/load tables syscmd | system commands temporal | temporal - date & time casts verbs | verbs/functions","title":"help.q"},{"location":"utilities/#htmlq","text":"An HTML utility has been added to accompany the HTML5 front end for the Monitoring process. It includes functions to format dates, tables to csv to configure the HTML file to work on the correct process. It is accessible from the .html namespace.","title":"html.q"},{"location":"utilities/#eodtimeq","text":"This script provides functionality for managing timezones. TorQ can be configured to timestamp data in a specific timezone, while also being configured to perform the end of day rollover in another timezone, at a configurable time. These options are handled by three settings: Setting Req Type Description .eodtime.rolltimeoffset Y timespan Offset from default midnight roll time .eodtime.rolltimezone Y symbol Time zone in which to rollover .eodtime.datatimezone Y symbol Time zone in which to timestamp data in TP The default configuration sets both timezones to GMT and has the rollover performed at midnight. A table containing the valid timezones is loaded into TorQ processes as .tz.t An example configuration where data is stamped in GMT, but the rollover occurs at 5PM New York time would be: .eodtime.rolltimeoffset:-0D07:00:00.000000000; // 5 PM i.e. 7 hours before midnight .eodtime.rolltimezone:`$\"America/New_YorK\"; // roll in NYC time .eodtime.datatimezone:`$\"GMT\"; // timestamp in GMT Note that the rolltimeoffset can be negative - this will cause the rollover to happen \"yesterday\", meaning that at the rolltime, the trading date will become the day after the calendar date. Where this is positive, the rollover occurs \"today\" and so the trading date will become the current calendar date.","title":"eodtime.q"},{"location":"utilities/#subscribercutoffq","text":"This script is used to provide functionality for cutting off any slow subscribers on any TorQ processes. The script will periodically check (time between checks set in .subcut.checkfreq. Default is 1 minute) the byte size of the queue for all the handles on the process to see if they have exceeded a set cut-off point (set in the variable .subcut.maxsize) and will only cut-off the handle if it exceeds this limit a set number of times in a row (default is 3 and set in the .subcut.breachlimit variable). This gives clients a chance to tidy up their behavior and will avoid cutting off clients if they happened to have a spike just before the check was performed. The .subcut.state variable is used to keep track of the handles and the number of times they have exceeded the size limit in a row. To enable this functionality the .subcut.enabled flag must be set to true and the timer.q script must be loaded on the desired processes. By default the chained tickerplant is the only processes with the functionality enabled.","title":"subscribercutoff.q"},{"location":"utilities/#datareplayq","text":"The datareplay utility provides functionality for generating tickerplant function calls from historcial data which can be executed by subscriber functions. This can be used to test a known data-set against a subscriber for testing or debugging purposes. It can load this data from the current TorQ session, or from a remote hdb if given its connection handle. It can also chunk the data by time increments (as if the tickerplant was in batch mode), and can also generate calls to a custom timer function for the same time increments (defaults to .z.ts). The functions provided by this utility are made available in the .datareplay namespace. The utility is mainly used via the tabesToDataStreamFunction, which accepts a dictionary parameter with the following fields: Key Example Value Description Required Default tabs `trade`quote or `trade List of tables to include Yes N/A sts 2014.04.04D07:00:00.000 Start timestamp for data Yes N/A ets 2014.04.04D16:30:00.000 End of timestamp for data Yes N/A syms `AAPL`IBM List of symbols to include No All syms where ,(=;`src;,`L) Custom where clause in functional form No none timer 1b Generate timer function flag No 0b h 5i Handle to hdb process No 0i (self) interval 0D00:00:01.00 Time interval used to chunk data, bucketed by timestamp if no time interval set No None tc `data_time Name of time column to cut on No `time timerfunc .z.ts Timer function to use if `timer parameter is set No .z.ts When the timer flag is set, the utility will interleave timer function calls in the message column at intervals based on the interval parameter, or every 10 seconds if interval is not set. This is useful if testing requires a call to a function at a set time, to generate a VWAP every 10 minutes for example. The function the timer messages call is based on the timerfunc parameter, or .z.ts if this parameter is not set. If the interval is set the messages will be aggregated into chunks based on the interval value, if no interval is specified, the data will be bucketed by timestamp (one message chunk per distinct timestamp per table). If no connection handle is specified (h parameter), the utility will retrieve the data from the process the utility is running on, using handle 0. The where parameter allows for the use of a custom where clause when extracting data, which can be useful when the dataset is large and only certain data is required, for example if only data where src=`L is required. The where clause(s) are required to be in functional form, for example enlist (=;`src;,`L) or ((=;`src;enlist `L);(>;`size;100)) (note, that if only one custom where clause is included it is required to be enlisted). It is possible to get the functional form of a where clause by running parse on a mock select string like below: q)parse \"select from t where src=`L,size>100\" ? `t ,((=;`src;,`L);(>;`size;100)) 0b () The where clause is then the 3rd item returned in the parse tree.","title":"datareplay.q"},{"location":"utilities/#examples","text":"Extract all data between sts and ets from the trades table in the current process. q)input tabs| `trades sts | 2014.04.21D07:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:00:23.478000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. 2014.04.21D08:00:49.511000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. 2014.04.21D08:01:45.623000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. 2014.04.21D08:02:41.346000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. .. q)first .datareplay.tablesToDataStream input time| 2014.04.21D08:00:23.478000000 msg | (`upd;`trades;`sym`time`src`price`size!(`YHOO;2014.04.21D08:00:23.47800.. Extract all data between sts and ets from the trades table from a remote hdb handle=3i. q)input tabs| `trades sts | 2014.04.21D07:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 h | 3i q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:00:07.769000000 `upd `trades `sym`time`src`price`size!(`IBM;201.. 2014.04.21D08:00:13.250000000 `upd `trades `sym`time`src`price`size!(`NOK;201.. 2014.04.21D08:00:19.070000000 `upd `trades `sym`time`src`price`size!(`MSFT;20.. 2014.04.21D08:00:23.678000000 `upd `trades `sym`time`src`price`size!(`YHOO;20.. .. q)first .datareplay.tablesToDataStream input time| 2014.04.21D08:00:07.769000000 msg | (`upd;`trades;`sym`time`src`price`size!(`IBM;2014.04.21D08:00:07.769000.. Same as above but including quote table and with interval of 10 minutes: q)input tabs | `quotes`trades sts | 2014.04.21D07:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 h | 3i interval| 0D00:10:00.000000000 q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:09:47.600000000 `upd `trades +`sym`time`src`price`size!(`YHOO`A.. 2014.04.21D08:09:55.210000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize.. 2014.04.21D08:19:39.467000000 `upd `trades +`sym`time`src`price`size!(`CSCO`N.. 2014.04.21D08:19:49.068000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize.. .. q)first .datareplay.tablesToDataStream input time| 2014.04.21D08:09:47.600000000 msg | (`upd;`trades;+`sym`time`src`price`size!(`YHOO`AAPL`MSFT`NOK`DELL`YHOO`.. \u200b All messages from trades where src=`L bucketed in 10 minute intervals interleaved with calls to the function `vwap . q)input tabs | `trades h | 3i sts | 2014.04.21D08:00:00.000000000 ets | 2014.05.02D17:00:00.000000000 where | ,(=;`src;,`L) timer | 1b timerfunc| `vwap interval | 0D00:10:00.000000000 q).datareplay.tablesToDataStream input time msg .. -----------------------------------------------------------------------------.. 2014.04.21D08:00:00.000000000 (`vwap;2014.04.21D08:00:00.000000000) .. 2014.04.21D08:09:46.258000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`.. 2014.04.21D08:10:00.000000000 (`vwap;2014.04.21D08:10:00.000000000) .. 2014.04.21D08:18:17.188000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`.. ..","title":"Examples:"},{"location":"utilities/#modified-uq","text":"Starting in kdb+ v3.4, the new broadcast feature has some performance benefits. It works by serialising a message once before sending it asynchronously to a list of subscribers whereas the previous method would serialise it separately for each subscriber. To take advantage of this, we\u2019ve modified u.q. This can be turned off by setting .u.broadcast to false. It is enabled by default, but will only override default publishing if the kdb+ version being used is 3.4 or after.","title":"Modified u.q"},{"location":"utilities/#bglaunchutilsq","text":"The background launch utilities allow other processes to be programmatically launched, or terminated, from inside a TorQ process. The two main functions here are .sys.bglaunch and .sys.bgkill. .sys.bglaunch is the q function which takes a dictionary of input parameters which are then passed to a bash script. This bash script functions like torq.sh in how it starts processes. It is important to note that the background launch utilities are only supported on Linux as a result. q)input:`procname`proctype`localtime`p!(\"hdb1\";\"hdb\";\"0\";\"1234\"); The input parameter dictionary, as shown above, should contain symbols as keys and strings as values. Any standard or custom TorQ process can be launched using .sys.bglaunch, and as such the function can accept any desired command line parameters in the input dictionary. The minimum required are `procname and `proctype . In the case that only these two are used the other arguments will be given default values. Parameter Default Value U The password file used for the parent launching process, if none exists ${KDBAPPCONFIG}/passwords/accesslist.txt is used localtime The .proc.localtime value of the parent launching process p 0W - a random port will be chosen qcmd q The .sys.bgkill function is passed a single argument: the `procname of the process to be terminated, as a string. If the default value of \"0W\" is used for the port, a random port will be chosen on which to launch the process. In this case the process will neeed to register itself with discovery in order for other processes to be able to connect to it. This is a standard behaviour for TorQ processes on startup (for more information see Connection Management ).","title":"bglaunchutils.q"},{"location":"utilities/#full-api","text":"The full public api can be found by running q).api.u` name | vartype namespace public descrip .. -----------------| --------------------------------------------------.. .proc.createlog | function .proc 1 \"Create the standard out.. .proc.rolllogauto| function .proc 1 \"Roll the standard out/e.. .proc.loadf | function .proc 1 \"Load the specified file.. .proc.loaddir | function .proc 1 \"Load all the .q and .k .. .lg.o | function .lg 1 \"Log to standard out\" .. .. Combined with the commented configuration file, this should give a good overview of the functionality available. A description of the individual namespaces is below- run .api.u namespace*to list the functions. Namespace Description .proc Process API .lg Standard out/error logging API .err Error throwing API .usage Usage logging API .access Permissions API .clients Client tracking API .servers Server tracking API .async Async communication API .timer Timer API .cache Caching API .tz Timezone conversions API .checks Monitoring API .cmp Compression API .ps Publish and Subscribe API .hb Heartbeating API .loader Data Loader API .sort Data sorting and attribute setting API .sub Subscription API .gc Garbage Collection API .tplog Tickerplant Log Replay API .api API management API","title":"Full API"},{"location":"utilities/#api-table","text":"name vartype namespace descrip params return .proc.createlog function .proc Create the standard out and standard err log files. Redirect to them [string: log directory; string: name of the log file;mixed: timestamp suffix for the file (can be null); boolean: suppress the generation of an alias link] null .proc.rolllogauto function .proc Roll the standard out/err log files [] null .proc.loadf function .proc Load the specified file [string: filename] null .proc.loaddir function .proc Load all the .q and .k files in the specified directory. If order.txt is found in the directory, use the ordering found in that file [string: name of directory] null .proc.getattributes function .proc Called by external processes to retrieve the attributes (advertised functionality) of this process [] dictionary of attributes .proc.override function .proc Override configuration varibles with command line parameters. For example, if you set -.servers.HOPENTIMEOUT 5000 on the command line and call this function, then the command line value will be used [] null .proc.overrideconfig function .proc Override configuration varibles with values in supplied parameter dictionary. Generic version of .proc.override [dictionary: command line parameters. .proc.params should be used] null .lg.o function .lg Log to standard out [symbol: id of log message; string: message] null .lg.e function .lg Log to standard err [symbol: id of log message; string: message] null .lg.l function .lg Log to either standard error or standard out, depending on the log level [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function] null .lg.err function .lg Log to standard err [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function] null .lg.ext function .lg Extra function invoked in standard logging function .lg.l. Can be used to do more with the log message, e.g. publish externally [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters] null .err.ex function .err Log to standard err, exit [symbol: id of log message; string: message; int: exit code] null .err.usage function .err Throw a usage error and exit [] null .err.param function .err Check a dictionary for a set of required parameters. Print an error and exit if not all required are supplied [dict: parameters; symbol list: the required param values] null .err.env function .err Check if a list of required environment variables are set. If not, print an error and exit [symbol list: list of required environment variables] null .usage.rolllogauto function .usage Roll the .usage txt files [] null .usage.readlog function .usage Read and return a usage log file as a table [string: name of log file] null .usage.logtodisk variable .usage whether to log to disk .usage.logtomemory variable .usage whether to log to .usage.usage .usage.ignore variable .usage whether to check the ignore list for functions to ignore .usage.ignorelist variable .usage the list of functions to ignore .usage.logroll variable .usage whether to automatically roll the log file .usage.usage table .usage log of messages through the message handlers .clients.clients table .clients table containing client handles and session values .sub.getsubscriptionhandles function .sub Connect to a list of processes of a specified type [symbol: process type to match; symbol: process name to match; dictionary:attributes of process] table of process names, types and the handle connected on .sub.subscribe function .sub Subscribe to a table or list of tables and specified instruments [symbol (list):table names; symbol (list): instruments; boolean: whether to set the schema from the server; boolean: wether to replay the logfile; dictionary: procname,proctype,handle .pm.adduser function .pm Adds a user to be permissioned as well as setting their password and the method used to hash it. [symbol: the username; symbol: method used to authenticate; symbol: method used to hash the password; string: password, hashed using the proper method] null .pm.addgroup function .pm Add a group which will have access to certain tables and variables [symbol: the name of the group; string: a description of the group] null .pm.addrole function .pm Add a role which will have access to certain functions [symbol: the name of the role; string: a description of the role] null .pm.addtogroup function .pm Add a user to a group, giving them access to all of its variables [symbol: the name of the user to add; symbol: group the user is to be added to] null .pm.assignrole function .pm Assign a user a role, giving them access to all of its functions [symbol: the name of the user to add; symbol: role the user is to be assigned to] null .pm.grantaccess function .pm Give a group access to a variable [symbol: the name of the variable the group should get access to; symbol: group that is to be given this access; symbol: the type of access that should be given, eg. read, write] null .pm.grantfunction function .pm Give a role access to a function symbol: name of the function to be added; symbol: role that is to be given this access; TO CLARIFY null .pm.createvirtualtable function .pm Create a virtual table that a group might be able to access instead of the full table [symbol: new name of the table; symbol: name of the actual table t add; TO CLARIFY] null .pm.cloneuser function .pm Add a new user that is identical to another user [symbol: name of the new user; symbol: name of the user to be cloned; string: password of the new user] null .access.addsuperuser function .access Add a super user [symbol: user] null .access.addpoweruser function .access Add a power user [symbol: user] null .access.adddefaultuser function .access Add a default user [symbol: user] null .access.readpermissions function .access Read the permissions from a directory [string: directory containing the permissions files] null .access.USERS table .access Table of users and their types .servers.opencon function .servers open a connection to a process using the default timeout. If no user:pass supplied, the default one will be added if set [symbol: the host:port[:user:pass]] int: the process handle, null if the connection failed .servers.addh function .servers open a connection to a server, store the connection details [symbol: the host:port:user:pass connection symbol] int: the server handle .servers.addw function .servers add the connection details of a process behind the handle [int: server handle] null .servers.addnthawc function .servers add the details of a connection to the table [symbol: process name; symbol: process type; hpup: host:port:user:pass connection symbol; dict: attributes of the process; int: handle to the process;boolean: whether to check the handle is valid on insert int: the handle of the process .servers.getservers function .servers get a table of servers which match the given criteria [symbol: pick the server based on the name value or the type value. Can be either `procname`proctype; symbol(list): lookup values. ` for any; dict: requirements dictionary; boolean: whether to automatically open dead connections for the specified lookup values; boolean: if only one of each of the specified lookup values is required (means dead connections aren't opened if there is one available)] table: processes details and requirements matches .servers.gethandlebytype function .servers get a server handle for the supplied type [symbol: process type; symbol: selection criteria. One of `roundrobin`any`last] int: handle of server .servers.gethpbytype function .servers get a server hpup connection symbol for the supplied type [symbol: process type; symbol: selection criteria. One of `roundrobin`any`last] symbol: h:p:u:p connection symbol of server .servers.startup function .servers initialise all the connections. Must processes should call this during initialisation [] null .servers.refreshattributes function .servers refresh the attributes registered with the discovery service. Should be called whenever they change e.g. end of day for an HDB [] null .servers.SERVERS table .servers table containing server handles and session values .timer.repeat function .timer Add a repeating timer with default next schedule [timestamp: start time; timestamp: end time; timespan: period; mixedlist: (function and argument list); string: description string] null .timer.once function .timer Add a one-off timer to fire at a specific time [timestamp: execute time; mixedlist: (function and argument list); string: description string] null .timer.remove function .timer Delete a row from the timer schedule [int: timer id to delete] null .timer.removefunc function .timer Delete a specific function from the timer schedule [mixedlist: (function and argument list)] null .timer.rep function .timer Add a repeating timer - more flexibility than .timer.repeat [timestamp: execute time; mixedlist: (function and argument list); short: scheduling algorithm for next timer; string: description string; boolean: whether to check if this new function is already present on the schedule] null .timer.one function .timer Add a one-off timer to fire at a specific time - more flexibility than .timer.once [timestamp: execute time; mixedlist: (function and argument list); string: description string; boolean: whether to check if this new function is already present on the schedule] null .timer.timer table .timer The table containing the timer information .cache.execute function .cache Check the cache for a valid result set, return the results if found, execute the function, cache it and return if not [mixed: function or string to execute;timespan: maximum allowable age of cache item if found in cache] mixed: result of function .cache.getperf function .cache Return the performance statistics of the cache [] table: cache performance .cache.maxsize variable .cache The maximum size in MB of the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation. To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want .cache.maxindividual variable .cache The maximum size in MB of an individual item in the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation. To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want .tz.dg function .tz default from GMT. Convert a timestamp from GMT to the default timezone [timestamp (list): timestamps to convert] timestamp atom or list .tz.lg function .tz local from GMT. Convert a timestamp from GMT to the specified local timezone [symbol (list): timezone ids;timestamp (list): timestamps to convert] timestamp atom or list .tz.gd function .tz GMT from default. Convert a timestamp from the default timezone to GMT [timestamp (list): timestamps to convert] timestamp atom or list .tz.gl function .tz GMT from local. Convert a timestamp from the specified local timezone to GMT [symbol (list): timezone ids; timestamp (list): timestamps to convert] timestamp atom or list .tz.ttz function .tz Convert a timestamp from a specified timezone to a specified destination timezone [symbol (list): destination timezone ids; symbol (list): source timezone ids; timestamp (list): timestamps to convert] timestamp atom or list .tz.default variable .tz Default timezone .tz.t table .tz Table of timestamp information .email.connectdefault function .email connect to the default mail server specified in configuration [] .email.senddefault function .email connect to email server if not connected. Send email using default settings [dictionary of email parameters. Required dictionary keys are to (symbol (list) of email address to send to), subject (character list), body (list of character arrays). Optional parameters are cc (symbol(list) of addresses to cc), bodyType (can be `html, default is `text), attachment (symbol (list) of files to attach), image (symbol of image to append to bottom of email. `none is no image), debug (int flag for debug level of connection library. 0i=no info, 1i=normal. 2i=verbose)] size in bytes of sent email. -1 if failure .email.test function .email send a test email [symbol(list):email address to send test email to] size in bytes of sent email. -1 if failure .hb.addprocs function .hb Add a set of process types and names to the heartbeat table to actively monitor for heartbeats. Processes will be automatically added and monitored when the heartbeats are subscribed to, but this is to allow for the case where a process might already be dead and so can't be subscribed to [symbol(list): process types; symbol(list): process names] .hb.processwarning function .hb Callback invoked if any process goes into a warning state. Default implementation is to do nothing - modify as required [table: processes currently in warning state] .hb.processerror function .hb Callback invoked if any process goes into an error state. Default implementation is to do nothing - modify as required [table: processes currently in error state] .hb.storeheartbeat function .hb Store a heartbeat update. This function should be added to you update callback when a heartbeat is received [table: the heartbeat table data to store] .hb.warningperiod function .hb Return the warning period for a particular process type. Default is to return warningtolerance * publishinterval. Can be overridden as required [symbollist: the process types to return the warning period for] timespan list of warning period .hb.errorperiod function .hb Return the error period for a particular process type. Default is to return errortolerance * publishinterval. Can be overridden as required [symbollist: the process types to return the error period for] timespan list of error period .rdb.moveandclear function .rdb Move a variable (table) from one namespace to another, deleting its contents. Useful during the end-of-day roll down for tables you do not want to save to the HDB [symbol: the namespace to move the table from; symbol:the namespace to move the variable to; symbol: the name of the variable] null .api.f function .api Find a function/variable/table/view in the current process [string:search string] table of matching elements .api.p function .api Find a public function/variable/table/view in the current process [string:search string] table of matching public elements .api.u function .api Find a non-standard q public function/variable/table/view in the current process. This excludes the .q, .Q, .h, .o namespaces [string:search string] table of matching public elements .api.s function .api Search all function definitions for a specific string [string: search string] table of matching functions and definitions .api.find function .api Generic method for finding functions/variables/tables/views. f,p and u are based on this [string: search string; boolean (list): public flags to include; boolean: whether the search is context senstive table of matching elements .api.search function .api Generic method for searching all function definitions for a specific string. s is based on this [string: search string; boolean: whether the search is context senstive table of matching functions and definitions .api.add function .api Add a function to the api description table [symbol:the name of the function; boolean:whether it should be called externally; string:the description; dict or string:the parameters for the function;string: what the function returns] null .api.fullapi function .api Return the full function api table [] api table .api.m function .api Return the ordered approximate memory usage of each variable and view in the process. Views will be re-evaluated if required [] memory usage table .api.mem function .api Return the ordered approximate memory usage of each variable and view in the process. Views are only returned if view flag is set to true. Views will be re-evaluated if required [boolean:return views] memory usage table .api.whereami function .api Get the name of a supplied function definition. Can be used in the debugger e.g. .api.whereami[.z.s] function definition symbol: the name of the current function .ps.publish function .ps Publish a table of data [symbol: name of table; table: table of data] .ps.subscribe function .ps Subscribe to a table and list of instruments [symbol(list): table name. ` for all; symbol(list): symbols to subscribe to. ` for all] mixed type list of table names and schemas .ps.initialise function .ps Initialise the pubsub routines. Any tables that exist in the top level can be published [] .async.deferred function .async Use async messaging to simulate sync communication [int(list): handles to query; query] (boolean list:success status; result list) .async.postback function .async Send an async message to a process and the results will be posted back within the postback function call [int(list): handles to query; query; postback function] boolean list: successful send status .cmp.showcomp function .cmp Show which files will be compressed and how; driven from csv file [`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress] table of files to be compressed .cmp.compressmaxage function .cmp Run compression on files using parameters specified in configuration csv file, and specifying the maximum age of files to compress [`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress] .cmp.docompression function .cmp Run compression on files using parameters specified in configuration csv file [`:/path/to/database; `:/path/to/configcsv] .loader.loadallfiles function .loader Generic loader function to read a directory of files in chunks and write them out to disk [dictionary of load parameters. Should have keys of headers (symbol list), types (character list), separator (character), tablename (symbol), dbdir (symbol). Optional params of dataprocessfunc (diadic function), datecol (name of column to extract date from: symbol), chunksize (amount of data to read at once:int), compression (compression parameters to use e.g. 16 1 0:int list), gc (boolean flag of whether to run garbage collection:boolean); directory containing files to load (symbol)] .sort.sorttab function .sort Sort and set the attributes for a table and set of partitions based on a configuration file (default is $KDBCONFIG/sort.csv) [2 item list of (tablename e.g. `trade; partitions to sort and apply attributes to e.g. `:/hdb/2000.01.01/trade`:hdb/2000.01.02/trade)] .sort.getsortcsv function .sort Read in the sort csv from the specified location [symbol: the location of the file e.g. `:config/sort.csv] .gc.run function .gc Run garbage collection, print debug info before and after .mem.objsize function .mem Returns the calculated memory size in bytes used by an object. It may take a little bit of time for objects with lots of nested structures (e.g. lots of nested columns) [q object] size of the object in bytes .tplog.check function .tplog Checks if tickerplant log can be replayed. If it can or can replay the first X messages, then returns the log handle, else it will read log as byte stream and create a good log and then return the good log handle [logfile (symbol), handle to the log file to check; lastmsgtoreplay (long), the index of the last message to be replayed from log ] handle to log file, will be either the input log handle or handle to repaired log, depends on whether the log was corrupt","title":"API Table"},{"location":"utilities/#grafanaq","text":"Grafana is an open source analytics platform, used to display time-series data from a web application. Currently it supports a variety of data sources including Graphite, InfluxDb & Prometheus with users including the likes of Paypal, Ebay, Intel and Booking.com. However, there is no in-built support for direct analysis of data from kdb+. Thus, using the SimpleJSON data source , we have engineered an adaptor to allow visualisation of kdb+ data.","title":"grafana.q"},{"location":"utilities/#requirements","text":"Grafana v5.2.2+ (Tested on Kdb v3.5+)","title":"Requirements"},{"location":"utilities/#getting-started","text":"Download and set up Grafana. This is well explained on the Grafana website , where you have the option to either download the software locally or let Grafana host it for you. For the purpose of this document, we host the software locally. Pull down this repository with the adaptor already installed in code/common. In your newly installed Grafana folder (eg.grafana-5.2.2/) run the command: ./bin/grafana-server web . This will start your Grafana server. If you would like to alter the port which this is run on, this can be changed in: /grafana-5.2.2/conf/custom.ini , Where custom.ini should be a copy of defaults.ini. You can now open the Grafana server in your web browser where you will be greeted with a login page to fill in appropriately. Once logged in, navigate to the configurations->plugin section where you will find the simple JSON adaptor, install this. Upon installation of the JSON you can now set-up your datasource. Host your data on a port accesible to Grafana, eg. the RDB. In the \"add new datasource\" panel, enter the details for the port in which your data is hosted, making the type SimpleJSON. Run the test button on the bottom of your page, this should succeed and you are ready to go!","title":"Getting Started"},{"location":"utilities/#using-the-adaptor","text":"As the adaptor is part of the TorQ framework it will automatically be loaded into TorQ sessions. From this point onwards you can proceed to use Grafana as it is intended, with the only difference coming in the form of the queries. Use cases and further examples of the queries can be seen in our blogpost: The Grafana-KDB Adaptor . For information and examples of how to execute server side functions in queries, please read our followup blogpost on the subject: Grafana kdb+ Adaptor Update . Here you can see examples of graphs, tables, heatmaps and single statistics. The best explanation of the inputs allowed in the query section can be seen pictorially here: Upon opening the query box, in the metrics tab, the user will be provided with a populated drop down of all possible options. Server functions are not included in the dropdown, but can be called by entering the letter f followed by the value of .grafana.del (see below) before their function call. Due to the limitations of the JSON messages, it is not possible for our adaptor to distinguish between panels. Consequently, every possible option is returned for each panel, the user can reduce these choices by simply entering the first letter of their panel type, g for graph, t for table and o for other (heatmap or single stat). From here, you can follow the above diagram to specify your type of query.","title":"Using the adaptor"},{"location":"utilities/#limitations-assumptions","text":"This adaptor has been built to allow visualisation of real-time and historical data. It is capable of handling static and timeseries data. In addition, the drop-down options have been formed such that only one query is possible per panel. If more than one query on a specfic panel is made it will throw an error. To get around this, we added the options of including all \"syms\" in queries so the options can be filtered out in the legend. Table queries should work for any table format supplied to the adaptor. However, time series data is limited by the requriment of a time column, in our adaptor we assume this column to be called time. This assumption can be modified to fit your data in the settings (config/settings/defualt.q) file which dictates the following lines at the start of the script: // user defined column name of time column timecol:@[value;`.grafana.timecol;`time]; // user defined column name of sym column sym:@[value;`.grafana.sym;`sym]; // user defined date range to find syms from timebackdate:@[value;`.grafana.timebackdate;2D]; // user defined number of ticks to return ticks:@[value;`.grafana.ticks;1000]; // user defined query argument deliminator del:@[value;`.grafana.del;\".\"]; .grafana.timecol represents the name of the time column and thus can be reassigned if your time column has a different name, eg. date. One more common modification could be changing the variable .grafana.sym which defines the name of the the sym column, which is normally referenced in financial data. However if the data is non-financial this could be tailored to represent another identifier such as name or postcode. This column is used to populate the drop down options in the query selector. .grafana.timebackdate is a user definable variable which dictates how far back into a hdb the adaptor will look to gather options for distinct syms to populate the dropdowns. It is important to note that this should be increased if all your required syms are not in the last 2 days. Optionally a user could hard code this list or implement their own search function to limit interrogation of the database. .grafana.ticks can be defined so that only n rows from the end of the table will be queried. This can be left as large as the user likes, but is included for managing large partitioned tables. One final important variable is .grafana.del , this dictates the delimeter between options in the drop down menus. This has significant repercussions if one of your columns includes full stops, eg. email adresses. As a result we have left this as definable so that the user can alter this to a non-disruptive value for their data eg./.","title":"Limitations &amp; Assumptions"},{"location":"visualisation/","text":"Visualisation kdb+ supports websockets and so HTML5 GUIs can be built. We have incorporated a set of server side and client side utilities to ease HTML GUI development. kdb+ Utilities The server side utilities are contained in html.q. These utilise some community code, specifically json.k and a modified version of u.q, both from Kx Systems. The supplied functionality includes: json.k provides two way conversion between kdb+ data structures and JSON; u.q is the standard pub/sub functionality provided with kdb+tick, and a modified version is incorporated to publish data structures which can be easily interpreted in JavaScript; functions for reformatting temporal types to be JSON compliant; page serving to utilise the inbuilt kdb+ webserver to serve custom web pages. An example would be instead of having to serve a page with a hardcoded websocket connection host and port, the kdb+ process can serve a page connecting back to itself no matter which host or port it is running on. JavaScript Utilities The JavaScript utilities are contained in kdbconnect.js. The library allows you to: create a connection to the kdb+ process; display the socket status; sending queries; binding results returned from kdb+ to updates in the webpage. Outline All communication between websockets and kdb+ is asynchronous. The approach we have adopted is to ensure that all data sent to the web browser is encoded as a JSON object containing a tag to enable the web page to decipher what the data relates to. The format we have chosen is for kdb+ to send dictionaries of the form: `name`data!(\"dataID\";dataObject) All the packing can be done by .html.dataformat. Please note that the temporal types are converted to longs which can easily be converted to JavaScript Date types. This formatting can be modified in the formating dictionary .html.typemap. q)a:flip `minute`time`date`month`timestamp`timespan`datetime`float`sym!enlist each (09:00; 09:00:00.0;.z.d; `month$.z.d; .z.p; .z.n;.z.z;20f;`a) q).html.dataformat[\"start\";(enlist `tradegraph)!enlist a] name| \"start\" data| (,`tradegraph)!,+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a) q)first (.html.dataformat[\"start\";(enlist `tradegraph)!enlist a])[`data;`tradegraph] minute | 32400000 time | 32400000 date | 1396828800000 month | 1396310400000 timestamp| \"2014-04-07T13:23:01Z\" timespan | 48181023 datetime | \"2014-04-07T13:23:01Z\" float | 20f sym | `a We have also extended this structure to allow web pages to receive data in a way similar to the standard kdb+tick pub/sub format. In this case, the data object looks like: `name`data!(\"upd\";`tablename`tabledata!(`trade;([]time:09:00 09:05 09:10; price:12 13 14))) This can be packed with .html.updformat: q).html.updformat[\"upd\";`tablename`tabledata!(`trade;a)] name| \"upd\" data| `tablename`tabledata!(`trade;+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a)) q)first(.html.updformat[\"upd\";`tablename`tabledata!(`trade;a)])[`data;`tabledata] minute | 32400000 time | 32400000 date | 1396828800000 month | 1396310400000 timestamp| \"2014-04-07T13:23:01Z\" timespan | 48181023 datetime | \"2014-04-07T13:23:01Z\" float | 20f sym | `a To utilise the pub/sub functionality, the web page must connect to the kdb+ process and subscribe for updates. Subscriptions are done using .html.wssub[`tablename] Publications from the kdb+ side are done with .html.pub[`tablename;tabledata] On the JavaScript side the incoming messages (data events) must be bound to page updates. For example, there might be an initialisation event called \u201cstart\u201d which allows the web page to retrieve all the initial data from the process. The code below redraws the areas of the page with the received data. /* Bind data - Data type \"start\" will execute the callback function */ KDBCONNECT.bind(\"data\",\"start\",function(data){ // Check that data is not empty if(data.hbtable.length !== 0) // Write HTML table to div element with id heartbeat-table { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.hbtable));} if(data.lmtable.length !== 0) // Write HTML table to div element with id logmsg-table { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.lmtable));} if(data.lmchart.length !== 0) // Log message error chart { MONITOR.barChart(data.lmchart,\"logmsg-chart\",\"Error Count\",\"myTab\"); } }); Similarly the upd messages must be bound to page updates. In this case, the structure is slightly different: KDBCONNECT.bind(\"data\",\"upd\",function(data){ if(data.tabledata.length===0) return; if(data.tablename === \"heartbeat\") { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.tabledata));} if(data.tablename === \"logmsg\") { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.tabledata));} if(data.tablename === \"lmchart\") { MONITOR.barChart(data.tabledata,\"logmsg-chart\",\"Error Count\",\"myTab\"); } }); To display the WebSocket connection status the event \u201cws_event\u201d must be bound and it will output one of these default messages: \u201cConnecting...\u201d, \u201cConnected\u201d and \u201cDisconnected\u201d depending on the connection state of the WebSocket. Alternatively the value of the readyState attribute will determine the WebSocket status. // Select html element using jQuery var $statusMsg = $(\"#status-msg\"); KDBCONNECT.bind(\"ws_event\",function(data){ // Data is the default message string $statusMsg.html(data); }); KDBCONNECT.core.websocket.readyState // Returns 1 if connected. Errors can be displayed by binding the event called \u201cerror\u201d. KDBCONNECT.bind(\"error\",function(data){ $statusMsg.html(\"Error - \" + data); }); Example A basic example is provided with the Monitor process. To get this to work, u.q from kdb+tick should be placed in the code/common directory to allow all processes to publish updates. It should be noted that this is not intended as a production monitoring visualisation screen, moreso a demonstration of functionality. See section monitorgui for more details. Further Work Further work planned includes: allow subscriptions on a key basis- currently all subscribers receive all updates; add JavaScript controls to allow in-place updates based on key pairs, and scrolling window updates e.g. add N new rows to top/bottom of the specified table; allow multiple websocket connections to be maintained at the same time.","title":"Visualisation"},{"location":"visualisation/#visualisation","text":"kdb+ supports websockets and so HTML5 GUIs can be built. We have incorporated a set of server side and client side utilities to ease HTML GUI development.","title":"Visualisation"},{"location":"visualisation/#kdb-utilities","text":"The server side utilities are contained in html.q. These utilise some community code, specifically json.k and a modified version of u.q, both from Kx Systems. The supplied functionality includes: json.k provides two way conversion between kdb+ data structures and JSON; u.q is the standard pub/sub functionality provided with kdb+tick, and a modified version is incorporated to publish data structures which can be easily interpreted in JavaScript; functions for reformatting temporal types to be JSON compliant; page serving to utilise the inbuilt kdb+ webserver to serve custom web pages. An example would be instead of having to serve a page with a hardcoded websocket connection host and port, the kdb+ process can serve a page connecting back to itself no matter which host or port it is running on.","title":"kdb+ Utilities"},{"location":"visualisation/#javascript-utilities","text":"The JavaScript utilities are contained in kdbconnect.js. The library allows you to: create a connection to the kdb+ process; display the socket status; sending queries; binding results returned from kdb+ to updates in the webpage.","title":"JavaScript Utilities"},{"location":"visualisation/#outline","text":"All communication between websockets and kdb+ is asynchronous. The approach we have adopted is to ensure that all data sent to the web browser is encoded as a JSON object containing a tag to enable the web page to decipher what the data relates to. The format we have chosen is for kdb+ to send dictionaries of the form: `name`data!(\"dataID\";dataObject) All the packing can be done by .html.dataformat. Please note that the temporal types are converted to longs which can easily be converted to JavaScript Date types. This formatting can be modified in the formating dictionary .html.typemap. q)a:flip `minute`time`date`month`timestamp`timespan`datetime`float`sym!enlist each (09:00; 09:00:00.0;.z.d; `month$.z.d; .z.p; .z.n;.z.z;20f;`a) q).html.dataformat[\"start\";(enlist `tradegraph)!enlist a] name| \"start\" data| (,`tradegraph)!,+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a) q)first (.html.dataformat[\"start\";(enlist `tradegraph)!enlist a])[`data;`tradegraph] minute | 32400000 time | 32400000 date | 1396828800000 month | 1396310400000 timestamp| \"2014-04-07T13:23:01Z\" timespan | 48181023 datetime | \"2014-04-07T13:23:01Z\" float | 20f sym | `a We have also extended this structure to allow web pages to receive data in a way similar to the standard kdb+tick pub/sub format. In this case, the data object looks like: `name`data!(\"upd\";`tablename`tabledata!(`trade;([]time:09:00 09:05 09:10; price:12 13 14))) This can be packed with .html.updformat: q).html.updformat[\"upd\";`tablename`tabledata!(`trade;a)] name| \"upd\" data| `tablename`tabledata!(`trade;+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a)) q)first(.html.updformat[\"upd\";`tablename`tabledata!(`trade;a)])[`data;`tabledata] minute | 32400000 time | 32400000 date | 1396828800000 month | 1396310400000 timestamp| \"2014-04-07T13:23:01Z\" timespan | 48181023 datetime | \"2014-04-07T13:23:01Z\" float | 20f sym | `a To utilise the pub/sub functionality, the web page must connect to the kdb+ process and subscribe for updates. Subscriptions are done using .html.wssub[`tablename] Publications from the kdb+ side are done with .html.pub[`tablename;tabledata] On the JavaScript side the incoming messages (data events) must be bound to page updates. For example, there might be an initialisation event called \u201cstart\u201d which allows the web page to retrieve all the initial data from the process. The code below redraws the areas of the page with the received data. /* Bind data - Data type \"start\" will execute the callback function */ KDBCONNECT.bind(\"data\",\"start\",function(data){ // Check that data is not empty if(data.hbtable.length !== 0) // Write HTML table to div element with id heartbeat-table { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.hbtable));} if(data.lmtable.length !== 0) // Write HTML table to div element with id logmsg-table { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.lmtable));} if(data.lmchart.length !== 0) // Log message error chart { MONITOR.barChart(data.lmchart,\"logmsg-chart\",\"Error Count\",\"myTab\"); } }); Similarly the upd messages must be bound to page updates. In this case, the structure is slightly different: KDBCONNECT.bind(\"data\",\"upd\",function(data){ if(data.tabledata.length===0) return; if(data.tablename === \"heartbeat\") { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.tabledata));} if(data.tablename === \"logmsg\") { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.tabledata));} if(data.tablename === \"lmchart\") { MONITOR.barChart(data.tabledata,\"logmsg-chart\",\"Error Count\",\"myTab\"); } }); To display the WebSocket connection status the event \u201cws_event\u201d must be bound and it will output one of these default messages: \u201cConnecting...\u201d, \u201cConnected\u201d and \u201cDisconnected\u201d depending on the connection state of the WebSocket. Alternatively the value of the readyState attribute will determine the WebSocket status. // Select html element using jQuery var $statusMsg = $(\"#status-msg\"); KDBCONNECT.bind(\"ws_event\",function(data){ // Data is the default message string $statusMsg.html(data); }); KDBCONNECT.core.websocket.readyState // Returns 1 if connected. Errors can be displayed by binding the event called \u201cerror\u201d. KDBCONNECT.bind(\"error\",function(data){ $statusMsg.html(\"Error - \" + data); });","title":"Outline"},{"location":"visualisation/#example","text":"A basic example is provided with the Monitor process. To get this to work, u.q from kdb+tick should be placed in the code/common directory to allow all processes to publish updates. It should be noted that this is not intended as a production monitoring visualisation screen, moreso a demonstration of functionality. See section monitorgui for more details.","title":"Example"},{"location":"visualisation/#further-work","text":"Further work planned includes: allow subscriptions on a key basis- currently all subscribers receive all updates; add JavaScript controls to allow in-place updates based on key pairs, and scrolling window updates e.g. add N new rows to top/bottom of the specified table; allow multiple websocket connections to be maintained at the same time.","title":"Further Work"}]}